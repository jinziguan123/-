# 计算机网络

## OSI和TCP/IP协议

1. 分别是什么样的结构：

   OSI协议：自顶向下分别为——应用层、表示层、会话层、运输层、网络层、数据链路层、物理层

   TCP/IP协议：自顶向下分别为——应用层、传输层、网际网层、网络接口层
   
   ![OSI 参考模型与 TCP/IP 的关系](assets/format,png-20230309230419839.png)



## 从输入URL到显示页面的全过程

1. URL解析：浏览器根据用户输入的URL，解析出协议（例如HTTP或HTTPS）、域名、端口号（如果有）、路径以及查询参数等信息。

2. DNS解析：浏览器向本地DNS服务器发送一个DNS查询请求，查询目标域名对应的IP地址。如果本地DNS服务器中不存在对应的IP地址，则会向上级DNS服务器发送查询请求，直到找到目标域名的IP地址为止。

3. 建立TCP连接：浏览器通过解析得到的IP地址和端口号，与服务器建立TCP连接。这个过程包括三次握手，即客户端发送SYN包，服务器返回SYN+ACK包，最后客户端发送ACK包。
4. 发起HTTP请求：建立TCP连接后，浏览器会向服务器发送一个HTTP请求，请求服务器提供对应URL的资源（通常是HTML页面）。
5. 服务器处理请求：服务器接收到浏览器发送的HTTP请求后，会根据请求的内容进行处理，然后将处理结果（通常是HTML文档）作为HTTP响应返回给浏览器。
6. 接收响应并渲染页面：浏览器接收到服务器的响应后，会解析HTML文档，然后根据HTML中的内容、样式表、JavaScript脚本等资源，渲染出页面的结构、样式和交互效果。
7. 关闭TCP连接：页面渲染完毕后，浏览器会关闭与服务器之间的TCP连接。
8. 显示页面：最后，浏览器会将渲染好的页面显示给用户。



## HTTP和HTTPS

### HTTP和HTTPS是什么：

* HTTP为超文本传输协议，是一种工作于应用层上的协议，是一个在计算机世界里专门在「两点」之间「传输」文字、图片、音频、视频等「超文本」数据的「约定和规范」；默认端口为80

* 由于HTTP是明码传输，存在着被窃听、篡改、冒充的风险，因此HTTPS在HTTP的基础上，于HTTP层和TCP层之间加入了一个SSL/TLS协议层，通过混合加密、摘要算法、数字证书的方法解决了HTTP的短板；默认端口为443



### HTTPS的通信过程：

如HTTP一样，HTTPS需要首先经过3次tcp层的握手，但在此之后，还要进行SSL/TLS握手，然后才能进行加密报文传输

上个问题也提到过HTTPS使用了混合加密，指的是在通信建立之前，使用的是非对称加密方式交换会话密钥（确保安全），而在建立通信之后，使用对称加密方式加密明文数据（保证效率）

> SSL/TLS协议握手：
>
> 其过程设计4次通信，使用不同的密钥交换算法，TLS 握手流程也会不一样；一般来说过程如下⬇️
>
> *1. ClientHello*
>
> 首先，由客户端向服务器发起加密通信请求，也就是 `ClientHello` 请求。
>
> 在这一步，客户端主要向服务器发送以下信息：
>
> （1）客户端支持的 TLS 协议版本，如 TLS 1.2 版本。
>
> （2）客户端生产的随机数（`Client Random`），后面用于生成「会话秘钥」条件之一。
>
> （3）客户端支持的密码套件列表，如 RSA 加密算法。
>
> *2. SeverHello*
>
> 服务器收到客户端请求后，向客户端发出响应，也就是 `SeverHello`。服务器回应的内容有如下内容：
>
> （1）确认 TLS 协议版本，如果浏览器不支持，则关闭加密通信。
>
> （2）服务器生产的随机数（`Server Random`），也是后面用于生产「会话秘钥」条件之一。
>
> （3）确认的密码套件列表，如 RSA 加密算法。
>
> （4）服务器的数字证书。
>
> *3.客户端回应*
>
> 客户端收到服务器的回应之后，首先通过浏览器或者操作系统中的 CA 公钥，确认服务器的数字证书的真实性。
>
> 如果证书没有问题，客户端会**从数字证书中取出服务器的公钥**，然后使用它加密报文，向服务器发送如下信息：
>
> （1）一个随机数（`pre-master key`）。该随机数会被服务器公钥加密。
>
> （2）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。
>
> （3）客户端握手结束通知，表示客户端的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘要，用来供服务端校验。
>
> 上面第一项的随机数是整个握手阶段的第三个随机数，会发给服务端，所以这个随机数客户端和服务端都是一样的。
>
> **服务器和客户端有了这三个随机数（Client Random、Server Random、pre-master key），接着就用双方协商的加密算法，各自生成本次通信的「会话秘钥」**。
>
> *4. 服务器的最后回应*
>
> 服务器收到客户端的第三个随机数（`pre-master key`）之后，通过协商的加密算法，计算出本次通信的「会话秘钥」。
>
> 然后，向客户端发送最后的信息：
>
> （1）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。
>
> （2）服务器握手结束通知，表示服务器的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘要，用来供客户端校验。
>
> 至此，整个 TLS 的握手阶段全部结束。接下来，客户端与服务器进入加密通信，就完全是使用普通的 HTTP 协议，只不过用「会话秘钥」加密内容

由于得到了CA的背书，HTTPS一定是安全的。如果说有什么风险，那就是有中间服务器同时冒充服务端和客户端夹在双方的通信之间进行消息的转发，但只要用户不在收到HTTPS警告的时候选择无视警告继续连接，那么HTTPS还是客观上安全的

### 常见的状态码：

* `1xx` 类状态码属于**提示信息**，是协议处理中的一种中间状态，实际用到的比较少

* `2xx` 类状态码表示服务器**成功**处理了客户端的请求，也是我们最愿意看到的状态

  - 「**200 OK**」是最常见的成功状态码，表示一切正常。如果是非 `HEAD` 请求，服务器返回的响应头都会有 body 数据
  - 「**204 No Content**」也是常见的成功状态码，与 200 OK 基本相同，但响应头没有 body 数据
  - 「**206 Partial Content**」是应用于 HTTP 分块下载或断点续传，表示响应返回的 body 数据并不是资源的全部，而是其中的一部分，也是服务器处理成功的状态

* `3xx` 类状态码表示客户端请求的资源发生了变动，需要客户端用新的 URL 重新发送请求获取资源，也就是**重定向**

  - 「**301 Moved Permanently**」表示永久重定向，说明请求的资源已经不存在了，需改用新的 URL 再次访问
  - 「**302 Found**」表示临时重定向，说明请求的资源还在，但暂时需要用另一个 URL 来访问

  301 和 302 都会在响应头里使用字段 `Location`，指明后续要跳转的 URL，浏览器会自动重定向新的 URL

  - 「**304 Not Modified**」不具有跳转的含义，表示资源未修改，重定向已存在的缓冲文件，也称缓存重定向，也就是告诉客户端可以继续使用缓存资源，用于缓存控制

* `4xx` 类状态码表示客户端发送的**报文有误**，服务器无法处理，也就是错误码的含义

  - 「**400 Bad Request**」表示客户端请求的报文有错误，但只是个笼统的错误
  - 「**403 Forbidden**」表示服务器禁止访问资源，并不是客户端的请求出错
  - 「**404 Not Found**」表示请求的资源在服务器上不存在或未找到，所以无法提供给客户端

* `5xx` 类状态码表示客户端请求报文正确，但是**服务器处理时内部发生了错误**，属于服务器端的错误码

  - 「**500 Internal Server Error**」与 400 类型，是个笼统通用的错误码，服务器发生了什么错误，我们并不知道
  - 「**501 Not Implemented**」表示客户端请求的功能还不支持，类似“即将开业，敬请期待”的意思
  - 「**502 Bad Gateway**」通常是服务器作为网关或代理时返回的错误码，表示服务器自身工作正常，访问后端服务器发生了错误
  - 「**503 Service Unavailable**」表示服务器当前很忙，暂时无法响应客户端，类似“网络服务正忙，请稍后重试”的意思



### GET与POST：

根据 RFC 规范，GET 的语义是从服务器获取指定的资源，POST 的语义是根据请求负荷（报文body）对指定的资源做出处理。简单来说，前者一般用来获取资源，后者用来提交请求

> 关于二者是否安全（不会破坏服务器上的资源）、幂等（多次执行相同的操作，结果是一样的）
>
> - **GET 方法就是安全且幂等的**，因为它是「只读」操作，无论操作多少次，服务器上的数据都是安全的，且每次的结果都是相同的。所以，**可以对 GET 请求的数据做缓存，这个缓存可以做到浏览器本身上（彻底避免浏览器发请求），也可以做到代理上（如nginx），而且在浏览器中 GET 请求可以保存为书签**。
> - **POST** 因为是「新增或提交数据」的操作，会修改服务器上的资源，所以是**不安全**的，且多次提交数据就会创建多个资源，所以**不是幂等**的。所以，**浏览器一般不会缓存 POST 请求，也不能把 POST 请求保存为书签**
>
> 然而现实使用的时候未必会遵守规范，所以GET可能也是不安全且不幂等，POST也可以是安全且幂等的



### HTTP/1.1、HTTP/2、HTTP/3:

* HTTP/1.1：

  优点

  - 使用长连接的方式改善了 HTTP/1.0 短连接造成的性能开销（短连接：每次发送请求都要进行3次握手、4次挥手）
  - 支持管道（pipeline）网络传输，只要第一个请求发出去了，不必等其回来，就可以发第二个请求出去，可以减少整体的响应时间

  缺点：

  * 请求 / 响应头部（Header）未经压缩就发送，首部信息越多延迟越大。只能压缩 `Body` 的部分
  * 发送冗长的首部。每次互相发送相同的首部造成的浪费较多
  * 服务器是按请求的顺序响应的，如果服务器响应慢，会招致客户端一直请求不到数据，也就是队头阻塞
  * 没有请求优先级控制
  * 请求只能从客户端开始，服务器只能被动响应

* HTTP/2（从HTTP/2开始都是基于HTTPS的了，所以安全性得到保证）：

  优点：

  * 头部压缩：如果你同时发出多个请求，他们的头是一样的或是相似的，那么，协议会帮你消除重复的部分
  * 二进制格式：HTTP/2 不再像 HTTP/1.1 里的纯文本形式的报文，而是全面采用了二进制格式，头信息和数据体都是二进制，并且统称为帧（frame）：头信息帧（Headers Frame）和数据帧（Data Frame）
  * 并发传输：提出了Stream概念，让多个Stream复用在同一条TCP连接上，解决了HTTP/1.1在应用层的队头阻塞问题
  * 服务器主动推送资源：服务端不再是被动地响应，可以主动向客户端发送消息

  缺点：

  * HTTP/2 通过 Stream 的并发能力，解决了 HTTP/1 队头阻塞的问题，但在TCP这一层依然存在队头阻塞问题：由于TCP是基于字节流的协议，只要前一个字节的数据没有送到，就会出发TCP的重传机制，那么还是会发生阻塞

* HTTP/3:

  优点：

  * 主要解决了HTTP/2的对头阻塞问题，方法就是：把TCP换成UDP（）虽然UDP本身不支持重传和发送顺序的，但基于 UDP的QUIC协议可以实现类似 TCP 的可靠性传输



## HTTP和RPC

### RPC是什么：

RPC全名远程过程调用，其本身虽然运作在应用层，但并不是一种协议，而是一种调用方式——通过屏蔽掉一些网络细节，使程序员能够像调用本地方法一样调用一个远程服务器的方法

### 既然有了HTTP，为什么还需要RPC：

然而事实是：RPC的出现比HTTP更早

> 现在电脑上装的各种**联网**软件，比如 xx管家，xx卫士，它们都作为**客户端（Client）需要跟服务端（Server）建立连接收发消息**，此时都会用到应用层协议，在这种 Client/Server (C/S) 架构下，它们可以使用自家造的 RPC 协议，因为它只管连自己公司的服务器就 ok 了。
>
> 但有个软件不同，**浏览器（Browser）**，不管是 Chrome 还是 IE，它们不仅要能访问自家公司的**服务器（Server）**，还需要访问其他公司的网站服务器，因此它们需要有个统一的标准，不然大家没法交流。于是，HTTP 就是那个时代用于统一 **Browser/Server (B/S)** 的协议。
>
> 也就是说在多年以前，**HTTP 主要用于 B/S 架构，而 RPC 更多用于 C/S 架构。但现在其实已经没分那么清了，B/S 和 C/S 在慢慢融合。\**很多软件同时支持多端，比如某度云盘，既要支持\**网页版**，还要支持**手机端和 PC 端**，如果通信协议都用 HTTP 的话，那服务器只用同一套就够了。而 RPC 就开始退居幕后，一般用于公司内部集群里，各个微服务之间的通讯

这里需要说一说两者之间的区别：

* 服务发现：两者其实大差不差，HTTP需要DNS来将域名转换为IP地址，而RPC需要诸如Etcd、Redis、Consul之类中间件来保存并获取IP和端口信息
* 底层连接形式：以主流的HTTP/1.1协议为例，其默认在建立底层 TCP 连接之后会一直保持这个连接，之后的请求和响应都会复用这条连接；而RPC协议，也跟 HTTP 类似，也是通过建立 TCP 长链接进行数据交互，但不同的地方在于，RPC 协议一般还会再建个**连接池**，在请求量大的时候，建立多条连接放在池内，要发数据的时候就从池里取一条连接出来，**用完放回去，下次再复用**，可以说非常环保。两者也差别不大
* **传输内容**：这里是两者最大的差异——HTTP传输的内容以字符串为主，由Header和Body构成，其中Body用Json进行序列化，整个结构体中有很多冗余数据，但是为了符合浏览器的协议，不得不保留这部分数据；而RPC定制化程度更高，用作公司内部的微服务的时候可以用体积更小的Protobuf或其他序列化协议去保存结构体数据，同时也不需要像 HTTP 那样考虑各种浏览器行为，比如 302 重定向跳转啥的，**因此性能也会更好一些**



## RPC

### RPC包含哪些部分

- 客户端和服务端建立网络连接模块( **server**模块、**client**模块 )
- 服务端**处理请求模块**
- **协议**模块
- **序列化**和**反序列**模块。



### RPC服务端如何处理请求

- **同步阻塞方式**（BIO）：客户端发一次请求，服务端生成一个对应线程去处理。当客户端同时发起的请求很多时，服务端需要创建多个线程去处理每一个请求，当达到了系统最大的线程数时，新来的请求就无法处理了。 

- **同步非阻塞方式** (NIO)：客户端发一次请求，服务端并不是每次都创建一个新线程来处理，而是通过 I/O 多路复用技术进行处理。就是把多个 I/O 的阻塞复用到同一个 select 的阻塞上，从而使系统在单线程的情况下可以同时处理多个客户端请求。这种方式的优势是开销小，不用为每个请求创建一个线程，可以节省系统开销。 

- **异步非阻塞方式**（AIO）：客户端发起一个 I/O 操作然后立即返回，等 I/O 操作真正完成以后，客户端会得到 I/O 操作完成的通知，此时客户端只需要对数据进行处理就好了，不需要进行实际的 I/O 读写操作，因为真正的 I/O 读取或者写入操作已经由内核完成了。这种方式的优势是客户端无需等待，不存在阻塞等待问题。





### 一个RPC请求的整体流程

![img](assets/739231-20190113163552755-1990586909.png)

1. 调用者（客户端Client）以本地调用的方式发起调用；
2. Client stub（客户端存根）收到调用后，负责将被调用的方法名、参数等打包编码成特定格式的能进行网络传输的消息体；
3. Client stub将消息体通过网络发送给服务端；
4. Server stub（服务端存根）收到通过网络接收到消息后按照相应格式进行拆包解码，获取方法名和参数；
5. Server stub根据方法名和参数进行本地调用；
6. 被调用者（Server）本地调用执行后将结果返回给server stub；
7. Server stub将返回值打包编码成消息，并通过网络发送给客户端；
8. Client stub收到消息后，进行拆包解码，返回给Client；
9. Client得到本次RPC调用的最终结果。



### 服务端收到请求，如何知道要调用哪个函数

#### 服务寻址

服务寻址可以使用 Call ID 映射。在本地调用中，函数体是直接通过函数指针来指定的，但是在远程调用中，函数指针是不行的，因为两个进程的地址空间是完全不一样的。

所以在 RPC 中，所有的函数都必须有自己的一个 ID。这个 ID 在所有进程中都是唯一确定的。

客户端在做远程过程调用时，必须附上这个 ID。然后我们还需要在客户端和服务端分别维护一个函数和Call ID的对应表。

当客户端需要进行远程调用时，它就查一下这个表，找出相应的 Call ID，然后把它传给服务端，服务端也通过查表，来确定客户端需要调用的函数，然后执行相应函数的代码。

#### 序列化和反序列化

**客户端怎么把参数值传给远程的函数呢**？在本地调用中，我们只需要把参数压到栈里，然后让函数自己去栈里读就行。

但是在远程过程调用时，**客户端跟服务端是不同的进程，不能通过内存来传递参数。甚至有时候客户端和服务端使用的都不是同一种语言**（比如服务端用C++，客户端用Java或者Python）。

**这时候就需要客户端把参数先转成一个字节流（编码），传给服务端后，再把字节流转成自己能读取的格式（解码）。这个过程叫序列化和反序列化。同理，从服务端返回的值也需要序列化反序列化的过程。**

#### 网络传输

远程调用往往用在网络上，客户端和服务端是通过网络连接的。**所有的数据都需要通过网络传输**，因此就需要有一个网络传输层。

**网络传输层需要把Call ID和序列化后的参数字节流传给服务端，然后再把序列化后的调用结果传回客户端。**只要能完成这两者的，都可以作为传输层使用。因此，它所使用的协议其实是不限的，能完成传输就行。**尽管大部分RPC框架都使用TCP协议，但其实UDP也可以，而gRPC干脆就用了HTTP2。**



### Protobuf编码原理

protobuf高效的秘密在于它的编码格式，它采用了 TLV(tag-length-value) 编码格式。每个字段都有唯一的 tag 值，它是字段的唯一标识。length 表示 value 数据的长度，length 不是必须的，对于固定长度的 value，是没有 length 的。value 是数据本身的内容

Protobuf的编码是基于**小端编码**的，一个具体的例子如下：

![pb4](assets/15e45db76fab28e2398a5e4c53035aaf.png)

其中前两个字节表示tag字段，后两个字节分别表示len和value，每一个字节的第一个bit表示后面还有没有这个字段的数据，1表示有，0表示没有。由于用小端法编码，所以tag字段需要看作```0 0 0 0 0 0 0 1  1 0 0 0 0 0 1 0```，把第一个bit的标识符去掉之后就变成了```0 0 0 0 0 0 1  0 0 0 0 0 1 0```，其中后三位表示wire type（这里是字符串类型，所以是2），前11位表示protobuf文件中这个变量的编号为16

> 综上我们可以得出，对于编号在1～15的变量，只需要一个字节就能够存了，再大一点就需要更多的字节了
>
> 因为最大也就是15 = 1111(2)，假设同样是string类型，那么编码之后就是```0 1 1 1 1 0 1 0```

这也就是为什么protobuf相较于普通的序列化来说更加高效，因为同样表示一个数字的时候，protobuf能够进行一定程度上的压缩



### 新老protocol buffer文件是如何做到兼容的

1. 不要随便修改字段的编号

2. 在添加新的字段之后，要保证新字段有默认值，这样旧代码在解析新的消息时可以使用默认值

3. 删除字段的时候要保留原本的字段编号，当然一般情况下不建议直接删除字段，可以

4. 不要使用required

   required 意味着消息中必须包含这个字段，并且字段的值必须被设置。如果在序列化或者反序列化的过程中，该字段没有被设置，那么protobuf库就会抛出一个错误。

   如果你在初期定义了一个 required 字段，但是在后来的版本中你想要删除它，那么这就会造成问题，因为旧的代码会期待该字段始终存在。为了确保兼容性，Google在最新版本的 protobuf（protobuf 3）中已经不再支持 required 修饰符。

5. 尽量使用小整数

   Varints 编码表示127以内的数字只需要 2 byte，1 byte 是 tag，1 byte 是值，压缩效果很好。但是如果表示一个很大的数如 ：`1<<31 - 1`，除去 tag 外需要占用 5 byte，比普通的 int 32 多1 byte，因为 protobuf 每个byte最高位有一个标识符占用 1 bit。

6. 不轻易修改字段类型

   通常不建议修改字段的类型，因为这可能会导致数据解析错误。如果必须修改，需要确保新类型与旧类型在序列化和反序列化时是兼容的。



## TCP

### TCP报文格式

![TCP 头格式](assets/format,png-20230309230534096.png)

比较重要的有以下几个：

* 序列号：在建立连接时由计算机生成的随机数作为其初始值，通过 SYN 包传给接收端主机，每发送一次数据，就「累加」一次该「数据字节数」的大小。**用来解决网络包乱序问题**
* 确认应答号：指下一次「期望」收到的数据的序列号，发送端收到这个确认应答以后可以认为在这个序号以前的数据都已经被正常接收。**用来解决丢包的问题**
* 控制位：
  * *ACK*：该位为 `1` 时，「确认应答」的字段变为有效，TCP 规定除了最初建立连接时的 `SYN` 包之外该位必须设置为 `1` 
  * *RST*：该位为 `1` 时，表示 TCP 连接中出现异常必须强制断开连接
  * *SYN*：该位为 `1` 时，表示希望建立连接，并在其「序列号」的字段进行序列号初始值的设定
  * *FIN*：该位为 `1` 时，表示今后不会再有数据发送，希望断开连接。当通信结束希望断开连接时，通信双方的主机之间就可以相互交换 `FIN` 位为 1 的 TCP 段



### UDP报文格式





### 为什么需要TCP：

因为IP层的传输是**不可靠的**，只负责传，不保证包的交付，不保证交付的顺序，不保证包的完整性。所以我们需要在传输层保证数据的可靠性。

这就决定了TCP是**面向连接的、可靠的、面向字节流**的传输协议



### TCP和UDP区别

1. TCP是面向连接的，UDP是无连接的
2. TCP是可靠的，UDP是不可靠的
3. TCP是面向字节流的，UDP是面向数据报文的
4. TCP只支持点对点通信，UDP支持一对一，一对多，多对多
5. TCP报文首部20个字节，UDP首部8个字节
6. TCP有拥塞控制机制，UDP没有
7. TCP协议下双方发送接受缓冲区都有，UDP并无实际意义上的发送缓冲区，但是存在接受缓冲区



### 面向报文和面向字节流

- 面向报文：

  操作系统不会对报文的内容进行拆分，一个报文携带的数据就是完整的数据

- 面向字节流：

  操作系统为了保证传输的稳定性和效率，会对报文进行分片然后一片一片传输，



### 粘包和解决方法

#### 什么是粘包

在面向字节流的传输过程中，本来逻辑上连在一起的数据被强行切割成了多个部分，也可能逻辑上不相连的数据被分到了同一个片段，也就是所谓的粘包

简单来说，之所以会出现粘包问题，本质上是接收方不清楚一条信息的边界在哪里，如果知道了边界在哪，接收方就可以通过边界来划分出有效的用户消息。

#### 怎么解决粘包

- 固定长度的消息

  即每个用户消息都是固定长度的，比如规定一个消息的长度是 64 个字节，当接收方接满 64 个字节，就认为这个内容是一个完整且有效的消息。

  缺点显然就是不够灵活，所以实际使用很少

  

- 特殊字符作为边界

  我们可以在两个用户消息之间插入一个特殊的字符串，这样接收方在接收数据时，读到了这个特殊字符，就把认为已经读完一个完整的消息。

  > 比如HTTP 通过设置回车符、换行符作为 HTTP 报文协议的边界

  有一点要注意，这个作为边界点的特殊字符，如果刚好消息内容里有这个特殊字符，我们要对这个字符转义，避免被接收方当作消息的边界点而解析到无效的数据。

  

- 自定义消息结构

  我们可以自定义一个消息结构，由包头和数据组成，其中包头包是固定大小的，而且包头里有一个字段来说明紧随其后的数据有多大。

  比如这个消息结构体，首先 4 个字节大小的变量来表示数据长度，真正的数据则在后面。

  ```c++
  struct {
    u_int32_t message_length;
    char message_data[];
  }message;
  ```

  当接收方接收到包头的大小（比如 4 个字节）后，就解析包头的内容，于是就可以知道数据的长度，然后接下来就继续读取数据，直到读满数据的长度，就可以组装成一个完整到用户消息来处理了。



### TCP和UDP可以使用同一个端口吗？为什么：

**可以的**

正如在数据链路层中，通过 MAC 地址来寻找局域网中的主机。在网际层中，通过 IP 地址来寻找网络中互连的主机或路由器。在传输层中，需要通过端口进行寻址，来识别同一计算机中同时通信的不同应用程序

所以，端口的作用是为了区分同一个主机上不同应用程序的数据包，而TCP和UDP在内核中是两个完全独立的软件模块。当主机收到数据包后，可以在 IP 包头的「协议号」字段知道该数据包是 TCP/UDP，所以可以根据这个信息确定送给哪个模块（TCP/UDP）处理，送给 TCP/UDP 模块的报文根据「端口号」确定送给哪个应用程序处理

因此，TCP/UDP 各自的端口号也相互独立，如 TCP 有一个 80 号端口，UDP 也可以有一个 80 号端口，二者并不冲突



### 3次握手以及能否增减次数：

过程如下：

* 一开始，客户端和服务端都处于 `CLOSE` 状态。先是服务端主动监听某个端口，处于 `LISTEN` 状态
* 客户端会随机初始化序号（`client_isn`），将此序号置于 TCP 首部的「序号」字段中，同时把 `SYN` 标志位置为 `1`，表示 `SYN` 报文。接着把第一个 SYN 报文发送给服务端，表示向服务端发起连接，该报文不包含应用层数据，之后客户端处于 `SYN-SENT` 状态
* 服务端收到客户端的 `SYN` 报文后，首先服务端也随机初始化自己的序号（`server_isn`），将此序号填入 TCP 首部的「序号」字段中，其次把 TCP 首部的「确认应答号」字段填入 `client_isn + 1`, 接着把 `SYN` 和 `ACK` 标志位置为 `1`。最后把该报文发给客户端，该报文也不包含应用层数据，之后服务端处于 `SYN-RCVD` 状态
* 客户端收到服务端报文后，还要向服务端回应最后一个应答报文，首先该应答报文 TCP 首部 `ACK` 标志位置为 `1` ，其次「确认应答号」字段填入 `server_isn + 1` ，最后把报文发送给服务端，这次报文可以携带客户到服务端的数据，之后客户端处于 `ESTABLISHED` 状态
* 服务端收到客户端的应答报文后，也进入 `ESTABLISHED` 状态

可以看出，**第三次握手时可以携带数据的，但前两次不行**

> ## 为什么是3次握手而不是2次或者4次
>
> 三次握手的原因如下：
>
> - 三次握手才可以阻止重复历史连接的初始化（主要原因）
> - 三次握手才可以同步双方的初始序列号
> - 三次握手才可以避免资源浪费
>
> 
>
> ### 如果是2次握手
>
> 在两次握手的情况下，服务端在收到 SYN 报文后，就进入 ESTABLISHED 状态，意味着这时可以给对方发送数据，但是客户端此时还没有进入 ESTABLISHED 状态，假设这次是历史连接，客户端判断到此次连接为历史连接，那么就会回 RST 报文来断开连接，而服务端在第一次握手的时候就进入 ESTABLISHED 状态，所以它可以发送数据的，但是它并不知道这个是历史连接，它只有在收到 RST 报文后，才会断开连接
>
> ### 如果是4次握手
>
> 三次握手就已经理论上最少可靠连接建立，所以不需要使用更多的通信次数



### 4次挥手以及能否减少次数：

过程如下：

* 客户端打算关闭连接，此时会发送一个 TCP 首部 `FIN` 标志位被置为 `1` 的报文，也即 `FIN` 报文，之后客户端进入 `FIN_WAIT_1` 状态
* 服务端收到该报文后，就向客户端发送 `ACK` 应答报文，接着服务端进入 `CLOSE_WAIT` 状态
* 客户端收到服务端的 `ACK` 应答报文后，之后进入 `FIN_WAIT_2` 状态
* 等待服务端处理完数据后，也向客户端发送 `FIN` 报文，之后服务端进入 `LAST_ACK` 状态
* 客户端收到服务端的 `FIN` 报文后，回一个 `ACK` 应答报文，之后进入 `TIME_WAIT` 状态
* 服务端收到了 `ACK` 应答报文后，就进入了 `CLOSE` 状态，至此服务端已经完成连接的关闭
* 客户端在经过 `2MSL` 一段时间后，自动进入 `CLOSE` 状态，至此客户端也完成连接的关闭

客户端和服务端各有一个FIN和ACK，所以是4次挥手，此外**主动关闭连接的才会有TIME_WAIT状态**

> ## 能否变成3次挥手
>
> 直接说结论，可以的，如果被动关闭方「**没有数据要发送」并且「开启了 TCP 延迟确认机制」**，那么第二次和第三次握手就会被合并到一起。事实上现代互联网三次握手是一种比较常见的情况，因为TCP延迟确认机制是默认开启的
>
> 
>
> ### 什么是延迟确认机制
>
> 由于不携带数据的ACK是效率非常低的（因为尽管没有数据，ACK依然占用了20个字节的IP头和20个字节的TCP头），所以我们希望尽可能让ACK携带数据进行传输，这就是延迟确认机制，具体策略如下：
>
> - 当有响应数据要发送时，ACK 会随着响应数据一起立刻发送给对方
> - 当没有响应数据要发送时，ACK 将会延迟一段时间，以等待是否有响应数据可以一起发送
> - 如果在延迟等待发送 ACK 期间，对方的第二个数据报文又到达了，这时就会立刻发送 ACK



### 为什么需要TIME_WAIT状态：

* 为了防止历史连接中的消息被后续相同的四元组接收
* 保证「被动关闭连接」的一方，能被正确的关闭
* 因为客户端发送的第四次挥手的ACK应答数据包，服务端可能没有收到，如果服务端在发送第三次挥手的FIN数据包后，等待一段时间后没有收到ACK应答，那么会重新发送第三次挥手的FIN数据包，客户端收到后再次发送第四次挥手的ACK数据包



### 为什么TIME_WAIT是2MSL时长

2MSL，即两个最大报文段生存时间。

这里一个比较合理的解释是：由于在IP报文头中有一个ttl字段，默认为64，表示IP数据包在网络中最多可以经过的路由数量。将TIME_WAIT设置为2MSL，实际上是允许数据包丢失一次后进行重传的

至于为什么不设置为更多的MSL，因为假设网络丢包率为1%，那么连续两次丢包的可能性为万分之一，概率太小了，忽视它更具性价比



### TCP滑动窗口：

类似于CPU的流水线，客户端在发送这个窗口范围之内的数据包的时候，不需要等待服务端的确认即可发送下一个数据包。在这个过程中，即使数据包的应答报文丢失了，也不会触发重传机制，可以通过「下一个确认应答进行确认」

滑动窗口的大小一般由接收方来决定，发送方发送的数据大小不能超过接收方的窗口大小，否则接收方就无法正常接收到数据



### TCP流量控制：

如果发送方把数据发送的过快，接收方就可能来不及接收，这就会造成数据的丢失，`流量控制`就是让发送方的发送速率不要太快，让接收方来得及接收所有数据，具体的实现方法就是通过动态调整滑动窗口大小——如果应用程序没有办法一次性消费所有数据包，那么尚未消费的部分会留在接收方的缓冲区中，这个时候接收方需要发送改变滑动窗口大小的通知给发送方

但是如果这个时候，由于操作系统实在过于繁忙，不得不缩小缓冲区使得缓冲区大小小于未消费数据包的大小，那么势必就会造成数据包的丢失，而另一方面，收到了收缩窗口大小请求的发送方可能会出现滑动窗口大小被调整成负值的情况。**为了防止这种情况发生，TCP 规定是不允许同时减少缓存又收缩窗口的，而是采用先收缩窗口，过段时间再减少缓存，这样就可以避免了丢包情况**



### TCP拥塞控制：

当网络出现拥堵时，数据包丢失、时延的可能性会变大，这个时候如果触发了发送方的超时重传机制，那么对整个网络的负担会更大，因此我们需要拥塞控制，当网络发送拥塞时，TCP会主动降低发送的数据量。这就引出了**拥塞窗口**，这是由发送方维护的一个状态变量，会根据网络的拥塞程度动态变化，大小为```min(发送窗口, 接受窗口)```

> ## 如何知道当前网络是否出现了拥塞
>
> 只要发送方没有在规定时间内收到ACK报文，触发了超时重传，则认为出现了拥塞
>
> 
>
> ## 拥塞控制的4种算法
>
> 1. 慢启动：
>
>    **当发送方每收到一个 ACK，拥塞窗口 cwnd 的大小就会加 1**，直观点的说法就是每一个轮次的拥塞窗口大小呈指数级增长：1、2、4、8、16......
>
>    虽然增长速度很快，但是启动时的窗口大小很小，所以叫慢启动
>
>    慢启动有一个ssthresh变量（**一般来说ssthresh的大小为2^16 - 1个字节**）：
>
>    * 当拥塞窗口大小cwnd < ssthresh，使用慢启动
>    * 当拥塞窗口大小cwnd >= ssthresh， 使用拥塞避免算法
>
> 2. 拥塞避免算法：
>
>    进入拥塞避免算法后，cwnd的规则是：**每当收到一个 ACK 时，cwnd 增加 1/cwnd**
>
> 3. 拥塞发生：
>
>    当网络出现拥塞，也就是会发生数据包重传，重传机制主要有两种：
>
>    - 超时重传
>    - 快速重传
>
>    ### 超时重传
>
>    当发生了「超时重传」，ssthresh 和 cwnd 的值会发生变化：
>
>    - `ssthresh` 设为 `cwnd/2`，
>    - `cwnd` 重置为 `1` （是恢复为 cwnd 初始化值，我这里假定 cwnd 初始化值 1）
>
>    接着，就重新开始慢启动，慢启动是会突然减少数据流的。这真是一旦「超时重传」，马上回到解放前。但是这种方式太激进了，反应也很强烈，会造成网络卡顿
>
>    ### 快速重传
>
>    如果接收方连续收到了3个重复的ACK，说明网络并不是那么糟糕，但是还是要处理，所以没有必要像RT0超时那样强烈，此时就会进入快速重传算法：
>
>    - 拥塞窗口 `cwnd = ssthresh + 3` （ 3 的意思是确认有 3 个数据包被收到了）
>    - 重传丢失的数据包
>    - 如果再收到重复的 ACK，那么 cwnd 增加 1
>    - 如果收到新数据的 ACK 后，把 cwnd 设置为第一步中的 ssthresh 的值，原因是该 ACK 确认了新的数据，说明从 duplicated ACK 时的数据都已收到，该恢复过程已经结束，可以回到恢复之前的状态了，也即再次进入拥塞避免状态



## IP

### 基本认知：

IP工作在TCP/IP网络的第三层——网络层，该层的主要作用是负责主机与主机之间的点对点通信

> ## 区分IP和MAC
>
> 很简单
>
> * **MAC 的作用是实现「直连」的两个设备之间通信，而 IP 则负责在「没有直连」的两个网络之间进行通信传输**
>
> IP层隐藏了数据链路层的很多细节，从外界来看实现了不同网络之间的两台主机的点对点连接，但在底层的设计上，其实是从主机到路由再到主机的一步步转发，而这个转发是由MAC实现的



### IP分类：

IPv4的地址被分为A,B,C,D,E一共五类，区分方式为关注IP地址的前1/2/3/4/5位是否为零，其中，ABC三类用于普通的网络服务，D类常用于多播，而E类还没有用到，具体的网络号、主机号划分可以直接百度（）



### 单播、广播和多播：



### 无地址分类CIDR：

简单来说就是IP地址+子网掩码，细节不多说了反正会了



## Ping和ICMP：

### ICMP协议

ICMP协议（互联网控制报文协议），主要功能包括：**确认 IP 包是否成功送达目标地址、报告发送过程中 IP 包被废弃的原因和改善网络设置等**，与IP同样工作在网络层

ICMP报文可以分为两大类：

- 一类是用于诊断的查询消息，也就是「**查询报文类型**」
- 另一类是通知出错原因的错误消息，也就是「**差错报文类型**」



### Ping——查询报文类型的使用

ping是一个应用层指令，它的功能比较简单，就是**尝试**发送一个小小的消息到目标机器上，判断目的机器是否**可达**，其实也就是判断目标机器网络是否能连通

在执行ping指令之后，ICMP报文+IP头+MAC头层层包装后发送到目标主机，目标主机在接收到数据包之后会返回一个响应报文，结构与查询报文相同



### 127.0.0.1 和 localhost 以及 0.0.0.0 有区别吗

* 首先localhost不是IP，本质上是和"baidu.com"一样是一个域名，但是会被默认解析为127.0.0.1，所以localhost和127.0.0.1实际上是等价的

* 其次，ping 0.0.0.0会失败，因为它在`IPV4`中表示的是无效的**目标地址**，但是，如果我们监听0.0.0.0的时候，它表示本机上的**所有IPV4地址**，这个时候不管使用的是**本机地址（en0）**还是**回环地址（lo）**都会访问到这个服务器



# Redis

## 什么是Redis

Redis 是一种基于内存的数据库，对数据的读写操作都是在内存中完成，因此**读写速度非常快**，常用于**缓存，消息队列、分布式锁等场景**

Redis 提供了多种数据类型来支持不同的业务场景，比如 String(字符串)、Hash(哈希)、 List (列表)、Set(集合)、Zset(有序集合)、Bitmaps（位图）、HyperLogLog（基数统计）、GEO（地理信息）、Stream（流），并且对数据类型的操作都是**原子性**的，因为执行命令由单线程负责的，不存在并发竞争的问题

一般使用Redis作为MySql的缓存，因为Redis的速度很快很快，而且是高性能和高并发的



## Redis为什么快

- 基于内存
- 服务模型为单线程，避免了上下文切换带来的开销
- 使用了io多路复用处理socket请求



## Redis数据结构

### 常见数据类型

- String 

  - 内部实现：SDS
  - 常用指令：SET、GET、EXISTS、STRLEN、DEL、INCR（BY）、DECR（BY）、EXPIRE、TTL、SETNX
  - 应用场景：缓存对象、常规计数、分布式锁、共享 session 信息等

- List 

  - 内部实现：Quick List
  - 常用指令：LPUSH、RPUSH、LPOP、RPOP、LRANGE、BLPOP、BRPOP（最后两个是阻塞的POP指令）
  - 应用场景：消息队列（但是有两个问题：1. 生产者需要自行实现全局唯一 ID；2. 不能以消费组形式消费数据）等

- Hash 类型

  - 内部实现：List Pack或者哈希表

    - 如果哈希类型元素个数小于 512个，所有值小于64字节，Redis 会使用**listpack**作为 Hash 类型的底层数据结构
    - 如果哈希类型元素不满足上面条件，Redis 会使用**哈希表**作为 Hash 类型的 底层数据结构。

  - 常用命令：HSET、HGET、HDEL、HGETALL、HINCRYBY

  - 应用场景：缓存对象、购物车等

    > 用购物车举个例子：
    >
    > - 添加商品：`HSET cart:{用户id} {商品id} 1`
    > - 添加数量：`HINCRBY cart:{用户id} {商品id} 1`
    > - 商品总数：`HLEN cart:{用户id}`
    > - 删除商品：`HDEL cart:{用户id} {商品id}`
    > - 获取购物车所有商品：`HGETALL cart:{用户id}`

- Set 

  - 内部结构：哈希表或者intset
    - 如果集合中的元素都是整数且元素个数小于512，Redis 会使用**整数集合**作为 Set 类型的底层数据结构
    - 如果集合中的元素不满足上面条件，则 Redis 使用**哈希表**作为 Set 类型的底层数据结构。
  - 常用指令：SADD、SREM、SMEMBERS、SCARD、SISMEMBER、SRANDMEMBER（随机挑n个但不删除）、SPOP（随机挑n个从set里面删掉）、SINTER（交集）、SUNION（并集）、SDIFF（差集）
  - 应用场景：聚合计算（并集、交集、差集）场景，比如点赞、共同关注、抽奖活动等

- Zset 

  - 内部结构：跳表或者listpack
    - 如果有序集合的元素个数小于 128 个，并且每个元素的值小于64字节时，Redis 会使用**压缩列表**作为 Zset 类型的底层数据结构；
    - 如果有序集合的元素不满足上面的条件，Redis 会使用**跳表**作为 Zset 类型的底层数据结构；
  - 常用指令：ZADD、ZREM、ZSCORE、ZCARD、ZINCRBY、ZRANGE、ZREVRANGE、ZUNION（并集）、ZINTER（交集）、ZRANGEBYSCORE、Z（REV）RANGEBYLEX（按照字典许来排序）
  - 应用场景：排序场景，比如排行榜、电话和姓名排序等

- BitMap（2.2 版新增）

  - 内部实现：String（因为String 类型是会保存为二进制的字节数组）
  - 常用指令：SETBIT、GETBIT、BITCOUNT（获取指定范围内1的个数）、BITOP（与或非异或运算）、BITPOS（第一次出现0或者1的位置）
  - 应用场景：二值状态统计的场景，比如签到、判断用户登陆状态、连续签到用户总数等；

- HyperLogLog（2.8 版新增）：海量数据基数统计的场景，比如百万级网页 UV 计数等；

- GEO（3.2 版新增）：存储地理位置信息的场景，比如滴滴叫车；

- Stream（5.0 版新增）

  - 常用指令
    - XADD：插入消息，保证有序，可以自动生成全局唯一 ID；
    - XLEN ：查询消息长度；
    - XREAD：用于读取消息，可以按 ID 读取数据；
    - XDEL ： 根据消息 ID 删除消息；
    - DEL ：删除整个 Stream；
    - XRANGE ：读取区间消息
    - XREADGROUP：按消费组形式读取消息；
    - XPENDING 和 XACK：
      - XPENDING 命令可以用来查询每个消费组内所有消费者「已读取、但尚未确认」的消息；
      - XACK 命令用于向消息队列确认消息处理已完成；
  - 应用场景：消息队列，相比于基于 List 类型实现的消息队列，有这两个特有的特性：自动生成全局唯一消息ID，支持以消费组形式消费数据

![在这里插入图片描述](assets/388f43060891ccdae94082a154e1ad54-20240825183648200.png)



### 底层数据类型

#### 字典（Dict）

dict是一个用于维护key和value映射关系的数据结构。Redis的一个database中所有key到value的映射，就是使用一个dict来维护的。dict本质上是为了解决算法中的查找问题

```c++
// dict的数据结构定义
struct dict {
    dictType *type;

    dictEntry **ht_table[2];
    unsigned long ht_used[2];

    long rehashidx; /* rehashing not in progress if rehashidx == -1 */

    /* Keep small vars at end for optimal (minimal) struct padding */
    unsigned pauserehash : 15; /* If >0 rehashing is paused */

    unsigned useStoredKeyApi : 1; /* See comment of storedHashFunction above */
    signed char ht_size_exp[2]; /* exponent of size. (size = 1<<exp) */
    int16_t pauseAutoResize;  /* If >0 automatic resizing is disallowed (<0 indicates coding error) */
    void *metadata[];
};
```

![img](assets/90ce604955f1b19ca3d02c55f98fa468.png)

- dict采用哈希函数对key取哈希值，得到在哈希表中的位置(桶的位置)，再采用拉链法解决hash冲突。

- 两个哈希表（ht[2]）：只有在重哈希的过程中，ht[0]和ht[1]才都有效。而在平常情况下，只有ht[0]有效，ht[1]里面没有任何数据。上图表示的就是重哈希进行到中间某一步时的情况。

- 重哈希过程：跟HashMap一样，当装载因子（load factor）超过预定值时就会进行rehash。dict进行重hash扩容是将ht[0]上某一个bucket（即一个dictEntry链表）上的每一个dictEntry移动到扩容后的ht[1]上，能触发rehash的操作有查询、插入和删除元素。每次移动一个链表（即渐进式rehash）原因是为了防止redis长时间的堵塞导致不可用。

- dict添加操作：如果正在重哈希中，会把数据插入到ht[1]；否则插入到ht[0]。

- dict查询操作：先在第一个哈希表ht[0]上进行查找，再判断当前是否在重哈希，如果没有，那么在ht[0]上的查找结果就是最终结果。否则，再在ht[1]上进行查找。查询时会先根据key计算出桶的位置，在到桶里的链表上寻找key。

- dict删除操作：判断当前是不是在重哈希过程中，如果是只在ht[0]中查找要删除的key；否则ht[0]和ht[1]它都要查找删除。

##### rehash的触发条件：

1. 计算负载因子：

   ```c++
   #哈希表的负载因子计算：负载因子 = 哈希表已保存节点数量 / 哈希表大小
   load_factor = ht[0].used / ht[0].size
   ```

2. 触发rehash：

   一种是触发扩容操作，另一种是触发收缩操作。两种rehash触发的条件是不一样的，需要各自满足一下条件才能导致rehash操作。

   - 扩容：

     - 服务器目前没有在执行 BGSAVE 命令或者 BGREWRITEAOF 命令，并且哈希表的负载因子大于等于 1
     - 服务器目前正在执行 BGSAVE 命令或者 BGREWRITEAOF 命令，并且哈希表的负载因子大于等于 5

     > 根据 BGSAVE 命令或 BGREWRITEAOF 命令是否正在执行，服务器执行扩展操作所需的负载因子并不相同，这是因为在执行 BGSAVE 命令或 BGREWRITEAOF命令的过程中， Redis会fork一个子进程，而大多数操作系统都采用写时复制（copy-on-write）技术来优化子进程的使用效率，所以在子进程存在期间， 服务器会提高执行扩展操作所需的负载因子，从而尽可能地避免在子进程存在期间进行哈希表扩展操作，这可以避免不必要的内存写入操作， 最大限度地节约内存

   - 收缩：

     - 当哈希表的负载因子小于0.1时，程序自动开始对哈希表执行收缩操作

   Rehash的整个过程是分多批次的、渐进的，这是为了避免在迁移过大的数据量的时候导致线程的阻塞



#### 简单动态字符串（SDS）

Redis中为了实现方便的扩展，考虑到安全和性能，自己定义了一个结构用来存储字符串，这个数据结构就是：简单动态字符串(Simple Dynamic String 简称sds)，并将 SDS 用作 Redis 的默认字符串

```c++
/*
 * redis中保存字符串对象的结构
 */
struct sdshdr {
    //用于记录buf数组中使用的字节的数目，和SDS存储的字符串的长度相等 
    int len;
    //用于记录buf数组中没有使用的字节的数目 
    int free;
    //字节数组，用于储存字符串
    char buf[]; //buf的大小等于len+free+1，其中多余的1个字节是用来存储’\0’的
};
```

##### SDS和C字符串的区别

1. 获取字符串长度：

   由于C字符串没有记录自身的长度信息，所以获取C字符串长度的时候，必须遍历整个字符串，其时间复杂度是O(n)，而SDS中有len属性，所以在获取其长度时，时间复杂度为O(1)

2. 内存分配释放策略

   - 对于C字符串而言，不管是字符串拼接，还是字符串缩短，都要扩展底层的char数组的空间大小，再将旧char数据拷贝过来。
   - SDS的内存分配策略就不一样，可以概括为**预分配 + 惰性释放**

   > #### SDS内存分配策略：预分配
   > 1. 如果对SDS字符串修改后，len的值小于1MB，那么程序会分配和len同样大小的空间给free，此时len和free的值是相同。
   >    例如：如果SDS的字符串长度修改为15字节，那么会分配15字节空间给free，SDS的buf属性长度为15（len）+15（free）+1（空字符） = 31字节。
   >
   > 2. 如果SDS字符串修改后，len大于等于1MB，那么程序会分配1MB的空间给free。
   >    例如：SDS字符串长度修改为50MB那么程序会分配1MB的未使用空间给free，SDS的buf属性长度为 50MB（len）+1MB（free）+1byte（空字符）。
   >
   > #### SDS内存释放策略：惰性释放
   > 1. 当需要缩短SDS字符串时，程序并不立刻将内存释放，而是使用free属性将这些空间记录下来，实际的buf大小不会变，以备将来使用。

   这样做带来的优点就是能够有效缓解字符串扩容时可能导致的缓冲区溢出问题，并且由于使用二进制存储数据，对于存放数据的格式没有要求（不像C字符串那样要求以"\0"结尾）



#### 链表（List）

ListNode的实现和传统的链表节点没什么区别，只不过在List的实现上，redis多封装了一些东西

```c++
typedef struct listNode{
  struct listNode *prev;
  struct listNode *next;
  void *value;
}listNode;

typedef struct list{
  listNode *head;
  listNode *tail;
  // 节点复制函数
  void *(*dup)(void *ptr);
  // 节点释放函数
  void (*free)(void *ptr);
  // 节点比较函数
  void (*match)(void *ptr, void *key);
  // 节点数量
  unsigned long len;
}list;
```



#### 整数集合（IntSet）

intset是Redis集合的底层实现之一，当存储整数集合并且数据量较小的情况下Redis会使用intset作为set的底层实现，当数据量较大或者集合元素为字符串时则会使用dict实现set

```c++
typedef struct intset {
    uint32_t encoding;
    uint32_t length;
    int8_t contents[];
} intset;
```

intset的特点：

1. 所有的元素都保存在contents 数组中，且按照从小到大的顺序排列，并且不包含任何重复项。
1. intset将整数元素按顺序存储在数组里，并通过二分法降低查找元素的时间复杂度。
1. 虽然contents 数组申明成了int8_t类型，但contents数组中具体存储什么类型完全取决于encoding变量的值，类似于继承。它可以保存具体类型为int16_t、int32_t 或者int64_t 的整数值。

> #### 元素升级
>
> 当新增的元素类型比原集合元素类型的长度要大时(比如：原来是int16_t，现在新增一个int64_t的元素)，需要对整数集合进行升级，才能将新元素放入整数集合中。具体步骤：
>
> 1、根据新元素类型，扩展整数集合底层数组的大小，并为新元素分配空间。
>
> 2、将底层数组现有的所有元素都转成与新元素相同类型的元素，并将转换后的元素放到正确的位置，放置过程中，维持整个元素顺序都是有序的。
>
> 3、将新元素添加到整数集合中（保证有序）
>
> **升级能极大地节省内存；整数集合不支持降级操作，一旦对数组进行了升级，编码就会一直保持升级后的状态**



#### 跳表（Skip List）

**跳表是在链表基础上改进过来的，实现了一种「多层」的有序链表**

![img](assets/cdc14698f629c74bf5a239cc8a611aeb.png)

「跳表节点」以及「跳表」的数据结构代码如下：

```c++
typedef struct zskiplistNode {
    //Zset 对象的元素值
    sds ele;
    //元素权重值
    double score;
    //后向指针
    struct zskiplistNode *backward;
  
    //节点的level数组，保存每层上的前向指针和跨度
    struct zskiplistLevel {
        struct zskiplistNode *forward;
        unsigned long span;
    } level[];
} zskiplistNode;

typedef struct zskiplist {
    struct zskiplistNode *header, *tail;
    unsigned long length;
    int level;
} zskiplist;
```

整体上的查询类似于二分查找的效果：

- 如果当前节点的权重「小于」要查找的权重时，跳表就会访问该层上的下一个节点。
- 如果当前节点的权重「等于」要查找的权重时，并且当前节点的 SDS 类型数据「小于」要查找的数据时，跳表就会访问该层上的下一个节点。

跳表一个比较明显的特征就是，相邻两层之间的节点数量比例近似为1:2



##### 那怎样才能维持相邻两层的节点数量的比例为 2 : 1 呢？

如果采用新增节点或者删除节点时，来调整跳表节点以维持比例的方法的话，会带来额外的开销。

Redis 则采用一种巧妙的方法是，**跳表在创建节点的时候，随机生成每个节点的层数**，并没有严格维持相邻两层的节点数量比例为 2 : 1 的情况。

具体的做法是，**跳表在创建节点时候，会生成范围为[0-1]的一个随机数，如果这个随机数小于 0.25（相当于概率 25%），那么层数就增加 1 层，然后继续生成下一个随机数，直到随机数的结果大于 0.25 结束，最终确定该节点的层数**。

这样的做法，相当于每增加一层的概率不超过 25%，层数越高，概率越低，层高最大限制是 64。



##### 为什么Zset用跳表而不是平衡树

主要是从内存占用、对范围查找的支持、实现难易程度这三方面总结的原因：

- **从内存占用上来比较，跳表比平衡树更灵活一些**。平衡树每个节点包含 2 个指针（分别指向左右子树），而跳表每个节点包含的指针数目平均为 1/(1-p)，具体取决于参数 p 的大小。如果像 Redis里的实现一样，取 p=1/4，那么平均每个节点包含 1.33 个指针，比平衡树更有优势。
- **在做范围查找的时候，跳表比平衡树操作要简单**。在平衡树上，我们找到指定范围的小值之后，还需要以中序遍历的顺序继续寻找其它不超过大值的节点。如果不对平衡树进行一定的改造，这里的中序遍历并不容易实现。而在跳表上进行范围查找就非常简单，只需要在找到小值之后，对第 1 层链表进行若干步的遍历就可以实现。
- **从算法实现难度上来比较，跳表比平衡树要简单得多**。平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而跳表的插入和删除只需要修改相邻节点的指针，操作简单又快速。



#### 压缩表（Zip List）

ziplist是一个经过特殊编码的双向链表，它的设计目标就是为了提高存储效率。ziplist可以用于存储字符串或整数，其中整数是按真正的二进制表示进行编码的，而不是编码成字符串序列。

Redis对外暴露的hash数据类型，在field比较少，各个value值也比较小的时候，hash采用ziplist来实现；而随着field增多和value值增大，hash可能会变成dict来实现。当hash底层变成dict来实现的时候，它的存储效率就没法跟那些序列化方式相比了。

![img](assets/a3b1f6235cf0587115b21312fe60289c.png)

> ZipList结构说明：
>
> 1. zlbytes: 表示整个ziplist占用的字节总数。
> 2. zltail：表示ziplist表中最后一项（entry）在ziplist中的偏移字节数。
> 3. zllen：16bit，表示ziplist中数据项（entry）的个数。当ziplist里数据大于2^16-1后，再获取元素个数时，ziplist从头到尾遍历。
> 4. entry：表示真正存放数据的数据项，长度不定，采用变长编码(对于大的数据，就多用一些字节来存储，而对于小的数据，就少用一些字节来存储)。
> 5. zlend: ziplist最后1个字节，是一个结束标记，值固定等于255。
>
> Entry结构说明：
>
> 1. Prevlen: 表示前一个数据项占用的总字节数。作用是为了让ziplist能够从后向前遍历（从后一项的位置，只需向前偏移prevrawlen个字节，就找到了前一项）,这个字段采用变长编码。
> 2. encoding：记录了当前节点实际数据的「类型和长度」，类型主要有两种：字符串和整数
> 3. data：存储的数据，类型和长度都由encoding决定。

##### 压缩表的特点

1. 内存空间连续：ziplist为了提高存储效率，从存储结构上看ziplist更像是一个表(list)，但不是一个链表(linkedlist)。ziplist将每一项数据存放在前后连续的地址空间内，一个ziplist整体占用一大块内存。而普通的双向链表每一项都占用独立的一块内存，各项之间用指针连接，这样会带来大量内存碎片，而且指针也会占用额外内存。

2. 查询元素：查找指定的数据项就会性能变得很低，需要进行遍历整个zipList。

3. 插入和修改：每次插入或修改引发的重新分配内存(realloc)操作会有更大的概率造成内存拷贝，从而降低性能。跟list一样，一旦发生内存拷贝，内存拷贝的成本也相应增加，因为要拷贝更大的一块数据。



#### 快速列表（Quick List）

在Redis 3.2之后，List的实现就从双向链表变成了快速列表，其本质是「双向链表+压缩列表」。Redis对外暴露的list数据类型，它底层实现所依赖的内部数据结构就是quicklist

> #### Redis对外暴露的上层list数据类型，它支持的一些修改操作如下：
> 1. lpush: 在左侧（即列表头部）插入数据。
> 2. lpop: 在左侧（即列表头部）删除数据。
> 3. rpop: 在右侧（即列表尾部）删除数据。
> 4. rpush: 在右侧（即列表尾部）插入数据。
>
> #### list也支持在任意中间位置的存取操作
>
> - 比如lindex和linsert，但它们都需要对list进行遍历，所以时间复杂度较高，为O(N)。
>
> #### list具有的特点：list的内部实现quicklist正是一个双向链表。
> - 它是一个能维持数据项先后顺序的列表（各个数据项的先后顺序由插入位置决定），便于在表的两端追加和删除数据，而对于中间位置的存取具有O(N)的时间复杂度。这正是一个双向链表所具有的特点。

##### Quick List的结构

quicklist双向链表是由多个节点（Node）组成，而quicklist的每个节点又是一个ziplist

![img](assets/b76b4b7c93d7398a99f5eea385a3e4d4.png)

之所以这样设计有下面几个原因：

- 双向链表便于在表的进行插入和删除节点操作，但是它的内存开销比较大。首先，它在每个节点上除了要保存数据之外，还要额外保存两个指针；其次，双向链表的各个节点是单独的内存块，地址不连续，节点多了容易产生内存碎片。

- ziplist由于是一整块连续内存，所以存储效率很高。但是，它不利于修改操作，每次数据变动都会引发一次内存的内存重新分配(realloc)。特别是当ziplist长度很长的时候，一次realloc可能会导致大批量的数据拷贝，进一步降低性能。

可见，一个quicklist节点上的ziplist要保持一个合理的长度。那到底多长合理呢？这可能取决于具体应用场景。实际上，Redis提供了一个配置参数**`list-max-ziplist-size`**，就是为了让使用者可以来根据自己的情况进行调整：

> - 当取正值的时候，表示按照数据项个数来限定每个quicklist节点上的ziplist长度。比如，当这个参数配置成5的时候，表示每个quicklist节点的ziplist最多包含5个数据项。
>
> - 当取负值的时候，表示按照占用字节数来限定每个quicklist节点上的ziplist长度。这时，它只能取-1到-5这五个值。每个值得含义如下:
>   - -5: 每个quicklist节点上的ziplist大小不能超过64 Kb。（注：1kb => 1024 bytes）
>   - -4: 每个quicklist节点上的ziplist大小不能超过32 Kb。
>   - -3: 每个quicklist节点上的ziplist大小不能超过16 Kb。
>   - -2: 每个quicklist节点上的ziplist大小不能超过8 Kb。（-2是Redis给出的默认值）
>   - -1: 每个quicklist节点上的ziplist大小不能超过4 Kb。



#### 链包（List Pack）

listpack压缩列表。作为ziplist的替代品，从2017年引入Redis后，到redis7.0已经完全取代ziplist作为redis底层存储数据结构之一。
相对于ziplist，listpack内存更紧凑，实现更简洁

![img](assets/1719035634415-c436d60e-58a7-4dfc-9e69-db8e2f96d19c.png)

listpack相较于压缩列表，其每一个entry都不再包含前一个节点的长度，而len=encoding+data的长度，这样做的好处就是当我们新加入一个节点的时候，不会影响其他节点保存的长度字段，从而避免了压缩表可能遇到的连锁更新问题



## Redis是单线程吗

Redis的单线程指的是**「接收客户端请求->解析请求 ->进行数据读写等操作->发送数据给客户端」这个过程是由一个线程（主线程）来完成的**，然而在内存释放、处理关闭文件、AOF刷盘的时候由子线程负责

尽管Redis使用单线程进行事务处理，但是由于使用IO多路复用、没有进程切换的开销等原因，其吞吐量可以达到10w/s

在Redis6.0之后，为了提高网络 I/O 的并行度，Redis 6.0 对于网络 I/O 采用多线程来处理。**但是对于命令的执行，Redis 仍然使用单线程来处理**



## 如何实现持久化

三种方法：

* AOF日志：每执行一条写操作命令，就把该命令以追加的方式写入到一个文件里
* RDB快照：将某一时刻的内存数据，以二进制的方式写入磁盘
* 混合持久方法：Redis 4.0 新增的方式，集成了 AOF 和 RBD 的优点

### AOF日志

Redis 在执行完一条写操作命令后，就会把该命令以追加的方式写入到一个文件里，然后 Redis 重启时，会读取该文件记录的命令，然后逐一执行命令的方式来进行数据恢复，这就是AOF（Append Only File）持久化功能，**只有写操作才会被记录，读操作不会**

优点：

* **避免额外的检查开销**：因为如果先将写操作命令记录到 AOF 日志里，再执行该命令的话，如果当前的命令语法有问题，那么如果不进行命令语法检查，该错误的命令记录到 AOF 日志里后，Redis 在使用日志恢复数据时，就可能会出错
* **不会阻塞当前写操作命令的执行**：因为当写操作命令执行成功后，才会将命令记录到 AOF 日志

缺点：

* **数据可能会丢失：** 执行写操作命令和记录日志是两个过程，那当 Redis 在还没来得及将命令写入到硬盘时，服务器发生宕机了，这个数据就会有丢失的风险
* **可能阻塞其他操作：** 由于写操作命令执行成功后才记录到 AOF 日志，所以不会阻塞当前命令的执行，但因为 AOF 日志也是在主线程中执行，所以当 Redis 把日志文件写入磁盘的时候，还是会阻塞后续的操作无法执行

其中写回的策略是可自定义的，分别为：同步写回、每秒写回、os自行判断写回

> 此外为了防止历史记录过多，降低效率，AOF还有一个重写机制

创建一个**子进程**来重写AOF，将无效的历史记录舍去，全部记录完后，将旧的AOF日志覆盖



#### AOF重写机制

Redis为了避免AOF文件过大引入了重写机制，当AOF文件大小超过某个阈值，就会启用该机制来压缩AOF文件

> 启用重写机制前，如果以此执行了```set name 金子官```和```set name jinziguan```，那么这两条都会被写进AOF
>
> 启用重写机制后，只有```set name jinziguan```会被写入AOF

这里需要强调，重写AOF的时候不直接复用原本的AOF，而是写到新的AOF后再删除原本的AOF，这是为了防止在AOF重写的过程中如果发生了崩溃，原本的AOF文件会被污染



**AOF的后台重写**

由于在触发AOF重写的时候，需要重新读取所有的键值对，重新生成一条指令然后写入新的AOF，是一个比较耗时的操作，所以Redis 的**重写 AOF 过程是由后台子进程 bgrewriteaof来完成的**

这有两个好处：

- AOF重写期间不会影响主进程的操作
- 由于重写是有子进程来实现的，不会涉及到与主进程的线程安全问题，避免了加锁解锁等开销（因为父子进程的数据是拷贝关系，数据副本是独立的）

在子进程进行重写的过程中，主进程主要会做三个工作：

1. 执行客户端发来的命令
2. 将执行后的命令写入「AOF缓冲区」
3. 将执行后的命令写入「AOF重写缓冲区」

子进程完成重写之后，会向主进程发一条信号，主进程收到后会：

1. 将AOF重写区缓冲的所有内容加入新的AOF文件中，使新旧两个AOF文件所保存的数据库状态一致
2. 让新的AOF文件覆盖旧的AOF文件



### RDB快照

因为 AOF 日志记录的是操作命令，不是实际的数据，所以用 AOF 方法做故障恢复时，需要全量把日志都执行一遍，一旦 AOF 日志非常多，势必会造成 Redis 的恢复操作缓慢。为了解决这个问题，Redis 增加了 RDB 快照

**RDB 快照就是记录某一个瞬间的内存数据，记录的是实际数据，而 AOF 文件记录的是命令操作的日志，而不是实际的数据**

> 快照是否会阻塞主进程

Redis 提供了两个命令来生成 RDB 文件，分别是 save 和 bgsave，他们的区别就在于是否在「主线程」里执行：

- 执行了 save 命令，就会在主线程生成 RDB 文件，由于和执行操作命令在同一个线程，所以如果写入 RDB 文件的时间太长，**会阻塞主线程**
- 执行了 bgsave 命令，会创建一个子进程来生成 RDB 文件，这样可以**避免主线程的阻塞**

Redis 的快照是**全量快照**，也就是说每次执行快照，都是把内存中的「所有数据」都记录到磁盘中。所以执行快照是一个比较重的操作，如果频率太频繁，可能会对 Redis 性能产生影响。如果频率太低，服务器故障时，丢失的数据会更多

在实际使用中，需要避免在写比较多的场景开启RDB快照



### AOF+RDB混合模式

顾名思义，两者混合实现redis的数据持久化，具体操作为：让AOF文件的前半部分为RDB，后半部分为AOF格式的增量数据



### 大Key对日志的影响

AOF写入磁盘的时候本质上调用了一个fsync()函数，不同的写回策略的调用时机不同

Redis 提供了 3 种 AOF 日志写回硬盘的策略，分别是:

1. Always，这个单词的意思是「总是」，所以它的意思是每次写操作命令执行完后，同步将 AOF 日志数据写回硬盘;

2. Everysec，这个单词的意思是「每秒」，所以它的意思是每次写操作命令执行完后，先将命令写入到AOF 文件的内核缓冲区，然后每隔一秒将缓冲区里的内容写回到硬盘;
3. No，意味着不由 Redis 控制写回硬盘的时机，转交给操作系统控制写回的时机，也就是每次写操作命令执行完后，先将命令写入到 AOF 文件的内核缓冲区，再由操作系统决定何时将缓冲区内容写回硬盘。

在使用 Always 策略的时候，主线程在执行完命令后，会把数据写入到 AOF 日志文件，然后会调用fsync() 函数，将内核缓冲区的数据直接写入到硬盘，等到硬盘写操作完成后，该函数才会返回。

**当使用 Always 策略的时候，如果写入是一个大Key，主线程在执行 fsync() 函数的时候，阻塞的时间会比较久，因为当写入的数据量很大的时候，数据同步到硬盘这个过程是很耗时的。**

当使用 Everysec 策略的时候，由于是异步执行 fsync() 函数，所以大 Key 持久化的过程（数据同步磁盘）不会影响主线程，

当使用 No 策略的时候，由于永不执行 fsync() 函数，所以大 Key 持久化的过程不会影响主线程



## Redis的大Key问题

**什么是大Key问题**

**通俗易懂的讲，Big Key就是某个key对应的value很大，占用的redis空间很大，本质上是大value问题。**key往往是程序可以自行设置的，value往往不受程序控制，因此可能导致value很大。

redis中这些Big Key对应的value值很大，在序列化/反序列化过程中花费的时间很大，因此当我们操作Big Key时，通常比较耗时，这就可能导致redis发生阻塞，从而降低redis性能。

用几个实际的例子对大Key的特征进行描述：

● 一个String类型的Key，它的值为5MB（数据过大）；
● 一个List类型的Key，它的列表数量为20000个（列表数量过多）；
● 一个ZSet类型的Key，它的成员数量为10000个（成员数量过多）；
● 一个Hash格式的Key，它的成员数量虽然只有1000个但这些成员的value总大小为100MB（成员体积过大）；

**大Key产生的场景**

1. redis数据结构使用不恰当

   将Redis用在并不适合其能力的场景，造成Key的value过大，如使用String类型的Key存放大体积二进制文件型数据。

2. 未及时清理垃圾数据

   没有对无效数据进行定期清理，造成如HASH类型Key中的成员持续不断的增加。即一直往value塞数据，却没有删除机制，value只会越来越大。

3. 对业务预估不准确

   业务上线前规划设计考虑不足没有对Key中的成员进行合理的拆分，造成个别Key中的成员数量过多。

4. 明星、网红的粉丝列表、某条热点新闻的评论列表

   假设我们使用List数据结构保存某个明星/网红的粉丝，或者保存热点新闻的评论列表，因为粉丝数量巨大，热点新闻因为点击率、评论数会很多，这样List集合中存放的元素就会很多，可能导致value过大，进而产生Big Key问题。

**大Key的危害**

1. 阻塞请求

   Big Key对应的value较大，我们对其进行读写的时候，需要耗费较长的时间，这样就可能阻塞后续的请求处理。Redis的核心线程是单线程，单线程中请求任务的处理是串行的，前面的任务完不成，后面的任务就处理不了。

2. 内存增大

   读取Big Key耗费的内存比正常Key会有所增大，如果不断变大，可能会引发OOM（内存溢出），或达到redis的最大内存maxmemory设置值引发写阻塞或重要Key被逐出。

3. 阻塞网络

   读取单value较大时会占用服务器网卡较多带宽，自身变慢的同时可能会影响该服务器上的其他Redis实例或者应用。

4. 影响主从同步、主从切换

   删除一个大Key造成主库较长时间的阻塞并引发同步中断或主从切换。

**如何识别大Key**

1. 使用redis自带的命令识别

   例如可以使用Redis官方客户端redis-cli加上--bigkeys参数，可以找到某个实例5种数据类型(String、hash、list、set、zset)的最大key。

   优点是可以在线扫描，不阻塞服务；缺点是信息较少，内容不够精确。

2. 使用debug object key命令

   根据传入的对象（Key的名称）来对Key进行分析并返回大量数据，其中serializedlength的值为该Key的序列化长度，需要注意的是，Key的序列化长度并不等同于它在内存空间中的真实长度，此外，debug object属于调试命令，运行代价较大，并且在其运行时，进入Redis的其余请求将会被阻塞直到其执行完毕。并且每次只能查找单个key的信息，官方不推荐使用。

3. redis-rdb-tools开源工具

   这种方式是在redis实例上执行bgsave，bgsave会触发redis的快照备份，生成rdb持久化文件，然后对dump出来的rdb文件进行分析，找到其中的大key。

   GitHub地址：https://github.com/sripathikrishnan/redis-rdb-tools

   优点在于获取的key信息详细、可选参数多、支持定制化需求，结果信息可选择json或csv格式，后续处理方便，其缺点是需要离线操作，获取结果时间较长。

**如何解决大Key**

1. 对大Key进行拆分

   将一个Big Key拆分为多个key-value这样的小Key，并确保每个key的成员数量或者大小在合理范围内，然后再进行存储，通过get不同的key或者使用mget批量获取。

2. 对大Key进行清理

   对Redis中的大Key进行清理，从Redis中删除此类数据。Redis自4.0起提供了UNLINK命令，该命令能够以非阻塞的方式缓慢逐步的清理传入的Key，通过UNLINK，你可以安全的删除大Key甚至特大Key。

3. 监控Redis的内存、网络带宽、超时等指标

   通过监控系统并设置合理的Redis内存报警阈值来提醒我们此时可能有大Key正在产生，如：Redis内存使用率超过70%，Redis内存1小时内增长率超过20%等。

4. 定期清理失效数据

   如果某个Key有业务不断以增量方式写入大量的数据，并且忽略了其时效性，这样会导致大量的失效数据堆积。可以通过定时任务的方式，对失效数据进行清理。

5. 压缩value

   使用序列化、压缩算法将key的大小控制在合理范围内，但是需要注意序列化、反序列化都会带来一定的消耗。如果压缩后，value还是很大，那么可以进一步对key进行拆分。



## Redis的热Key问题

**什么是热Key问题**

在Redis中，我们把访问频率高的Key，称为热Key。比如突然又几十万的请求去访问redis中某个特定的Key，那么这样会造成redis服务器短时间流量过于集中，很可能导致redis的服务器宕机。那么接下来对这个Key的请求，都会直接请求到我们的后端数据库中，数据库性能本来就不高，这样就可能直接压垮数据库，进而导致后端服务不可用

**热Key产生的场景**

1. 用户消费的数据远大于生产的数据，如商品秒杀、热点新闻、热点评论等读多写少的场景。

   双十一秒杀商品，短时间内某个爆款商品可能被点击/购买上百万次，或者某条爆炸性新闻等被大量浏览，此时会造成一个较大的请求Redis量，这种情况下就会造成热点Key问题。

2. 请求分片集中，超过单台Redis服务器的性能极限。

   在服务端读数据进行访问时，往往会对数据进行分片切分，例如采用固定Hash分片，hash落入同一台redis服务器，如果瞬间访问量过大，超过机器瓶颈时，就会导致热点 Key 问题的产生。

**热Key的危害**

缓存击穿，压垮redis服务器，导致大量请求直接发往后端服务，并且DB本身性能较弱，很可能进一步导致后端服务雪崩

**如何识别热Key**

1. 凭借个人经验，结合业务场景，判断哪些是热Key

   比如，双十一大促的时候，苹果手机正在秒杀，那么我们可以判断苹果手机这个sku就是热Key。

2. 使用redis之前，在客户端写程序统计上报

   修改我们的业务代码，在操作redis之前，加入Key使用次数的统计逻辑，定时把收集到的数据上报到统一的服务进行聚合计算，这样我们就可以找到那些热点Key。缺点就是对我们的业务代码有一定的侵入性。

3. 服务代理层上报

   这个要看具体公司redis集群架构是怎么样的，如果是在redis前面有一个代理层，那么我们可以在代理层进行收集上报，也是可以找到热点Key。

3. 使用redis自带的命令

   例如monitor、redis-cli加上--hotkeys选项等，不过这种方式执行起来很慢，可能会降低redis的处理请求的性能，慎用。

   monitor命令：可以实时抓取出redis服务器接收到的命令，然后写代码统计出热Key，也有现成的分析工具可以使用。

4. redis节点抓包分析

   自己写程序监听端口，解析数据，进行分析。

**如何解决热Key**

1. **Redis集群扩容：增加分片副本，分摊客户端发过来的读请求**
2. **使用二级缓存**，本地缓存拿不到再去redis拿



## Redis集群

### 如何实现服务高可用

有三种方法：主从模式、哨兵模式、切片集群

#### 主从模式

一台服务器为主，多台服务器为从，主服务器可以将数据同步到从服务器上。其中只有主服务器可以支持读写操作，从服务器只支持读操作

值得一提的是，主从服务器的命令复制操作是**异步的**，主服务器收到新的写命令后，会发送给从服务器。但是，主服务器并不会等到从服务器实际执行完命令后，再把结果返回给客户端，而是主服务器自己在本地执行完命令后，就会向客户端返回结果了。如果从服务器还没有执行主服务器同步过来的命令，主从服务器间的数据就不一致了

因此无法实现强一致性保证（主从数据时时刻刻保持一致），数据不一致是难以避免的

#### 主从复制是如何实现的

##### 第一次同步

主从服务器的第一次同步可以分为三个阶段：

1. 第一阶段是建立链接、协商同步
2. 第二阶段是主服务器同步数据给从服务器（执行bgsave，期间新的写操作将会写入replication buffer缓冲区）
3. 第三阶段是主服务器发送新写操作命令给从服务器

##### 命令传播

主从服务器完成第一次同步之后，会保持网络连接

主服务器收到写命令后会传给从服务器，从服务器执行，以此实现主从一致性

##### 分担主服务器压力

可以让部分从服务器承担传递写命令的任务，但是响应客户端依然是在主服务器执行的

##### 增量复制

主从服务器因为网络原因断开了链接，重新连接后需要同步数据，这个时候主服务器会把断链期间的数据传送给从服务器

> #### 主服务器怎么知道要传那些数据呢
>
> 答案藏在这两个东西里:
>
> - rep_backlog_buffer，是一个「环形」缓冲区，用于主从服务器断连后，从中找到差异的数据
> - replication offset，标记上面那个缓冲区的同步进度，主从服务器都有各自的偏移量，主服务器使用master_repl_offset 来记录自己「写」到的位置，从服务器使用 slave_repl_offset 来记录自己「读」到的位置。
>
> #### 缓冲区是什么时候写入的
>
> 在主服务器进行命令传播时，不仅会将写命令发送给从服务器，还会将写命令写入到replbacklog.buffer 缓冲区里，因此 这个缓冲区里会保存着最近传播的写命令。
>
> 网络断开后，当从服务器重新连上主服务器时，从服务器会通过 psync 命令将自己的复制偏移量slave_repl_offset 发送给主服务器，主服务器根据自己的 master_repl_offset 和 slave_repl_offset 之间的差距，然后来决定对从服务器执行哪种同步操作:
>
> - 如果判断出从服务器要读取的数据还在repl backlog_buffer 缓冲区里，那么主服务器将采用增量同步的方式;
>
> - 相反，如果判断出从服务器要读取的数据已经不存在repl_backlog_buffer 缓冲区里，那么主服务器将采用全量同步的方式。
>
> 当主服务器在 repl_backlog_buffer 中找到主从服务器差异(增量)的数据后，就会将增量的数据写入到replication buffer 缓冲区，这个缓冲区我们前面也提到过，它是缓存将要传播给从服务器的命令。



####  redis主从和集群可以保证数据一致性吗 ？

redis 主从和集群在CAP理论都属于AP模型，即在面临网络分区时选择保证可用性和分区容忍性，而牺牲了强一致性。这意味着在网络分区的情况下，Redis主从复制和集群可以继续提供服务并保持可用，但可能会出现部分节点之间的数据不一致。



#### 哨兵模式

在使用 Redis 主从服务的时候，会有一个问题，就是当 Redis 的主服务器出现故障宕机时，需要手动进行恢复

为了解决这个问题，Redis 增加了哨兵模式（**Redis Sentinel**），因为哨兵模式做到了可以监控主从服务器，并且提供**主从节点故障转移的功能**——它会监测主节点是否存活，如果发现主节点挂了，它就会选举一个从节点切换为主节点，并且把新主节点的相关信息通知给从节点和客户端

哨兵节点主要负责三件事情：**监控、选主、通知**



##### 如何判断主节点真的故障了

哨兵会每隔 1 秒给所有主从节点发送 PING 命令，当主从节点收到 PING 命令后，会发送一个响应命令给哨兵，这样就可以判断它们是否在正常运行，如果主节点或者从节点没有在规定的时间内响应哨兵的 PING 命令，哨兵就会将它们标记为「**主观下线**」。之所以是主观下线，是因为有可能主节点只是系统压力比较大或者网络发送了拥塞，导致没有在规定时间内响应哨兵的 PING 命令

所以，为了减少误判的情况，哨兵在部署的时候不会只部署一个节点，而是用多个节点部署成**哨兵集群**（*最少需要三台机器来部署哨兵集群*），**通过多个哨兵节点一起判断，就可以就可以避免单个哨兵因为自身网络状况不好，而误判主节点下线的情况**。同时，多个哨兵的网络同时不稳定的概率较小，由它们一起做决策，误判率也能降低



##### 具体是怎么判定主节点为「客观下线」的呢

当一个哨兵判断主节点为「主观下线」后，就会向其他哨兵发起命令，其他哨兵收到这个命令后，就会根据自身和主节点的网络状况，做出赞成投票或者拒绝投票的响应，哨兵判断完主节点客观下线后，哨兵就要开始在多个「从节点」中，选出一个从节点来做新主节点

构成客观下线的条件：

- 哨兵节点收到的赞成票数量达到哨兵配置文件中的 quorum 配置项设定的值



##### 由哪个哨兵进行主从故障转移

哪个哨兵节点判断主节点为「客观下线」，这个哨兵节点就是候选者，所谓的候选者就是想当 Leader 的哨兵，与此同时，候选者会向其他哨兵发送命令，表明希望成为 Leader 来执行主从切换，并让所有其他哨兵对它进行投票。每个哨兵只有一次投票机会，如果用完后就不能参与投票了，可以投给自己或投给别人，但是只有候选者才能把票投给自己

在投票过程中，任何一个「候选者」，要满足两个条件：

- 第一，拿到半数以上的赞成票
- 第二，拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值

这也就是为什么哨兵至少要有3个



##### 主从故障转移的过程是怎样的

主从故障转移操作包含以下四个步骤：

- 第一步：在已下线主节点（旧主节点）属下的所有「从节点」里面，挑选出一个从节点，并将其转换为主节点。

  这一步会对从节点进行三轮考察：**优先级、复制进度、ID号**

  - 第一轮考察：哨兵首先会根据从节点的优先级来进行排序，优先级越小排名越靠前
  - 第二轮考察：如果优先级相同，则查看复制的下标，哪个从「主节点」接收的复制数据多，哪个靠前
  - 第三轮考察：如果优先级和下标都相同，就选择从节点 ID 较小的那个

- 第二步：让已下线主节点属下的所有「从节点」修改复制目标，修改为复制「新主节点」；

- 第三步：将新主节点的 IP 地址和信息，通过「发布者/订阅者机制」通知给客户端；

- 第四步：继续监视旧主节点，当这个旧主节点重新上线时，将它设置为新主节点的从节点；



##### 哨兵集群是如何组成的

通过Redis的发布/订阅模式



### 集群脑裂

由于网络问题，集群节点之间失去联系。主从数据不同步；重新平衡选举，产生两个主服务。等网络恢复，旧主节点会降级为从节点，再与新主节点进行同步复制的时候，由于会从节点会清空自己的缓冲区，所以导致之前客户端写入的数据丢失了

> 解决方案

当主节点发现从节点下线或者通信超时的总数量小于阈值时，那么禁止主节点进行写数据，直接把错误返回给客户端



## Redis过期删除与内存淘汰

每当我们对一个 key 设置了过期时间时，Redis 会把该 key 带上过期时间存储到一个**过期字典**（expires dict）中，也就是说「过期字典」保存了数据库中所有 key 的过期时间。

当我们查询一个 key 时，Redis 首先检查该 key 是否存在于过期字典中：

- 如果不在，则正常读取键值；
- 如果存在，则会获取该 key 的过期时间，然后与当前系统时间进行比对，如果比系统时间大，那就没有过期，否则判定该 key 已过期。

Redis 使用的过期删除策略是「**惰性删除+定期删除**」这两种策略配和使用



### 惰性删除

惰性删除策略的做法是，**不主动删除过期键，每次从数据库访问 key 时，都检测 key 是否过期，如果过期则删除该 key**

惰性删除策略的**优点**：

- 因为每次访问时，才会检查 key 是否过期，所以此策略只会使用很少的系统资源，因此，惰性删除策略对 CPU 时间最友好。

惰性删除策略的**缺点**：

- 如果一个 key 已经过期，而这个 key 又仍然保留在数据库中，那么只要这个过期 key 一直没有被访问，它所占用的内存就不会释放，造成了一定的内存空间浪费。所以，惰性删除策略对内存不友好。



### 定期删除

定期删除策略的做法是，**每隔一段时间「随机」从数据库中取出一定数量的 key 进行检查，并删除其中的过期key。**

Redis 的定期删除的流程：

1. 从过期字典中随机抽取 20 个 key；
2. 检查这 20 个 key 是否过期，并删除已过期的 key；
3. 如果本轮检查的已过期 key 的数量，超过 5 个（20/4），也就是「已过期 key 的数量」占比「随机抽取 key 的数量」大于 25%，则继续重复步骤 1；如果已过期的 key 比例小于 25%，则停止继续删除过期 key，然后等待下一轮再检查。

定期删除策略的**优点**：

- 通过限制删除操作执行的时长和频率，来减少删除操作对 CPU 的影响，同时也能删除一部分过期的数据减少了过期键对空间的无效占用。

定期删除策略的**缺点**：

- 难以确定删除操作执行的时长和频率。如果执行的太频繁，就会对 CPU 不友好；如果执行的太少，那又和惰性删除一样了，过期 key 占用的内存不会及时得到释放。



![img](assets/过期删除策略.jpg)

可以看到，惰性删除策略和定期删除策略都有各自的优点，所以 **Redis 选择「惰性删除+定期删除」这两种策略配和使用**，以求在合理使用 CPU 时间和避免内存浪费之间取得平衡。



## Redis内存淘汰策略

### 不进行数据淘汰策略

内存满了不淘汰任何数据，但是会禁止写入

### 进行数据淘汰策略

可以分为「在已经过期的数据中淘汰」和「在所有数据中淘汰」，其中细分可以分为随机、lru、lfu、ttl之类的

![img](assets/内存淘汰策略-20240826141035852.jpg)



## Redis缓存

### 缓存雪崩

通常我们为了保证缓存中的数据与数据库中的数据一致性，会给 Redis 里的数据设置过期时间，当缓存数据过期后，用户访问的数据如果不在缓存里，业务系统需要重新生成缓存，因此就会访问数据库，并将数据更新到 Redis 里，这样后续请求都可以直接命中缓存

但如果，**大量缓存数据在同一时间过期（失效）或者 Redis 故障宕机**，恰在此时有大量的用户请求，都无法在 Redis 中处理，于是全部请求都直接访问数据库，从而导致数据库的压力骤增，严重的会造成数据库宕机，从而形成一系列连锁反应，造成整个系统崩溃，这就是**缓存雪崩**的问题

简单来说，雪崩有两个原因：

* 大量数据同时过期
* Redis 故障宕机

不同的原因，应对策略不同

#### 大量数据同时过期的应对措施

针对大量数据同时过期而引发的缓存雪崩问题，常见的应对方法有下面这几种：

- 均匀设置过期时间：

  如果要给缓存数据设置过期时间，应该避免将大量的数据设置成同一个过期时间。我们可以在对缓存数据设置过期时间时，**给这些数据的过期时间加上一个随机数**，这样就保证数据不会在同一时间过期

- 互斥锁：

  当业务线程在处理用户请求时，**如果发现访问的数据不在 Redis 里，就加个互斥锁，保证同一时间内只有一个请求来构建缓存**（从数据库读取数据，再将数据更新到 Redis 里），当缓存构建完成后，再释放锁。未能获取互斥锁的请求，要么等待锁释放后重新读取缓存，要么就返回空值或者默认值

  实现互斥锁的时候，最好设置**超时时间**，不然第一个请求拿到了锁，然后这个请求发生了某种意外而一直阻塞，一直不释放锁，这时其他请求也一直拿不到锁，整个系统就会出现无响应的现象

- 后台更新缓存：

  业务线程不再负责更新缓存，缓存也不设置有效期，而是**让缓存“永久有效”，并将更新缓存的工作交由后台线程定时更新**

  事实上，缓存数据不设置有效期，并不是意味着数据一直能在内存里，因为**当系统内存紧张的时候，有些缓存数据会被“淘汰”**，而在缓存被“淘汰”到下一次后台定时更新缓存的这段时间内，业务线程读取缓存失败就返回空值，业务的视角就以为是数据丢失了。

  解决上面的问题的方式有两种。

  第一种方式，后台线程不仅负责定时更新缓存，而且也负责**频繁地检测缓存是否有效**，检测到缓存失效了，原因可能是系统紧张而被淘汰的，于是就要马上从数据库读取数据，并更新到缓存。

  这种方式的检测时间间隔不能太长，太长也导致用户获取的数据是一个空值而不是真正的数据，所以检测的间隔最好是毫秒级的，但是总归是有个间隔时间，用户体验一般。

  第二种方式，在业务线程发现缓存数据失效后（缓存数据被淘汰），**通过消息队列发送一条消息通知后台线程更新缓存**，后台线程收到消息后，在更新缓存前可以判断缓存是否存在，存在就不执行更新缓存操作；不存在就读取数据库数据，并将数据加载到缓存。这种方式相比第一种方式缓存的更新会更及时，用户体验也比较好。

  在业务刚上线的时候，我们最好提前把数据缓起来，而不是等待用户访问才来触发缓存构建，这就是所谓的**缓存预热**，后台更新缓存的机制刚好也适合干这个事情

#### Redis故障宕机的应对措施

针对 Redis 故障宕机而引发的缓存雪崩问题，常见的应对方法有下面这几种：

- 服务熔断或请求限流机制：

  因为 Redis 故障宕机而导致缓存雪崩问题时，我们可以启动**服务熔断**机制，**暂停业务应用对缓存服务的访问，直接返回错误**，不用再继续访问数据库，从而降低对数据库的访问压力，保证数据库系统的正常运行，然后等到 Redis 恢复正常后，再允许业务应用访问缓存服务。

  服务熔断机制是保护数据库的正常允许，但是暂停了业务应用访问缓存服系统，全部业务都无法正常工作

  为了减少对业务的影响，我们可以启用**请求限流**机制，**只将少部分请求发送到数据库进行处理，再多的请求就在入口直接拒绝服务**，等到 Redis 恢复正常并把缓存预热完后，再解除请求限流的机制。

- 构建 Redis 缓存高可靠集群：

  服务熔断或请求限流机制是缓存雪崩发生后的应对方案，我们最好通过**主从节点的方式构建 Redis 缓存高可靠集群**。

  如果 Redis 缓存的主节点故障宕机，从节点可以切换成为主节点，继续提供缓存服务，避免了由于 Redis 故障宕机而导致的缓存雪崩问题。



### 缓存击穿

我们的业务通常会有几个数据会被频繁地访问，比如秒杀活动，这类被频地访问的数据被称为热点数据。

如果缓存中的**某个热点数据过期**了，此时大量的请求访问了该热点数据，就无法从缓存中读取，直接访问数据库，数据库很容易就被高并发的请求冲垮，这就是**缓存击穿**的问题。

可以发现缓存击穿跟缓存雪崩很相似，你可以认为缓存击穿是缓存雪崩的一个子集。

应对缓存击穿可以采取前面说到两种方案：

- 互斥锁方案，保证同一时间只有一个业务线程更新缓存，未能获取互斥锁的请求，要么等待锁释放后重新读取缓存，要么就返回空值或者默认值。
- 不给热点数据设置过期时间，由后台异步更新缓存，或者在热点数据准备要过期前，提前通知后台线程更新缓存以及重新设置过期时间；



### 缓存穿透

当发生缓存雪崩或击穿时，数据库中还是保存了应用要访问的数据，一旦缓存恢复相对应的数据，就可以减轻数据库的压力，而缓存穿透就不一样了。

当用户访问的数据，**既不在缓存中，也不在数据库中**，导致请求在访问缓存时，发现缓存缺失，再去访问数据库时，发现数据库中也没有要访问的数据，没办法构建缓存数据，来服务后续的请求。那么当有大量这样的请求到来时，数据库的压力骤增，这就是**缓存穿透**的问题。

缓存穿透的发生一般有这两种情况：

- 业务误操作，缓存中的数据和数据库中的数据都被误删除了，所以导致缓存和数据库中都没有数据；
- 黑客恶意攻击，故意大量访问某些读取不存在数据的业务；

应对缓存穿透的方案，常见的方案有三种。

- 第一种方案，非法请求的限制:

  当有大量恶意请求访问不存在的数据的时候，也会发生缓存穿透，因此在 API 入口处我们要判断求请求参数是否合理，请求参数是否含有非法值、请求字段是否存在，如果判断出是恶意请求就直接返回错误，避免进一步访问缓存和数据库。

- 第二种方案，缓存空值或者默认值:

  当我们线上业务发现缓存穿透的现象时，可以针对查询的数据，在缓存中设置一个空值或者默认值，这样后续请求就可以从缓存中读取到空值或者默认值，返回给应用，而不会继续查询数据库。

- 第三种方案，使用布隆过滤器快速判断数据是否存在，避免通过查询数据库来判断数据是否存在:

  我们可以在写入数据库数据时，使用布隆过滤器做个标记，然后在用户请求到来时，业务线程确认缓存失效后，可以通过查询布隆过滤器快速判断数据是否存在，如果不存在，就不用通过查询数据库来判断数据是否存在。

> ### 布隆过滤器
>
> 布隆过滤器由「初始值都为 0 的位图数组」和「 N 个哈希函数」两部分组成。当我们在写入数据库数据时，在布隆过滤器里做个标记，这样下次查询数据是否在数据库时，只需要查询布隆过滤器，如果查询到数据没有被标记，说明不在数据库中。
>
> 布隆过滤器会通过 3 个操作完成标记：
>
> - 第一步，使用 N 个哈希函数分别对数据做哈希计算，得到 N 个哈希值；
> - 第二步，将第一步得到的 N 个哈希值对位图数组的长度取模，得到每个哈希值在位图数组的对应位置。
> - 第三步，将每个哈希值在位图数组的对应位置的值设置为 1；
>
> 布隆过滤器一般只负责插入和查询，其无法保证删除的安全性，用布隆过滤器的原因是：**查询布隆过滤器说数据存在，并不一定证明数据库中存在这个数据，但是查询到数据不存在，数据库中一定就不存在这个数据**



## 数据库和缓存的一致性

直接上结论：

* 无论是先更新数据库还是先更新缓存，在并发的场景下都会出现数据不一致的问题
* 如果采用「删缓存+更新数据库」的策略，那么先删除再更新，在读写并发的场景中也会出现数据不一致
* **「先更新数据库 + 再删除缓存」的方案，是可以保证数据一致性的**，但是这样会对缓存的命中率造成大幅度的降低

所以，**如果我们的业务对缓存命中率有很高的要求，我们可以采用「更新数据库 + 更新缓存」的方案，因为更新缓存并不会出现缓存未命中的情况**

但是这个方案前面我们也分析过，在两个更新请求并发执行的时候，会出现数据不一致的问题，因为更新数据库和更新缓存这两个操作是独立的，而我们又没有对操作做任何并发控制，那么当两个线程并发更新它们的话，就会因为写入顺序的不同造成数据的不一致。

所以我们得增加一些手段来解决这个问题，这里提供两种做法：

- 在更新缓存前先加个**分布式锁**，保证同一时间只运行一个请求更新缓存，就会不会产生并发问题了，当然引入了锁后，对于写入的性能就会带来影响。
- 在更新完缓存时，给缓存加上较短的**过期时间**，这样即时出现缓存不一致的情况，缓存的数据也会很快过期，对业务还是能接受的。



### 如何保证「更新数据库」和「删除缓存」这两件事一定都能够成功执行呢

有两种方法：

- 重试机制：

  我们可以引入**消息队列**，将第二个操作（删除缓存）要操作的数据加入到消息队列，由消费者来操作数据。

  - 如果应用**删除缓存失败**，可以从消息队列中重新读取数据，然后再次删除缓存，这个就是**重试机制**。当然，如果重试超过的一定次数，还是没有成功，我们就需要向业务层发送报错信息了。
  - 如果**删除缓存成功**，就要把数据从消息队列中移除，避免重复操作，否则就继续重试。

- 订阅 MySQL binlog，再操作缓存：

  「**先更新数据库，再删缓存**」的策略的第一步是更新数据库，那么更新数据库成功，就会产生一条变更日志，记录在 binlog 里。

  于是我们就可以通过订阅 binlog 日志，拿到具体要操作的数据，然后再执行缓存删除



## Redis实战

### 如何用Redis实现分布式锁

分布式锁是用于分布式环境下并发控制的一种机制，用于控制某个资源在同一时刻只能被一个应用所使用。

Redis 的 SET 命令有个 NX 参数可以实现「key不存在才插入」，所以可以用它来实现分布式锁：

- 如果 key 不存在，则显示插入成功，可以用来表示加锁成功；
- 如果 key 存在，则会显示插入失败，可以用来表示加锁失败。

基于 Redis 节点实现分布式锁时，对于加锁操作，我们需要满足三个条件。

- 加锁包括了读取锁变量、检查锁变量值和设置锁变量值三个操作，但需要以原子操作的方式完成，所以，我们使用 SET 命令带上 NX 选项来实现加锁；
- 锁变量需要设置过期时间，以免客户端拿到锁后发生异常，导致锁一直无法释放，所以，我们在 SET 命令执行时加上 EX/PX 选项，设置其过期时间；
- 锁变量的值需要能区分来自不同客户端的加锁操作，以免在释放锁时，出现误释放操作，所以，我们使用 SET 命令设置锁变量值时，每个客户端设置的值是一个唯一值，用于标识客户端；

满足这三个条件的分布式命令如下：

```shell
SET lock_key unique_value NX PX 10000 
```

而解锁的过程就是将 lock_key 键删除（del lock_key），但不能乱删，要保证执行操作的客户端就是加锁的客户端。所以，解锁的时候，我们要先判断锁的 unique_value 是否为加锁客户端，是的话，才将 lock_key 键删除。

> #### 优缺点
>
> **优点**：
>
> 1. 性能高效（这是选择缓存实现分布式锁最核心的出发点）。
> 2. 实现方便。很多研发工程师选择使用 Redis 来实现分布式锁，很大成分上是因为 Redis 提供了 setnx 方法，实现分布式锁很方便。
> 3. 避免单点故障（因为 Redis 是跨集群部署的，自然就避免了单点故障）。
>
> **缺点**：
>
> - 超时时间不好设置
>
>   。如果锁的超时时间设置过长，会影响性能，如果设置的超时时间过短会保护不到共享资源。比如在有些场景中，一个线程 A 获取到了锁之后，由于业务代码执行时间可能比较长，导致超过了锁的超时时间，自动失效，注意 A 线程没执行完，后续线程 B 又意外的持有了锁，意味着可以操作共享资源，那么两个线程之间的共享资源就没办法进行保护了。
>
>   - **那么如何合理设置超时时间呢？** 我们可以基于续约的方式设置超时时间：先给锁设置一个超时时间，然后启动一个守护线程，让守护线程在一段时间后，重新设置这个锁的超时时间。实现方式就是：写一个守护线程，然后去判断锁的情况，当锁快失效的时候，再次进行续约加锁，当主线程执行完成后，销毁续约锁即可，不过这种方式实现起来相对复杂。
>
> - **Redis 主从复制模式中的数据是异步复制的，这样导致分布式锁的不可靠性**。如果在 Redis 主节点获取到锁后，在没有同步到其他节点时，Redis 主节点宕机了，此时新的 Redis 主节点依然可以获取锁，所以多个应用服务就可以同时获取到锁。



### 如何用Redis实现延迟队列

使用有序集合ZSet的```zadd score1 value1```就可以一直往内存中生产消息。再利用 ```zrangebyscore key min max withscores limit 0 1``` 查询符合条件的所有待处理的任务， 通过循环执行队列任务即可。

此外，可以用LUA脚本将获取消息和消费消息的操作放在一起执行，保证了原子性



# 操作系统

## 堆和栈

### 区别是什么

1. 管理方式

- **栈**：栈是由操作系统自动管理的，遵循后进先出（LIFO）的原则。当函数被调用时，其局部变量和函数参数会被压入栈中；当函数返回时，这些数据会从栈中弹出。栈的管理是自动的，无需程序员干预。
- **堆**：堆是由程序员手动管理的，用于存储动态分配的内存。程序员需要使用特定的函数（如C语言中的`malloc`和`free`，C++中的`new`和`delete`）来分配和释放堆内存。堆的管理需要程序员显式地进行，容易出现内存泄漏或悬挂指针等问题。

2. 分配和释放速度

- **栈**：栈的分配和释放速度非常快，因为它们只是简单的内存地址的增减操作。
- **堆**：堆的分配和释放速度相对较慢，因为需要进行复杂的内存管理操作，如查找合适的内存块、合并空闲块等。

3. 内存分配方式

- **栈**：栈上的内存分配是连续的，由操作系统自动完成。
- **堆**：堆上的内存分配是不连续的，可能会有碎片问题，需要更复杂的内存管理算法来处理。

4. 内存大小限制

- **栈**：栈的大小通常是有限的，由操作系统预先设定。如果栈空间用尽，会导致栈溢出（Stack Overflow）错误。
- **堆**：堆的大小通常较大，受限于系统的物理内存和虚拟内存大小。

5. 数据访问速度

- **栈**：栈上的数据访问速度较快，因为栈的内存布局是连续的，且访问局部变量和函数参数的地址是固定的。
- **堆**：堆上的数据访问速度相对较慢，因为堆内存的分配是不连续的，可能需要更多的内存访问操作。

6. 数据生命周期

- **栈**：栈上的数据生命周期由函数的调用和返回自动管理，数据在函数调用结束后自动释放。
- **堆**：堆上的数据生命周期由程序员控制，数据可以在多个函数调用之间共享，直到程序员显式释放。

### 为什么要区分堆和栈？

区分堆和栈的主要目的是为了更有效地管理内存，满足不同类型数据的存储需求：

1. **性能优化**：栈的自动管理和快速分配释放机制使得局部变量和函数参数的访问非常高效。堆的动态分配机制则适用于需要长时间存在或大小不确定的数据。
2. **灵活性**：堆内存的动态分配允许程序在运行时根据需要分配和释放内存，适用于复杂的数据结构和动态数据需求。
3. **内存管理**：栈的自动管理减少了程序员的负担，避免了内存泄漏等问题。堆的手动管理则提供了更大的灵活性，但也需要程序员更加谨慎地管理内存。



## 虚拟内存

如果没有虚拟地址——要想在内存中同时运行两个程序是不可能的。如果第一个程序在 2000 的位置写入一个新的值，将会擦掉第二个程序存放在相同位置上的所有内容，所以同时运行两个程序是根本行不通的，这两个程序会立刻崩溃

**操作系统会提供一种机制，将不同进程的虚拟地址和不同内存的物理地址映射起来。**

如果程序要访问虚拟地址的时候，由操作系统转换成不同的物理地址，这样不同的进程运行的时候，写入的是不同的物理地址，这样就不会冲突了。

于是，这里就引出了两种地址的概念：

- 我们程序所使用的内存地址叫做**虚拟内存地址**（*Virtual Memory Address*）
- 实际存在硬件里面的空间地址叫**物理内存地址**（*Physical Memory Address*）

当程序访问物理地址的时候，会首先经过内存管理单元（MMU）的映射将自己的虚拟内存地址转换为真实的物理内存地址



## 内存分段和内存分页

为了管理虚拟地址和物理地址之间的关系，人们先后想出了分段和分页两种方法

### 内存分段

程序是由若干个逻辑分段组成的，如可由代码分段、数据分段、栈段、堆段组成。**不同的段是有不同的属性的，所以就用分段（\*Segmentation\*）的形式把这些段分离出来。**

分段机制下的虚拟地址由两部分组成，**段选择因子**和**段内偏移量**。

分段的办法很好，解决了程序本身不需要关心具体的物理内存地址的问题，但它也有一些不足之处：

- 第一个就是**内存碎片**的问题：

  内存碎片主要分为，内部内存碎片和外部内存碎片。

  内存分段管理可以做到段根据实际需求分配内存，所以有多少需求就分配多大的段，所以**不会出现内部内存碎片**。

  但是由于每个段的长度不固定，所以多个段未必能恰好使用所有的内存空间，会产生了多个不连续的小物理内存，导致新的程序无法被装载，所以**会出现外部内存碎片**的问题。

  解决「外部内存碎片」的问题就是**内存交换**。

- 第二个就是**内存交换的效率低**的问题：

  对于多进程的系统来说，用分段的方式，外部内存碎片是很容易产生的，产生了外部内存碎片，那不得不重新 `Swap` 内存区域，这个过程会产生性能瓶颈。

  因为硬盘的访问速度要比内存慢太多了，每一次内存交换，我们都需要把一大段连续的内存数据写到硬盘上。

  所以，**如果内存交换的时候，交换的是一个占内存空间很大的程序，这样整个机器都会显得卡顿。**

  

### 内存分页

**分页是把整个虚拟和物理内存空间切成一段段固定尺寸的大小**。这样一个连续并且尺寸固定的内存空间，我们叫**页**（*Page*）。在 Linux 下，每一页的大小为 `4KB`。

虚拟地址与物理地址之间通过**页表**来映射

内存分页由于内存空间都是预先划分好的，也就不会像内存分段一样，在段与段之间会产生间隙非常小的内存，这正是分段会产生外部内存碎片的原因。而**采用了分页，页与页之间是紧密排列的，所以不会有外部碎片。**

但是，因为内存分页机制分配内存的最小单位是一页，即使程序不足一页大小，我们最少只能分配一个页，所以页内会出现内存浪费，所以针对**内存分页机制会有内部内存碎片**的现象。

如果内存空间不够，操作系统会把其他正在运行的进程中的「最近没被使用」的内存页面给释放掉，也就是暂时写在硬盘上，称为**换出**（*Swap Out*）。一旦需要的时候，再加载进来，称为**换入**（*Swap In*）。所以，一次性写入磁盘的也只有少数的一个页或者几个页，不会花太多时间，**内存交换的效率就相对比较高。**

```c++
// 实现LRU页面置换算法
// 直观思路是使用双向链表做查找和删除
// 优化思路是使用哈希+链表的方式将put和get的时间复杂度降为O(1)
class LRUCache {
private:
    int capacity;
    list<pair<int, int>> LRUList;
    unordered_map<int, list<pair<int, int>>::iterator> Cache;

public:
    LRUCache(int capacity) {
        this->capacity = capacity;
    }
    
    int get(int key) {
        if(Cache.find(key) == Cache.end()){
            return -1;
        }
        auto it = Cache[key];
        int value = it->second;
        LRUList.erase(it);
        LRUList.push_front({key, value});
        Cache[key] = LRUList.begin();
        return value;
    }
    
    void put(int key, int value) {
        if(Cache.find(key) == Cache.end()){
            // not in cache
            if(this->capacity == Cache.size()){
                // full
                auto it = LRUList.back();
                LRUList.pop_back();
                Cache.erase(it.first);
            }
            // insert new
            LRUList.push_front({key, value});
            Cache[key] = LRUList.begin();
        }else{
            // in the cache
            auto it = Cache[key];
            LRUList.erase(it);
            LRUList.push_front({key, value});
            Cache[key] = LRUList.begin();
        }
    }
};
```



#### 多级页表

由于单级页表会占用很大的内存空间（毕竟每一个进程都需要一个页表），所以在单级页表的基础上，可以优化出来多级页表

在前面我们知道了，对于单页表的实现方式，在 32 位和页大小 `4KB` 的环境下，一个进程的页表需要装下 100 多万个「页表项」，并且每个页表项是占用 4 字节大小的，于是相当于每个页表需占用 4MB 大小的空间。

我们把这个 100 多万个「页表项」的单级页表再分页，将页表（一级页表）分为 `1024` 个页表（二级页表），每个表（二级页表）中包含 `1024` 个「页表项」，形成**二级分页**

> 你可能会问，分了二级表，映射 4GB 地址空间就需要 4KB（一级页表）+ 4MB（二级页表）的内存，这样占用空间不是更大了吗？

如果使用了二级分页，一级页表就可以覆盖整个 4GB 虚拟地址空间，但**如果某个一级页表的页表项没有被用到，也就不需要创建这个页表项对应的二级页表了，即可以在需要时才创建二级页表**

我们从页表的性质来看，保存在内存中的页表承担的职责是将虚拟地址翻译成物理地址。假如虚拟地址在页表中找不到对应的页表项，计算机系统就不能工作了。所以**页表一定要覆盖全部虚拟地址空间，不分级的页表就需要有 100 多万个页表项来映射，而二级分页则只需要 1024 个页表项**



#### TLB

类似于cache



### 段页式内存管理

分段和分页混合起来用，先分段再分页



## malloc如何分配内存

malloc 申请内存的时候，会有两种方式向操作系统申请堆内存。

- 方式一：通过 brk() 系统调用从堆分配内存
- 方式二：通过 mmap() 系统调用在文件映射区域分配内存；

malloc() 源码里默认定义了一个阈值：

- 如果用户分配的内存小于 128 KB，则通过 brk() 申请内存；
- 如果用户分配的内存大于 128 KB，则通过 mmap() 申请内存；

### malloc分配的是物理内存吗

不是的，**malloc() 分配的是虚拟内存**。

**如果分配后的虚拟内存没有被访问的话，虚拟内存是不会映射到物理内存的，这样就不会占用物理内存了。**这也就是为什么可以在内存大小为4GB的机器上申请8GB的内存而不报错

### 为什么不全部使用mmap来分配内存

**频繁通过 mmap 分配的内存话，不仅每次都会发生运行态的切换，还会发生缺页中断（在第一次访问虚拟地址后），这样会导致 CPU 消耗较大**

### 为什么不全部使用br()来分配内存

br()分配内存的方式类似于栈，是从堆空间顶部开始一点一点分配的，申请多少就分配多少，那么考虑以下情况：

如果我们连续申请了 10k，20k，30k 这三片内存，如果 10k 和 20k 这两片释放了，变为了空闲内存空间，如果下次申请的内存小于 30k，那么就可以重用这个空闲内存空间

但是如果下次申请的内存大于 30k，没有可用的空闲内存空间，必须向 OS 申请，实际使用内存继续增大。

因此，随着系统频繁地 malloc 和 free ，尤其对于小块内存，堆内将产生越来越多不可用的碎片，导致“内存泄露”。而这种“泄露”现象使用 valgrind 是无法检测出来的



## 进程

### 进程上下文切换

进程上下文切换、线程上下文切换、终端上下文切换都是CPU上下文切换的效果

进程是由内核管理和调度的，所以进程的切换只能发生在内核态。

所以，**进程的上下文切换不仅包含了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的资源。**

通常，会把交换的信息保存在进程的 PCB，当要运行另外一个进程的时候，我们需要从这个进程的 PCB 取出上下文，然后恢复到 CPU 中，这使得这个进程可以继续执行

> 发生上下文切换的场景
>
> - 为了保证所有进程可以得到公平调度，CPU 时间被划分为一段段的时间片，这些时间片再被轮流分配给各个进程。这样，当某个进程的时间片耗尽了，进程就从运行状态变为就绪状态，系统从就绪队列选择另外一个进程运行；
> - 进程在系统资源不足（比如内存不足）时，要等到资源满足后才可以运行，这个时候进程也会被挂起，并由系统调度其他进程运行；
> - 当进程通过睡眠函数 sleep 这样的方法将自己主动挂起时，自然也会重新调度；
> - 当有优先级更高的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行；
> - 发生硬件中断时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序；



### 进程的状态

![七种状态变迁](assets/10-进程七中状态.jpg)

简单说一下状态变迁：

- *NULL -> 创建状态*：一个新进程被创建时的第一个状态
- *创建状态 -> 就绪状态*：当进程被创建完成并初始化后，一切就绪准备运行时，变为就绪状态，这个过程是很快的；
- *就绪态 -> 运行状态*：处于就绪状态的进程被操作系统的进程调度器选中后，就分配给 CPU 正式运行该进程；
- *运行状态 -> 结束状态*：当进程已经运行完成或出错时，会被操作系统作结束状态处理；
- *运行状态 -> 就绪状态*：处于运行状态的进程在运行过程中，由于分配给它的运行时间片用完，操作系统会把该进程变为就绪态，接着从就绪态选中另外一个进程运行；
- *运行状态 -> 阻塞状态*：当进程请求某个事件且必须等待时，例如请求 I/O 事件；
- *阻塞状态 -> 就绪状态*：当进程要等待的事件完成时，它从阻塞状态变到就绪状态；

还有一种挂起状态，这是指进程被操作系统从内存调度到了磁盘中来节约内存的空间



### 进程之间的通信方式

1. 管道

   管道是一种半双工的通信方式，**数据只能单向流动**，是FIFO的通讯方式

   非命名管道只能在父子进程进程间使用；命名管道可用于非父子进程

2. 消息队列

   用于两个进程之间的通讯，首先在一个进程中创建一个消息队列，然后再往消息队列中写数据，而另一个进程则从那个消息队列中取数据

   需要注意的是，消息队列是用创建文件的方式建立的，如果一个进程向某个消息队列中写入了数据之后，另一个进程并没有取出数据，即使向消息队列中写数据的进程已经结束，保存在消息队列中的数据并没有消失，也就是说下次再从这个消息队列读数据的时候，就是上次的数据

3. 信号量

   不能传递复杂消息，只能用来同步

4. 共享内存

   只要首先创建一个共享内存区，其它进程按照一定的步骤就能访问到这个共享内存区中的数据，当然可读可写

   **最快**，但需要保持同步

5. 信号、socket



### 创建进程的时候操作系统分配了什么资源

1. CPU时间
  进程被分配CPU时间片，允许它在CPU上执行指令。

  注意：这里是在只有用户级线程时，操作系统并不知道线程的存在。CPU时间分配给进程后再由进程分配给线程。

2. 内存
进程被分配一块内存区域，用于存储代码、数据和运行时堆栈。

3. 文件描述符
用于访问文件和设备的标识符。进程可以打开文件、读取数据和写入数据，每隔打开的文件都会有一个对应的文件描述符。

4. 网络资源
进程可以通过网络与其他计算机或进程进行通信。它可以打开网络连接，发送和接收数据等。

5. 设备资源
进程可能需要访问硬件设备，如打印机、摄像头或其他外部设备。

6. 用户权限
操作系统可能会授予进程特定的权限级别，以限制或允许其执行特定的操作。

7. 进程标识符
每个进程都有一个唯一的标识符，用于在操作系统中标识和跟踪它。

8. 父进程标识符
进程通常由其他进程（称为父进程）创建。父进程标识符用于标识创建该进程的进程。

9. 进程状态
进程可以处于不同的状态，如运行、等待、挂起等。

10. 时间片
操作系统通过时间片轮转算法，将CPU时间划分成小的时间片，每个进程在一个时间片内运行，然后切换到下一个进程。



## 调度

### 调度时机

当进程从一个运行状态到另一个运行状态的时候会触发进程调度

以下状态的变化都会触发操作系统的调度：

- *从就绪态 -> 运行态*：当进程被创建时，会进入到就绪队列，操作系统会从就绪队列选择一个进程运行；
- *从运行态 -> 阻塞态*：当进程发生 I/O 事件而阻塞时，操作系统必须选择另外一个进程运行；
- *从运行态 -> 结束态*：当进程退出结束后，操作系统得从就绪队列选择另外一个进程运行；

另外，如果硬件时钟提供某个频率的周期性中断，那么可以根据如何处理时钟中断 ，把调度算法分为两类：

- **非抢占式调度算法**挑选一个进程，然后让该进程运行直到被阻塞，或者直到该进程退出，才会调用另外一个进程，也就是说不会理时钟中断这个事情。
- **抢占式调度算法**挑选一个进程，然后让该进程只运行某段时间，如果在该时段结束时，该进程仍然在运行时，则会把它挂起，接着调度程序从就绪队列挑选另外一个进程。这种抢占式调度处理，需要在时间间隔的末端发生**时钟中断**，以便把 CPU 控制返回给调度程序进行调度，也就是常说的**时间片机制**。

### 调度算法

1. 先来先服务调度算法（FIFO）

   顾名思义，先来后到，**每次从就绪队列选择最先进入队列的进程，然后一直运行，直到进程退出或被阻塞，才会继续从队列中选择第一个进程接着运行。**

   这似乎很公平，但是当一个长作业先运行了，那么后面的短作业等待的时间就会很长，不利于短作业。

   FCFS 对长作业有利，适用于 CPU 繁忙型作业的系统，而不适用于 I/O 繁忙型作业的系统。

2. 最短作业优先调度算法

   顾名思义，它会**优先选择运行时间最短的进程来运行**，这有助于提高系统的吞吐量

   这显然对长作业不利，很容易造成一种极端现象。

   比如，一个长作业在就绪队列等待运行，而这个就绪队列有非常多的短作业，那么就会使得长作业不断的往后推，周转时间变长，致使长作业长期不会被运行。

3. 高响应比优先调度算法

   **每次进行进程调度时，先计算「响应比优先级」，然后把「响应比优先级」最高的进程投入运行**，响应比计算公式为：
   $$
   优先权=\frac{等待时间+要求服务时间}{要求服务时间}
   $$
   从上面的公式，可以发现：

   - 如果两个进程的「等待时间」相同时，「要求的服务时间」越短，「响应比」就越高，这样短作业的进程容易被选中运行；
   - 如果两个进程「要求的服务时间」相同时，「等待时间」越长，「响应比」就越高，这就兼顾到了长作业进程，因为进程的响应比可以随时间等待的增加而提高，当其等待时间足够长时，其响应比便可以升到很高，从而获得运行的机会；

   **然而现实中我们没有办法预先知道作业的要求服务时间，所以高响应比算法是一种理想的算法**

4. 时间片轮转调度算法

   **每个进程被分配一个时间段，称为时间片（\*Quantum\*），即允许该进程在该时间段中运行**，这是最古老、最简单、最公平且使用最广的算法

5. 最高优先级调度算法

   前面的「时间片轮转算法」做了个假设，即让所有的进程同等重要，也不偏袒谁，大家的运行时间都一样。

   但是，对于多用户计算机系统就有不同的看法了，它们希望调度是有优先级的，即希望调度程序能**从就绪队列中选择最高优先级的进程进行运行，这称为最高优先级（\*Highest Priority First，HPF\*）调度算法**。

   进程的优先级可以分为，静态优先级和动态优先级：

   - 静态优先级：创建进程时候，就已经确定了优先级了，然后整个运行时间优先级都不会变化；
   - 动态优先级：根据进程的动态变化调整优先级，比如如果进程运行时间增加，则降低其优先级，如果进程等待时间（就绪队列的等待时间）增加，则升高其优先级，也就是**随着时间的推移增加等待进程的优先级**。

   该算法也有两种处理优先级高的方法，非抢占式和抢占式：

   - 非抢占式：当就绪队列中出现优先级高的进程，运行完当前进程，再选择优先级高的进程。
   - 抢占式：当就绪队列中出现优先级高的进程，当前进程挂起，调度优先级高的进程运行。

   但是依然有缺点，可能会导致低优先级的进程永远不会运行。

6. 多级反馈队列调度算法

   **多级反馈队列（\*Multilevel Feedback Queue\*）调度算法**是「时间片轮转算法」和「最高优先级算法」的综合和发展。

   顾名思义：

   - 「多级」表示有多个队列，每个队列优先级从高到低，同时优先级越高时间片越短。
   - 「反馈」表示如果有新的进程加入优先级高的队列时，立刻停止当前正在运行的进程，转而去运行优先级高的队列；

   它是如何工作的：

   - 设置了多个队列，赋予每个队列不同的优先级，每个**队列优先级从高到低**，同时**优先级越高时间片越短**；
   - 新的进程会被放入到第一级队列的末尾，按先来先服务的原则排队等待被调度，如果在第一级队列规定的时间片没运行完成，则将其转入到第二级队列的末尾，以此类推，直至完成；
   - 当较高优先级的队列为空，才调度较低优先级的队列中的进程运行。如果进程运行时，有新进程进入较高优先级的队列，则停止当前运行的进程并将其移入到原队列末尾，接着让较高优先级的进程运行；



## 线程、多线程冲突、锁

### 线程是什么

进程与进程之间没有办法共享数据，只能够依靠通信，并且维护进程的系统开销较大，如创建进程时，分配资源、建立 PCB；终止进程时，回收资源、撤销 PCB；进程切换时，保存当前进程的状态信息......

所以我们需要一种新的实体，其能够满足：

* 实体之间可以并发运行
* 实体之间可以共享地址空间

这就是线程

**线程是进程当中的一条执行流程**

同一个进程内多个线程之间可以共享代码段、数据段、打开的文件等资源，但每个线程各自都有一套独立的寄存器和栈，这样可以确保线程的控制流是相对独立的

> ### 线程的优缺点：
>
> 线程的优点：
>
> - 一个进程中可以同时存在多个线程；
> - 各个线程之间可以并发执行；
> - 各个线程之间可以共享地址空间和文件等资源；
>
> 线程的缺点：
>
> - 当进程中的一个线程崩溃时，会导致其所属进程的所有线程崩溃（这里是针对 C/C++ 语言，Java语言中的线程奔溃不会造成进程崩溃（这主要是因为在进程中，**各个线程的地址空间是共享的**，既然是共享，那么某个线程对地址的非法访问就会导致内存的不确定性，进而可能会影响到其他线程，这种操作是危险的，操作系统会认为这很可能导致一系列严重的后果，于是干脆让整个进程崩溃）
>
>   至于Java为什么线程崩溃不会导致进程崩溃，这是因为**JVM 自定义了自己的信号处理函数，拦截了 SIGSEGV 信号，针对这两者不让它们崩溃**
>
> ### 线程的共享与独占
>
>
> 线程共享的东西有：
>
> 1. 进程的虚拟地址（即代码段、数据段和堆栈）
> 2. 文件描述符（可以同时读写一个文件）
> 3. 全局变量
> 4. 静态变量
> 5. 进程id
>
> 线程独占的东西有：
>
> 1. 线程id
> 2. 寄存器组的值
> 3. 线程自己的堆栈



### 线程上下文切换

- 当两个线程不是属于同一个进程，则切换的过程就跟进程上下文切换一样；
- **当两个线程是属于同一个进程，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据**；

所以，线程的上下文切换相比进程，开销要小很多



### 线程与协程

协程=用户级线程，切换的时候不需要陷入内核态，用户自己来控制程序创建、销毁协程

> ### 两者的区别：
>
> 1. 线程是操作系统的概念，而协程是程序级的概念。线程由操作系统调度执行，每个线程都有自己的执行上下文，包
>    括程序计数器、寄存器等。而协程由程序自身控制。
>
> 2. 多个线程之间通过切换执行的方式实现并发。线程切换时需要保存和恢复上下文，涉及到上下文切换的开销。而协
>    程切换时不需要操作系统的介入，只需要保存和恢复自身的上下文，切换开销较小。
>
> 3. 线程是抢占式的并发，即操作系统可以随时剥夺一个线程的执行权。而协程是合作式的并发，协程的执行权由程序
>    自身决定，只有当协程主动让出执行权时，其他协程才会得到执行机会。



### 线程安全

一个简单的例子就是：开两个线程对一个变量n=0分别进行++和--操作1000次，打印最终的结果，会发现在没有保证线程安全的情况下，n的最终结果每次不一样并且可能不为0

我们简单定义：

* **当多个线程访问某个类时，不管运行时环境采用何种调度方式或者这些线程将如何交替执行，并且在调用代码中不需要任何额外的同步，这个类都能表现出正确的行为，那么这个类就是线程安全的**

#### 线程不安全的原因

1. 原子性

   由于代码的执行可能不是原子性的，而是分为：

   * 从内存把数据读到CPU
   * 对数据进行更新操作
   * 再把更新后的操作写入内存

   这三步，所以可能导致结果出错

2. 可见性

   多个线程工作的时候都是**在自己的工作内存中（CPU寄存器）** 来执行操作的，**线程之间是不可见**的

   * 线程之间的共享变量存在主内存

   * 每一个线程都有自己的工作内存

   * 线程读取共享变量时，先把变量从主存拷贝到工作内存（寄存器），再从工作内存（寄存）读取数据

   * 线程修改共享变量时，先修改工作内存中的变量值，再同步到主内存

   我们需要保证可见性，保证每次读取变量的值时都从主存获取最新的值

3. 有序性

   * 重排序：

     这是JVM的一种机制

     > **JVM翻译字节码指令，CPU执行机器码指令，都可能发生重排序来优化执行效率**
     >
     > 比如有这样三步操作：(1) 去前台取U盘 (2) 去教室写作业 (3) 去前台取快递
     >
     > JVM会对指令优化，也就是重排序，新的顺序为(1)(3)(2),这样来提高效率

     这样的机制不影响单线程的执行，但会影响多线程并发

#### 线程不安全总结

1. 线程是抢占式的执行，线程间的调度充满了随机性

2. 多个线程对同一个变量进行修改操作

3. 对变量的操作不是原子性的

4. 内存可见性导致的线程安全

5. 指令重排序也会影响线程安全



### 线程同步

#### 什么是同步和互斥

同步就是协同步调，按预定的先后次序进行运行。如：你说完，我再说。这里的同步千万不要理解成那个同时进行，应是指协同、协助、互相配合。线程同步是指多线程通过特定的设置（如互斥量，事件对象，临界区）来控制线程之间的执行顺序（即所谓的同步）也可以说是在线程之间通过同步建立起执行顺序的关系，如果没有同步，那线程之间是各自运行各自的！

线程互斥是指对于共享的进程系统资源，在各单个线程访问时的排它性。当有若干个线程都要使用某一共享资源时，任何时刻最多只允许一个线程去使用，其它要使用该资源的线程必须等待，直到占用资源者释放该资源。线程互斥可以看成是一种特殊的线程同步（下文统称为同步）。

#### 线程同步的方式

- 临界区（Critical Section）、互斥对象（Mutex）：主要用于互斥控制；都具有拥有权的控制方法，只有拥有该对象的线程才能执行任务，所以拥有，执行完任务后一定要释放该对象。

- 信号量（Semaphore）、事件对象（Event）：事件对象是以通知的方式进行控制，主要用于同步控制！

**临界区**

通过对多线程的串行化来访问公共资源或一段代码，速度快，适合控制数据访问。在任意时刻只允许一个线程对共享资源进行访问，如果有多个线程试图访问公共资源，那么在有一个线程进入后，其他试图访问公共资源的线程将被挂起，并一直等到进入临界区的线程离开，临界区在被释放后，其他线程才可以抢占。它并不是核心对象，不是属于操作系统维护的，而是属于进程维护的。

总结下关键段：

1. 关键段共初始化化、销毁、进入和离开关键区域四个函数。
2. 关键段可以解决线程的互斥问题，但因为具有“线程所有权”，所以无法解决同步问题。
3. 推荐关键段与旋转锁配合使用。

**互斥对象**

互斥对象和临界区很像，采用互斥对象机制，只有拥有互斥对象的线程才有访问公共资源的权限。因为互斥对象只有一个，所以能保证公共资源不会同时被多个线程同时访问。当前拥有互斥对象的线程处理完任务后必须将线程交出，以便其他线程访问该资源。

总结下互斥量Mutex：

1. 互斥量是内核对象，它与关键段都有“线程所有权”所以不能用于线程的同步。
2. 互斥量能够用于多个进程之间线程互斥问题，并且能完美的解决某进程意外终止所造成的“遗弃”问题。

**信号量**

信号量也是内核对象。它允许多个线程在同一时刻访问同一资源，但是需要限制在同一时刻访问此资源的最大线程数目

在用CreateSemaphore()创建信号量时即要同时指出允许的最大资源计数和当前可用资源计数。一般是将当前可用资源计数设置为最 大资源计数，每增加一个线程对共享资源的访问，当前可用资源计数就会减1 ，只要当前可用资源计数是大于0 的，就可以发出信号量信号。但是当前可用计数减小 到0 时则说明当前占用资源的线程数已经达到了所允许的最大数目，不能在允许其他线程的进入，此时的信号量信号将无法发出。线程在处理完共享资源后，应在离 开的同时通过ReleaseSemaphore （）函数将当前可用资源计数加1 。在任何时候当前可用资源计数决不可能大于最大资源计数。

**事件对象**

通过通知操作的方式来保持线程的同步，还可以方便实现对多个线程的优先级比较的操作

总结下事件Event

1. 事件是内核对象，事件分为手动置位事件和自动置位事件。事件Event内部它包含一个使用计数（所有内核对象都有），一个布尔值表示是手动置位事件还是自动置位事件，另一个布尔值用来表示事件有无触发。
2. 事件可以由SetEvent()来触发，由ResetEvent()来设成未触发。还可以由PulseEvent()来发出一个事件脉冲。
3. 事件可以解决线程间同步问题，因此也能解决互斥问题。



### 如何解决线程不安全问题

* JAVA版本：

   1. synchronized关键字：

      sychronized是基于对象头加锁的，特别注意：不是对代码加锁，所说的加锁操作就是给这个对象的对象头里设置了一个标志位

      一个对象在同一时间只能有一个线程获取到该对象的锁

      sychronized保证了原子性，可见性，有序性（这里的有序不是指指令重排序，而是具有相同锁的代码块按照获取锁的顺序执行）

* C++版本：

  1. 使用互斥量（mutex）来对共享资源进行保护。互斥量可以用来防止多个线程同时访问共享资源，从而避免数据竞争的问题。
  2. 使用读写锁（reader-writer lock）来对共享资源进行保护。读写锁允许多个读线程同时访问共享资源，但是写线程必须独占资源。这样可以在保证线程安全的同时，也尽可能地提高系统的并发性。
  3. 使用原子操作来对共享资源进行保护。在 C++ 中，可以使用 std::atomic 类型来定义原子变量，并使用原子操作来对共享资源进行操作。这样可以确保在多线程环境中，原子变量的操作是安全的。
  4. 使用条件变量（condition variable）来协调线程间的协作。条件变量可以用来在线程之间传递信号，从而控制线程的执行流程。
  5. 使用线程本地存储（thread-local storage）来保存线程的私有数据。线程本地存储可以用来给每个线程分配独立的存储空间，从而避免数据冲突的问题

* GO版本：

  1. 使用互斥锁，但是会带来额外的开销
  2. 使用atomic包，但是无法解决数据竞争的根本问题



### 锁的种类

#### 互斥锁与自旋锁

最底层的两种就是会「互斥锁和自旋锁」，有很多高级的锁都是基于它们实现的。加锁的目的是为了让共享资源在某段时间内只有一个线程能够访问，互斥锁于自旋锁的区别在于他们在获取锁失败之后的行为：

- **互斥锁**加锁失败后，线程会**释放 CPU** ，给其他线程；
- **自旋锁**加锁失败后，线程会**忙等待**，直到它拿到锁；

所以互斥锁在加锁失败之后，会从用户态陷入内核态，存在两次线程的上下文切换。虽然用起来简单，但是存在性能上的开销：

而如果加锁后执行代码的时间很短，甚至比两次上下文切换加起来的时间短，那么显然性价比就会很低。所以如果能够确定锁内执行代码执行时间比较短，那么自旋锁比互斥锁更优

#### 读写锁

适用于能够明确区分读写操作的场景

工作原理为：

- 当写锁没有被线程持有时，多个线程能够持有读锁
- 当写锁被线程持有，那么其他线程获取读锁和写锁的操作都会被阻塞

#### 乐观锁和悲观锁

- 乐观锁：先修改完共享资源，然后验证这段时间内有没有冲突，如果没有，结束操作；如果有，放弃本次操作
- 悲观锁：在访问共享资源之前先上锁

上述三种锁都属于悲观锁，而git使用了乐观锁的思想

总的来说，乐观锁适合在冲突发生概率低、并且加锁成本非常高的场景下使用



### 死锁

简单来说就是互相等待对方持有的锁

#### 死锁产生原因

死锁只有当以下四个条件同时满足才会出现：

- 互斥条件：

  指**多个线程不能同时使用同一个资源**

- 持有并等待条件：

  线程等待资源的时候不会主动释放手上的资源

- 不可剥夺条件：

  当线程已经持有了资源 ，**在自己使用完之前不能被其他线程获取**

- 环路等待条件：

  **两个线程获取资源的顺序构成了环形链**

#### 如何避免死锁

要避免死锁问题，就是要破坏其中一个条件即可，最常用的方法就是使用资源有序分配法来破坏环路等待条件。

##### 银行家算法

银行家算法是一种最有代表性的避免死锁的算法。在避免死锁方法中允许进程动态地申请资源，但系统在进行资源分配之前，应先计算此次分配资源的安全性，若分配不会导致系统进入不安全状态，则分配，否则等待

- 简单来说就是遍历所有的进程，对于其需要的资源和当前空闲的资源，如果空闲资源满足其所需，那么就加入**安全队列**，反之则不加入

- 然后回收这个进程所使用的资源数量，继续遍历其他进程，直到所有进程都放入了安全队列，或者无法满足所有剩下的进程的资源需求



## 网络系统

### 零拷贝

### IO多路复用：select/poll/epoll

首先什么是IO多路复用，已知让服务端为每一个请求都分配一个进程/线程来进行IO操作是不现实的（因为如果请求量上去之后会造成大量的空间占用），因此我们发明了只用一个进程来维护多个Socket的方式，这就是IO多路复用

一个进程虽然任一时刻只能处理一个请求，但是处理每个请求的事件时，耗时控制在 1 毫秒以内，这样 1 秒内就可以处理上千个请求，把时间拉长来看，多个请求复用了一个进程，这就是多路复用，这种思想很类似一个 CPU 并发多个进程，所以也叫做时分多路复用

select/poll/epoll 是如何获取网络事件的呢？在获取事件时，先把所有连接（文件描述符）传给内核，再由内核返回产生了事件的连接，然后在用户态中再处理这些连接对应的请求即可

**select**

select 实现多路复用的方式是，将已连接的 Socket 都放到一个**文件描述符集合**，然后调用 select 函数将文件描述符集合**拷贝**到内核里，让内核来检查是否有网络事件产生，检查的方式很粗暴，就是通过**遍历**文件描述符集合的方式，当检查到有事件产生后，将此 Socket 标记为可读或可写， 接着再把整个文件描述符集合**拷贝**回用户态里，然后用户态还需要再通过**遍历**的方法找到可读或可写的 Socket，然后再对其处理。

总的来说，select一共需要进行 **2 次「遍历」文件描述符集合**，而且**2 次「拷贝」文件描述符集合**：先从用户空间传入内核空间，由内核修改后，再传出到用户空间中

select 使用固定长度的 BitsMap，表示文件描述符集合，而且所支持的文件描述符的个数是有限制的，在 Linux 系统中，由内核中的 FD_SETSIZE 限制， 默认最大值为 `1024`，只能监听 0~1023 的文件描述符

> ### 人话版
>
> - 每次轮训都用文件标识符生成bitmap拷贝到内核态
>
> - 方法为遍历，有事件发生就返回
>
>   //  未完待续

**poll**

poll 不再用 BitsMap 来存储所关注的文件描述符，取而代之用动态数组，以链表形式来组织，突破了 select 的文件描述符个数限制，当然还会受到系统文件描述符限制

但是 poll 和 select 并没有太大的本质区别，**都是使用「线性结构」存储进程关注的 Socket 集合，因此都需要遍历文件描述符集合来找到可读或可写的 Socket，时间复杂度为 O(n)，而且也需要在用户态与内核态之间拷贝文件描述符集合**，这种方式随着并发数上来，性能的损耗会呈指数级增长

**epoll**

首先是epoll的大致流程：先用epoll_create 创建一个 epoll对象 epfd，再通过 epoll_ctl 将需要监视的 socket 添加到epfd中，最后调用 epoll_wait 等待数据

```c++
int s = socket(AF_INET, SOCK_STREAM, 0);
bind(s, ...);
listen(s, ...)

int epfd = epoll_create(...);
epoll_ctl(epfd, ...); //将所有需要监听的socket添加到epfd中

while(1) {
    int n = epoll_wait(...);
    for(接收到数据的socket){
        //处理
    }
}
```

epoll 通过两个方面，很好解决了 select/poll 的问题。

* epoll 在内核里使用**红黑树来跟踪进程所有待检测的文件描述字**：

  select/poll 每次操作时都传入整个 socket 集合给内核，而 epoll 因为在内核维护了红黑树，可以保存所有待检测的 socket ，所以只需要传入一个待检测的 socket，减少了内核和用户空间大量的数据拷贝和内存分配

*  epoll 使用**事件驱动**的机制，内核里**维护了一个链表来记录就绪事件**：

  当某个 socket 有事件发生时，通过**回调函数**内核会将其加入到这个就绪事件列表中，当用户调用 `epoll_wait()` 函数时，只会返回有事件发生的文件描述符的个数，不需要像 select/poll 那样轮询扫描整个 socket 集合，大大提高了检测的效率

**epoll的边缘触发和水平触发**

- 使用边缘触发模式时，当被监控的 Socket 描述符上有可读事件发生时，**服务器端只会从 epoll_wait 中苏醒一次**，即使进程没有调用 read 函数从内核读取数据，也依然只苏醒一次，因此我们程序要保证一次性将内核缓冲区的数据读取完；
- 使用水平触发模式时，当被监控的 Socket 上有可读事件发生时，**服务器端不断地从 epoll_wait 中苏醒，直到内核缓冲区数据被 read 函数读完才结束**，目的是告诉我们有数据需要读取；



# MySQL

## 基础

### MySQL表空间结构

**表空间由段（segment）、区（extent）、页（page）、行（row）组成**：

1、行（row）

数据库表中的记录都是按行（row）进行存放的，每行记录根据不同的行格式，有不同的存储结构。

后面我们详细介绍 InnoDB 存储引擎的行格式，也是本文重点介绍的内容。

2、页（page）

记录是按照行来存储的，但是数据库的读取并不以「行」为单位，否则一次读取（也就是一次 I/O 操作）只能处理一行数据，效率会非常低。

因此，**InnoDB 的数据是按「页」为单位来读写的**，也就是说，当需要读一条记录的时候，并不是将这个行记录从磁盘读出来，而是以页为单位，将其整体读入内存。

**默认每个页的大小为 16KB**，也就是最多能保证 16KB 的连续存储空间。

页是 InnoDB 存储引擎磁盘管理的最小单元，意味着数据库每次读写都是以 16KB 为单位的，一次最少从磁盘中读取 16K 的内容到内存中，一次最少把内存中的 16K 内容刷新到磁盘中。

页的类型有很多，常见的有数据页、undo 日志页、溢出页等等。总之知道表中的记录存储在「数据页」里面就行。

3、区（extent）

我们知道 InnoDB 存储引擎是用 B+ 树来组织数据的。

B+ 树中每一层都是通过双向链表连接起来的，如果是以页为单位来分配存储空间，那么链表中相邻的两个页之间的物理位置并不是连续的，可能离得非常远，那么磁盘查询时就会有大量的随机I/O，随机 I/O 是非常慢的。

解决这个问题也很简单，就是让链表中相邻的页的物理位置也相邻，这样就可以使用顺序 I/O 了，那么在范围查询（扫描叶子节点）的时候性能就会很高。

那具体怎么解决呢？

**在表中数据量大的时候，为某个索引分配空间的时候就不再按照页为单位分配了，而是按照区（extent）为单位分配。每个区的大小为 1MB，对于 16KB 的页来说，连续的 64 个页会被划为一个区，这样就使得链表中相邻的页的物理位置也相邻，就能使用顺序 I/O 了**。

4、段（segment）

表空间是由各个段（segment）组成的，段是由多个区（extent）组成的。段一般分为数据段、索引段和回滚段等。

- 索引段：存放 B + 树的非叶子节点的区的集合；
- 数据段：存放 B + 树的叶子节点的区的集合；
- 回滚段：存放的是回滚数据的区的集合，之前讲[事务隔离 (opens new window)](https://xiaolincoding.com/mysql/transaction/mvcc.html)的时候就介绍到了 MVCC 利用了回滚段实现了多版本查询数据。

### InnoDB的行格式

变长字段长度列表、NULL值列表、记录头信息（这三个是额外信息）， 数据，列1～n的值（记录的真实数据）

* 对于变长字段长度列表：

  **变长字段的真实数据占用的字节数会按照列的顺序逆序存放**，==只有变长字段在会在这里存放信息==

  变长字段是可以不存在的，**当数据表没有变长字段的时候，比如全部都是 int 类型的字段，这时候表里的行格式就不会有「变长字段长度列表」了**

* NULL值列表：

  用比特位逆序存放数据为NULL的字段

  > 例如name、phone、age三个字段，其中name和age为NULL
  >
  > 则NULL值列表为101，对应十进制为5

  同样，当数据表的字段都定义为NOT NULL的时候，也就不会有NULL值列表了

* 记录头信息：

  记录头信息中包含的内容很多，就不一一列举了，这里说几个比较重要的：

  - delete_mask ：标识此条数据是否被删除。从这里可以知道，我们执行 detele 删除记录的时候，并不会真正的删除记录，只是将这个记录的 delete_mask 标记为 1。
  - next_record：下一条记录的位置。从这里可以知道，记录与记录之间是通过链表组织的。在前面我也提到了，指向的是下一条记录的「记录头信息」和「真实数据」之间的位置，这样的好处是向左读就是记录头信息，向右读就是真实数据，比较方便。
  - record_type：表示当前记录的类型，0表示普通记录，1表示B+树非叶子节点记录，2表示最小记录，3表示最大记录

* 记录的真实数据：

  记录真实数据部分除了我们定义的字段，还有三个隐藏字段，分别为：row_id、trx_id、roll_pointer

  - row_id：

    如果我们建表的时候指定了主键或者唯一约束列，那么就没有 row_id 隐藏字段了。如果既没有指定主键，又没有唯一约束，那么 InnoDB 就会为记录添加 row_id 隐藏字段。row_id不是必需的，占用 6 个字节。

  - trx_id：

    事务id，表示这个数据是由哪个事务生成的。 trx_id是必需的，占用 6 个字节。

  - roll_pointer：

    这条记录上一个版本的指针。roll_pointer 是必需的，占用 7 个字节

## 索引

### 为什么InnoDB使用B+树而不是B树作为索引

首先是两者有什么差异：

- B+树中只有叶子结点才会存放数据（可以是直接获取的数据也可以是对应主键id）
- 所有的索引都会在叶子结点出现，叶子结点之间有链表进行连接

B树在提高了IO性能的同时并没有解决元素遍历的我效率低下的问题，正是为了解决这个问题，B+树应用而生。B+树只需要去遍历叶子节点就可以实现整棵树的遍历。而且在数据库中基于范围的查询是非常频繁的，而B树不支持这样的操作或者说效率太低。

使用B+树所带来的优势：

- **B+树的磁盘读写代价更低**：B+树的内部节点并没有指向关键字具体信息的指针，因此其内部节点相对B树更小，如果把所有同一内部节点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多，一次性读入内存的需要查找的关键字也就越多，相对IO读写次数就降低了。
- **B+树的查询效率更加稳定**：由于非终结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。
- 由于B+树的数据都存储在叶子结点中，分支结点均为索引，方便扫库，只需要扫一遍叶子结点即可，但是B树因为其分支结点同样存储着数据，我们要找到具体的数据，需要进行一次中序遍历按序来扫，所以B+树更加适合在区间查询的情况，所以通常B+树用于数据库索引。



### 索引分类：

#### 按字段分类：

可以分为主键索引、唯一索引、普通索引、前缀索引

#### 按字段个数分类：

从字段个数的角度来看，索引分为单列索引、联合索引



### 优化索引：

- 前缀索引优化：

  使用前缀索引是为了减小索引字段大小，可以增加一个索引页中存储的索引值，有效提高索引的查询速度。在一些大字符串的字段作为索引时，使用前缀索引可以帮助我们减小索引项的大小

  不过，前缀索引有一定的局限性，例如：

  - order by 就无法使用前缀索引；
  - 无法把前缀索引用作覆盖索引；

- 覆盖索引优化：

  覆盖索引是指 SQL 中 query 的所有字段，在索引 B+Tree 的叶子节点上都能找得到的那些索引，从二级索引中查询得到记录，而不需要通过聚簇索引查询获得，可以避免回表的操作

- 主键索引最好是自增的：

  每次**插入一条新记录，都是追加操作，不需要重新移动数据**，因此这种插入数据的方法效率非常高

  使用自增主键，在进行插入的时候可以避免数据页发生页分裂操作

- 防止索引失效：

  这里简单说一下，发生索引失效的情况：

  - 当我们使用左或者左右模糊匹配的时候，也就是 `like %xx` 或者 `like %xx%`这两种方式都会造成索引失效；
  - 当我们在查询条件中对索引列做了计算、函数、类型转换操作，这些情况下都会造成索引失效；
  - 联合索引要能正确使用需要遵循最左匹配原则，也就是按照最左优先的方式进行索引的匹配，否则就会导致索引失效。
  - 在 WHERE 子句中，如果在 OR 前的条件列是索引列，而在 OR 后的条件列不是索引列，那么索引会失效。



### 创建索引的原则

1. 字段的数值有唯一性限制
2. 频繁作为Where查询条件的字段
3. 经常Group by和Order by的列
4. Update、Delete的where条件列
5. Distinct字段需要创建索引
6. 使用字符串前缀创建索引
7. 使用最频繁的列放到联合索引的左侧



## 慢查询

慢查询，顾名思义就是很慢的查询。MySQL的慢查询日志是MySQL提供的一种日志记录，它用来记录在MySQL中响应时间超过阀值的语句，具体指运行时间超过long_query_time值的SQL，则会被记录到慢查询日志中。long_query_time的默认值为10s。默认情况下，Mysql数据库并不启动慢查询日志，需要我们手动来设置这个参数，当然，如果不是调优需要的话，一般不建议启动该参数，因为开启慢查询日志或多或少会带来一定的性能影响。慢查询日志支持将日志记录写入文件，也支持将日志记录写入数据库表。

### 开启慢查询日志

要使用慢查询日志，首先要检查慢查询日志是否开启，如果没有，将其开启，并设置慢查询阈值即慢查询文件存储位置等属性。通过下面的命令查看相关属性

```sql
show variables like '%query%';
```

### 慢查询监控

- 可以使用```show processlist ```显示用户正在运行的线程，需要注意的是，除了 root 用户能看到所有正在运行的线程外，其他用户都只能看到自己正在运行的线程。show processlist 显示的信息都是来自MySQL系统库 **information_schema 中的 processlist 表**

  ![img](assets/d8433f48c4594eac80803eea6cbf1236.png)

  - Id：就是这个线程的唯一标识，当我们发现这个线程有问题的时候，可以通过 kill 命令，加上这个Id值将这个线程杀掉。是这个表的主键。
  - User：就是指启动这个线程的用户。
  - Host：记录了发送请求的客户端的 IP 和 端口号。通过这些信息在排查问题的时候，我们可以定位到是哪个客户端的哪个进程发送的请求。
  - DB：当前执行的命令是在哪一个数据库上。如果没有指定数据库，则该值为 NULL 。
  - Command：是指此刻该线程正在执行的命令。这个很复杂，下面单独解释
  - Time：表示该线程处于当前状态的时间，单位是秒。
  - State：线程的状态，和 Command 对应，下面单独解释。
  - Info：一般记录的是线程执行的语句。默认只显示前100个字符，也就是你看到的语句可能是截断了的，要看全部信息，需要使用 show full processlist。

- 可以使用explain字段显示sql的执行计划

  使用之后可以看到如下几个字段：

  > - id:                        选择标识符
  > - select_type:       表示查询的类型。
  > - table:                   输出结果集的表
  > - partitions:           匹配的分区
  > - type:                    表示表的连接类型
  > - possible_keys:   表示查询时，可能使⽤的索引
  > - key:                      表示实际使⽤的索引
  > - key_len:               索引字段的长度
  > - ref:                       列与索引的比较
  > - rows:                    扫描出的行数(估算的行数)
  > - filtered:                按表条件过滤的⾏百分比
  > - Extra:                   执行情况的描述和说明

  其中type字段的介绍：

  > - system:表中只有一条数据，等于系统表（引擎只能使MYISAM和MEMORY）
  >
  > - const:使用主键或者唯一索引，可以将查询的变量转成常量。(例如：… where id=3 或者where name=‘name1’
  >
  > - eq_ref:类似ref,区别在于使用唯一索引，返回匹配的唯一一条数据（通常在连接时出现，例如：explain select t1.name from t1, t2 where t1.name= t2.name）。
  > - ref: 非唯一性索引，可以返回多行匹配的数据。
  > - range: 范围查询，使用索引返回一个范围中的行（例如：… where id >3)
  > - index: 以索引顺序进行全表扫描，优点是不用排序，缺点是还要全表扫描。
  > - all: 全表扫描，应尽量避免。

  其中Extra字段的介绍：

  > - using index: 使用了覆盖索引，覆盖索引的好处是一条SQL通过索引就可以返回我们需要的数据, 不需要通过索引回表。
  > - using index condition: 在5.6版本后加入的新特性：索引下推（Index Condition Pushdown），索引下推是在非主键索引上的优化，可以有效减少回表的次数，大大提升了查询的效率。。查询的列不完全被索引覆盖，where条件中是一个前导列的范围。会先条件过滤索引，过滤完索引后找到所有符合索引条件的数据行，随后用 WHERE 子句中的其他条件去过滤这些数据行； using index condition = using index + 回表 + where 过滤。
  > - using where: 查询时没使用到索引，然后通过where条件过滤获取到所需的数据。
  > - using temporary: 表示查询时，mysql使用临时表保存结果。效率较低，应当尽量避免。
  > - using filesort: 当SQL中包含 ORDER BY 操作，而且无法利用索引完成排序操作的时候，MySQL不得不选择相应的排序算法来实现，这时就会出现Using filesort，效率较低，应该尽量避免。

### 慢查询优化

1. 避免使用子查询

2. 读取适当的记录limit

3. 对于分组统计可以禁止排序

4. 避免不必要的排序

5. 避免三张表以上的join

6. 在varchar字段上建立索引时，必须指定索引长度

7. 避免使用select *，只返回必要的列

8. 尽量使用数字型字段

9. 尽量避免索引失效

   1. 字段类型转换导致不用索引

      > 如字符串类型的不用引号，数字类型的用引号等，这有可能会用不到索引导致全表扫描；

   2. 根据联合索引的第二个及以后的字段单独查询用不到索引

   3. 字段前面不能加函数/加减运算，否则会导致索引失效

   4. 搜索严禁左模糊或者全模糊

   5. 避免在 where 子句中使用!=或<>操作符，否则将引擎放弃使用索引而进行全表扫描

   6. 避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描

   7. 用IN或UNION来替换OR低效查询

   8. 在 where 子句中使用参数，也会导致全表扫描

   9. 删除表所有记录请用 truncate，不要用 delete



## 事务

### 事务的特性：

也就是常说的ACID：

- **原子性（Atomicity）**：一个事务中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节，而且事务在执行过程中发生错误，会被回滚到事务开始前的状态，就像这个事务从来没有执行过一样，就好比买一件商品，购买成功时，则给商家付了钱，商品到手；购买失败时，则商品在商家手中，消费者的钱也没花出去。
- **一致性（Consistency）**：是指事务操作前和操作后，数据满足完整性约束，数据库保持一致性状态。比如，用户 A 和用户 B 在银行分别有 800 元和 600 元，总共 1400 元，用户 A 给用户 B 转账 200 元，分为两个步骤，从 A 的账户扣除 200 元和对 B 的账户增加 200 元。一致性就是要求上述步骤操作后，最后的结果是用户 A 还有 600 元，用户 B 有 800 元，总共 1400 元，而不会出现用户 A 扣除了 200 元，但用户 B 未增加的情况（该情况，用户 A 和 B 均为 600 元，总共 1200 元）。
- **隔离性（Isolation）**：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致，因为多个事务同时使用相同的数据时，不会相互干扰，每个事务都有一个完整的数据空间，对其他并发事务是隔离的。也就是说，消费者购买商品这个事务，是不影响其他消费者购买的。
- **持久性（Durability）**：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。



### 并发事务可能会引发的问题：

按照严重性从高到低分别如下：

* **脏读**：

  **如果一个事务「读到」了另一个「未提交事务修改过的数据」，就意味着发生了「脏读」现象**

* **不可重复读**：

  **在一个事务内多次读取同一个数据，如果出现前后两次读到的数据不一样的情况，就意味着发生了「不可重复读」现象**

* **幻读**：

  **在一个事务内多次查询某个符合查询条件的「记录数量」，如果出现前后两次查询到的记录数量不一样的情况，就意味着发生了「幻读」现象**
  
  > 幻读并不是说两次读取获取的结果集不同，幻读侧重的方面是某一次的 select 操作得到的结果所表征的数据状态无法支撑后续的业务操作。更为具体一些：A事务select 某记录是否存在，结果为不存在，准备插入此记录，但执行 insert 时发现此记录已存在，无法插入，此时就发生了幻读。产生这样的原因是因为有另一个事务往表中插入了数据。
  >



### MVCC：

MVCC全称是多版本并发控制 (Multi-Version Concurrency Control)，只有在InnoDB引擎下存在。MVCC机制的作用其实就是避免同一个数据在不同事务之间的竞争，提高系统的并发性能。

它的特点如下：

- 允许多个版本同时存在，并发执行。
- 不依赖锁机制，性能高。
- 只在读已提交和可重复读的事务隔离级别下工作。



### 事务的隔离级别：

SQL 标准提出了四种隔离级别来规避上述这些现象，隔离级别越高，性能效率就越低，这四个隔离级别如下：

- **读未提交（\*read uncommitted\*）**，指一个事务还没提交时，它做的变更就能被其他事务看到（可能会有脏读、不可重复读、幻读）；
- **读提交（\*read committed\*）**，指一个事务提交之后，它做的变更才能被其他事务看到（可能会有不可重复读、幻读）；
- **可重复读（\*repeatable read\*）**，指一个事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，**MySQL InnoDB 引擎的默认隔离级别**（可能会有幻读）；
- **串行化（\*serializable\* ）**；会对记录加上读写锁，在多个事务对这条记录进行读写操作时，如果发生了读写冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行；



### 如何实现四种隔离级别：

- 对于「读未提交」隔离级别的事务来说，因为可以读到未提交事务修改的数据，所以直接读取最新的数据就好了；
- 对于「串行化」隔离级别的事务来说，通过加读写锁的方式来避免并行访问；
- 对于「读提交」和「可重复读」隔离级别的事务来说，它们是通过 **Read View 来实现的，它们的区别在于创建 Read View 的时机不同，大家可以把 Read View 理解成一个数据快照，就像相机拍照那样，定格某一时刻的风景。「读提交」隔离级别是在「每个语句执行前」都会重新生成一个 Read View，而「可重复读」隔离级别是「启动事务时」生成一个 Read View，然后整个事务期间都在用这个 Read View**。



## 锁

### MySQL有哪些锁

#### 全局锁：

一把大锁把整个数据库都锁上进入只读状态，所有的修改手段都会被阻塞

#### 表级锁

其中又能够分为：

- 表锁：

  一把锁锁住当前这张表，表锁除了会限制别的线程的读写外，也会限制本线程接下来的读写操作

- 元数据锁（MDL）：

  我们不需要显示的使用 MDL，因为当我们对数据库表进行操作时，会自动给这个表加上 MDL：

  - 对一张表进行 CRUD 操作时，加的是 **MDL 读锁**；
  - 对一张表做结构变更操作的时候，加的是 **MDL 写锁**；

  MDL 是为了保证当用户对表执行 CRUD 操作时，防止其他线程对这个表结构做了变更。

- 意向锁：

  - 在使用 InnoDB 引擎的表里对某些记录加上「共享锁」之前，需要先在表级别加上一个「意向共享锁」；
  - 在使用 InnoDB 引擎的表里对某些纪录加上「独占锁」之前，需要先在表级别加上一个「意向独占锁」；

  也就是，当执行插入、更新、删除操作，需要先对表加上「意向独占锁」，然后对该记录加独占锁。

  而普通的 select 是不会加行级锁的，普通的 select 语句是利用 MVCC 实现一致性读，是无锁的。

  **意向共享锁和意向独占锁是表级锁，不会和行级的共享锁和独占锁发生冲突，而且意向锁之间也不会发生冲突，只会和共享表锁（\*lock tables ... read\*）和独占表锁（\*lock tables ... write\*）发生冲突。**

  **意向锁的目的是为了快速判断表里是否有记录被加锁**。

#### 行级锁

行级锁的类型主要有三类：

- Record Lock，记录锁，也就是仅仅把一条记录锁上：

  Record Lock 称为记录锁，锁住的是一条记录。而且记录锁是有 S 锁和 X 锁之分的：

  - 当一个事务对一条记录加了 S 型记录锁后，其他事务也可以继续对该记录加 S 型记录锁（S 型与 S 锁兼容），但是不可以对该记录加 X 型记录锁（S 型与 X 锁不兼容）;
  - 当一个事务对一条记录加了 X 型记录锁后，其他事务既不可以对该记录加 S 型记录锁（S 型与 X 锁不兼容），也不可以对该记录加 X 型记录锁（X 型与 X 锁不兼容）。

- Gap Lock，间隙锁，锁定一个范围，但是不包含记录本身：

  Gap Lock 称为间隙锁，只存在于可重复读隔离级别，目的是为了解决可重复读隔离级别下幻读的现象。

  假设，表中有一个范围 id 为（3，5）间隙锁，那么其他事务就无法插入 id = 4 这条记录了，这样就有效的防止幻读现象的发生。

  间隙锁虽然存在 X 型间隙锁和 S 型间隙锁，但是并没有什么区别，**间隙锁之间是兼容的，即两个事务可以同时持有包含共同间隙范围的间隙锁，并不存在互斥关系，因为间隙锁的目的是防止插入幻影记录而提出的**。

- Next-Key Lock：Record Lock + Gap Lock 的组合，锁定一个范围，并且锁定记录本身：

  Next-Key Lock 称为临键锁，是 Record Lock + Gap Lock 的组合，锁定一个范围，并且锁定记录本身。

  假设，表中有一个范围 id 为（3，5] 的 next-key lock，那么其他事务即不能插入 id = 4 记录，也不能修改 id = 5 这条记录。

  **next-key lock 是包含间隙锁+记录锁的，如果一个事务获取了 X 型的 next-key lock，那么另外一个事务在获取相同范围的 X 型的 next-key lock 时，是会被阻塞的**。



## 日志

### undo log、redo log、binlog

- 为什么需要undo log：
  1. 实现回滚，保证事务的原子性
  2. MVCC是通过快照+undo log实现的，MySQL在执行快照读（select）的时候会根据快照的信息，顺着undo log找到可见部分
  
- 为什么需要redo log：
  1. 为了确保数据库在发生故障时能够恢复数据的一致性和完整性，保证事务的持久性
  2. 提高性能，redo log是追加操作，在刷盘的时候的写操作是顺序写，比数据刷盘的随机写要快
  
- 为什么需要binlog：
  1. 用于备份复制，主从复制
  
  > ### binlog的三种格式
  >
  > - Statement（Statement-Based Replication,SBR）：每一条会修改数据的 SQL 都会记录在 binlog 中。
  >   - Statement 模式只记录执行的 SQL，不需要记录每一行数据的变化，因此极大的减少了 binlog 的日志量，避免了大量的 IO 操作，提升了系统的性能。（**比如update user set name="张三" where id >1 and id <10000,假设被修改的数据有2000条，那么Row的日志量就是2000条，而Statement只是这条sql语句一条日志而已，所以Statement的日志量相对Row会少很多**）
  >   - **但是，正是由于 Statement 模式只记录 SQL，而如果一些 SQL 中 包含了函数，那么可能会出现执行结果不一致的情况**。比如说 uuid() 函数，每次执行的时候都会生成一个随机字符串，在 master 中记录了 uuid，当同步到 slave 之后，再次执行，就得到另外一个结果了。
  >   - 所以使用 Statement 格式会出现一些数据一致性问题。
  > - Row（Row-Based Replication,RBR）：不记录 SQL 语句上下文信息，仅保存哪条记录被修改。
  >   - 从 MySQL5.1.5 版本开始，binlog 引入了 Row 格式，Row 格式不记录 SQL 语句上下文相关信息，仅仅只需要记录某一条记录被修改成什么样子了。
  >   - Row 格式的日志内容会非常清楚地记录下每一行数据修改的细节，这样就不会出现 Statement 中存在的那种数据无法被正常复制的情况。
  >   - 不过 Row 格式也有一个很大的问题，那就是日志量太大了，特别是批量 update、整表 delete、alter 表等操作，由于要记录每一行数据的变化，此时会产生大量的日志，大量的日志也会带来 IO 性能问题。
  >   - 此外，新版的MySQL中对row级别也做了一些优化，当表结构发生变化的时候，会记录语句而不是逐行记录。
  > - Mixed（Mixed-Based Replication,MBR）：Statement 和 Row 的混合体。
  >   - 从 MySQL5.1.8 版开始，MySQL 又推出了 Mixed 格式，这种格式实际上就是 Statement 与 Row 的结合。
  >   - 在 Mixed 模式下，系统会自动判断 该 用 Statement 还是 Row：一般的语句修改使用 Statement 格式保存 binlog；**对于一些 Statement 无法准确完成主从复制的操作，则采用 Row 格式保存 binlog**。
  >   - Mixed 模式中，MySQL 会根据执行的每一条具体的 SQL 语句来区别对待记录的日志格式，也就是在 Statement 和 Row 之间选择一种。

#### 如何实现主从复制

主要依赖于binlog，分成三个阶段：

> 1. 写入binlog：主库写入binlog日志，提交事务，并且更新本地缓存
> 2. 同步binlog：把binlog复制到从库中，每个从库把binlog写到暂存日志中
> 3. 回放binlog：从库回放binlog，并且更新存储引擎中的数据

主从复制有三种复制模型：同步复制、异步复制、半同步复制（两者之间）



# Golang 八股

具体参考[golang八股文整理（持续搬运）_golang面试八股文-CSDN博客](https://blog.csdn.net/qq_43716830/article/details/124405506)

## 进程、线程、协程、Goroutine

### 进程

资源分配和CPU调度的基本单位

### 线程

CPU调度的基本单位，线程除了有一些自己的必要的堆栈空间之外，其它的资源都是共享的线程中的

共享的资源包括：

> 1. 所有线程共享相同的虚拟地址空间，即它们可以访问同样的代码段、数据段和堆栈段。
> 2. 文件描述符：进程打开的文件描述符是进程级别的资源，所以同一个进程中的线程可以共享打开的文件描述符，这
>    意味着它们可以同时读写同一个文件。
> 3. 全局变量：全局变量是进程级别的变量，因此可以被同一个进程中的所有线程访问和修改。
> 4. 静态变量：静态变量也是进程级别的变量，在同一个进程中的线程之间共享内存空间。
> 5. 进程ID、进程组ID

### 协程

用户态的线程，可以通过用户程序创建、删除。协程切换时不需要切换内核态

> 协程与线程之间的区别：
>
> 1. 线程是操作系统的概念，而协程是程序级的概念。线程由操作系统调度执行，每个线程都有自己的执行上下文，包
> 括程序计数器、寄存器等。而协程由程序自身控制。
> 1. 多个线程之间通过切换执行的方式实现并发。线程切换时需要保存和恢复上下文，涉及到上下文切换的开销。而协
> 程切换时不需要操作系统的介入，只需要保存和恢复自身的上下文，切换开销较小。
> 1. 线程是抢占式的并发，即操作系统可以随时剥夺一个线程的执行权。而协程是合作式的并发，协程的执行权由程序
> 自身决定，只有当协程主动让出执行权时，其他协程才会得到执行机会。

### Goroutine

Goroutine，经 Golang 优化后的特殊“协程”，核心点如下：

1. 与线程存在映射关系，为 M：N；
2. 创建、销毁、调度在用户态完成，对内核透明，足够轻便；
3. 可利用多个线程，实现并行；
4. 通过调度器的斡旋，实现和线程间的动态绑定和灵活调度；
5. 栈空间大小可动态扩缩，因地制宜.



## 内存分配

Golang的内存管理组件主要有：mspan、mcache、mcentral和mheap

![图片](assets/640)

### 内存管理单元：mspan

mspan是内存管理的基本单元，该结构体中包含next和 prev两个字段，它们分别指向了前一个和后一个mspan，每个mspan都管理npages个大小为8KB的页，一个span是由多个page组成的，这里的页不是操作系统中的内存页，它们是操作系统内存页的整数倍，mspan根据大小被划分为67个等级

### 线程缓存：mcache

mache管理线程在本地缓存的mspan，每个goroutine绑定的P都有一个mcache字段



## 垃圾回收（GC）

### 标记-清除

1. 暂停业务逻辑，找到不可达的对象，和可达对象
2. 开始标记，程序找出它所有可达的对象，并做上标记
3. 标记完了之后，然后开始清除未标记的对象。
4. 停止暂停，让程序继续跑。然后循环重复这个过程，直到process程序生命周期结束

标记-清除的缺点：

- STW（stop the world）：让程序暂停，程序出现卡顿
- 标记需要扫描整个heap
- 清除数据会产生heap碎片

### 三色标记法

为了减少STW的时间，引入三色标记法改造了标记清除法的第三步和第四步

![在这里插入图片描述](assets/b8b9aa457948fdd1e587d61837860e5d.png)

**具体步骤如下：**

1. 把新创建的对象，默认的颜色都标记为“白色”
2. 每次GC回收开始，然后从根节点开始遍历所有对象，把遍历到的对象从白色集合放入“灰色”集合
3. 遍历灰色集合，将灰色对象引用的对象从白色集合放入到灰色集合，之后将此灰色对象放入到黑色集合
4. 重复第三步，直到灰色中无任何对象
5. 回收所有的白色标记的对象，也就是回收垃圾

**三色标记法在不采用STW保护时会出现：**

1. 一个白色对象被黑色对象引用
2. 灰色对象与它之间的可达关系的白色对象遭到破坏

这两种情况同时满足，会出现对象丢失

### 读写屏障

为了解决三色标记法提到的的问题，引入了两种三色不变式：

1. 强三色不变式：强制性的不允许黑色对象引用白色对象（破坏1）
2. 弱三色不变式：黑色对象可以引用白色对象，白色对象存在其他灰色对象对它的引用，或者可达它的链路上游存在灰色对象（破坏2）

而具体实现上述三色不变式的代码就是读写屏障：

- 插入屏障

  在A对象引用B对象的时候，B对象被标记为灰色（满足强三色不变式，黑色引用的白色对象会被强制转换为灰色）。只有堆上的对象触发插入屏障，栈上的对象不触发插入屏障。在准备回收白色前，重新遍历扫描一次栈空间。此时加STW暂停保护栈，防止外界干扰。

  ![在这里插入图片描述](assets/f614e8583f095e45f5334d031e25b8c0.png)

  不足：结束时需要使用STW来重新扫描栈

- 删除屏障：

  被删除的对象，如果自身为灰色或者白色，那么被标记为灰色（满足弱三色不变式）

  ![在这里插入图片描述](assets/321fcbf2a6055aa5b02ec6c1fd99319b.png)

  不足：回收精度低，一个对象即使被删除了最后一个指向它的指针也依旧可以活过这一轮，在下一轮GC中被清理掉。

### 三色标记法+混合屏障

具体操作：

1. GC开始将栈上的可达对象全部扫描并标记为黑色（之后不再进行第二次重复扫描，无需STW）
2. GC期间，任何在栈上创建的新对象，均为黑色
3. 堆上被删除对象标记为灰色
4. 堆上被添加的对象标记为灰色

满足：变形的弱三色不变式（结合了插入、删除写屏障的优点）

对于堆上的对象，采用三色标记法+写屏障保护

> ### 完整的流程阶段
>
> - 标记准备阶段：
>   1. 启动后台标记任务 暂停程序（STW），所有的处理器在这时会进入安全点（Safe point）； 
>   2. 如果当前垃圾收集循环是强制触发的，我们还需要处理还未被清理的内存管理单元； 
>   3. 将根对象入队 开启写屏障
> - 标记阶段：
>   1. 恢复用户协程
>   2. 使用三色标记法开始标记，此时用户协程和标记协程并发执行
> - 标记终止阶段：
>   1. 暂停用户协程
>   2. 计算下一次触发GC时需要达到的堆目标
>   3. 唤醒后台清扫协程
> - 清理阶段：
>   1. 关闭写屏障
>   2. 恢复用户协程
>   3. 异步清理回收

缺点：

1. 暂停时间：在进行垃圾回收时，必须停止程序执行，这会导致应用程序暂停。引入写屏障保护可以减少暂停时间，
   但仍然可能导致性能下降。
2. 内存开销：三色标记法需要为每个对象维护额外的状态信息，以记录其标记状态。这会增加内存开销，并可能对内
   存资源造成负担。
3. 频繁的垃圾回收：三色标记法需要频繁地迭代标记和清除对象，如果要回收的垃圾对象很多，可能会导致回收过程
   变得非常耗时。
4. 碎片化：垃圾回收过程中，如果频繁进行对象的移动和重新分配内存，可能会导致内存碎片化，降低内存的利用
   率。

### GC触发条件

1. 主动触发(手动触发)，通过调用 runtime.GC 来触发GC，此调用阻塞式地等待当前GC运行完毕。
2. 被动触发，分为两种方式：
   1. 使用步调（Pacing）算法，其核心思想是控制内存增长的比例,每次内存分配时检查当前内存分配量是否已达到阈值（环境变量GOGC）：默认100%，即当内存扩大一倍时启用GC。
   2. 使用系统监控，当超过两分钟没有产生任何GC时，强制触发 GC。

### GC调优

1. 控制内存分配的速度，限制Goroutine的数量，提高赋值器mutator的CPU利用率（降低GC的CPU利用率）
2. 少量使用+连接string
3. slice提前分配足够的内存来降低扩容带来的拷贝
4. 避免map key对象过多，导致扫描时间增加
5. 变量复用，减少对象分配，例如使用sync.Pool来复用需要频繁创建临时对象、使用全局变量等
6. 增大GOGC的值，降低GC的运行频率



## GMP模型

gmp = goroutine + machine + processor （+ 一套有机组合的机制），下面先单独拆出每个组件进行介绍，最后再总览全局，对 gmp 进行总述.

### G

1. g 即goroutine，是 golang 中对协程的抽象；

2. g 有自己的运行栈、状态、以及执行的任务函数（用户通过 go func 指定）；
3. g 需要绑定到 p 才能执行，在 g 的视角中，p 就是它的 cpu.

### M

1. m 即 machine，是 golang 中对线程的抽象；
2. m 不直接执行 g，而是先和 p 绑定，由其实现代理；
3. 借由 p 的存在，m 无需和 g 绑死，也无需记录 g 的状态信息，因此 g 在全生命周期中可以实现跨 m 执行.

### P

1. p 即 processor，是 golang 中的调度器；
2. p 是 gmp 的中枢，借由 p 承上启下，实现 g 和 m 之间的动态有机结合；
3. 对 g 而言，p 是其 cpu，g 只有被 p 调度，才得以执行；
4. 对 m 而言，p 是其执行代理，为其提供必要信息的同时（可执行的 g、内存分配情况等），并隐藏了繁杂的调度细节；
5. p 的数量决定了 g 最大并行数量，可由用户通过 GOMAXPROCS 进行设定（超过 CPU 核数时无意义）.

![image-20240827224746864](assets/image-20240827224746864.png)

GMP 宏观模型如上图所示，下面对其要点和细节进行逐一介绍：

1. M 是线程的抽象；G 是 goroutine；P 是承上启下的调度器；

2. M调度G前，需要和P绑定；
3. 全局有多个M和多个P，但同时并行的G的最大数量等于P的数量；
4. G的存放队列有三类：P的本地队列；全局队列；和wait队列（图中未展示，为io阻塞就绪态goroutine队列）；
5. M调度G时，优先取P本地队列，其次取全局队列，最后取wait队列；这样的好处是，取本地队列时，可以接近于无锁化，减少全局锁竞争；
6. 为防止不同P的闲忙差异过大，设立work-stealing机制，本地队列为空的P可以尝试从其他P本地队列偷取一半的G补充到自身队列.

### 调度流程

goroutine可以被分为两类：

- 负责调度普通 g 的 g0，执行固定的调度流程，与 m 的关系为一对一；
- 负责执行用户函数的普通 g.

**Goroutine调度策略:**

1. 队列轮转：P会周期性的将G调度到M中执行，执行一段时间后，保存上下文，将G放到队列尾部，然后从队列中再取出一个G进行调度，P还会周期性的查看全局队列是否有G等待调度到M中执行
2. 系统调用：当G0即将进入系统调用时，M0将释放P，进而某个空闲的M1获取P，继续执行P队列中剩下的G。M1的来源有可能是M的缓存池，也可能是新建的。
3. 当G0系统调用结束后，如果有空闲的P，则获取一个P，继续执行G0。如果没有，则将G0放入全局队列，等待被其他的P调度。然后M0将进入缓存池睡眠。

**gmp 的宏观调度流程：**

1. 以 g0 -> g -> g0 的一轮循环为例进行串联；
2. g0 执行 schedule() 函数，寻找到用于执行的 g；
3. g0 执行 execute() 方法，更新当前 g、p 的状态信息，并调用 gogo() 方法，将执行权交给 g；
4. g 因主动让渡( gosche_m() )、被动调度( park_m() )、正常结束( goexit0() )等原因，调用 m_call 函数，执行权重新回到 g0 手中；
5. g0 执行 schedule() 函数，开启新一轮循环.

### Goroutine的抢锁模式

![在这里插入图片描述](assets/fa5b5113a8eab7b68c4fb2e55acb40f4.png)

![在这里插入图片描述](assets/d08bc57cc2a088467bac7bdc83ea1800.png)

1. 正常模式
   - 在刚开始的时候，是处于正常模式(Barging)，也就是，当一个G1持有着一个锁的时候，G2会自旋地去	
     尝试获取这个锁
   - 当自旋超过4次还没有能获取到锁的时候，这个G2就会被加入到获取锁的等待队列里面，并阻塞等待
     唤醒goroutine 竞争锁，新请求锁的 goroutine具有优势：它正在CPU上执行，而且可能有好几个，所以刚
     唤醒的 goroutine有很大可能在锁竞争中失败，长时间获取不到锁，就会切换  到饥饿模式
2. 饥饿模式
   - 同时饥饿模式下，新进来的goroutine不会参与抢锁也不会进入自旋
     状态，会直接进入等待队列的尾部,这样很好的解决了老的goroutine一直抢不到锁的场景
   - 在同时满足下面两个条件之后回到正常模式：
     - G的等待的时间小于1ms
     - 等待队列已经全部清空了



## new和make的区别

var声明值类型的变量时，系统会默认为他分配内存空间，并赋该类型的零值
如果是指针类型或者引用类型的变量，系统不会为它分配内存，默认是nil。

1. make 仅用来分配及初始化类型为 slice、map、chan 的数据。
2. new 可分配任意类型的数据，根据传入的类型申请一块内存，返回指向这块内存的指针，即类型 *Type。
3. make 返回引用，即 Type，new 分配的空间被清零， make 分配空间后，会进行初始。
4. make函数返回的是slice、map、chan类型本身
5. new函数返回一个指向该类型内存地址的指针



## Golang如何实现单例模式

1. 使用包级别的变量

2. 使用sync.Once

   ```go
   type singleton struct {
       // 单例对象的字段
   }
   
   var (
       instance *singleton
       once     sync.Once
   )
   
   func GetInstance() *singleton {
       once.Do(func() {
           instance = &singleton{}
       })
       return instance
   }
   ```

3. 使用互斥锁sync.Mutex



## Golang的内存分配

Go的内存分配器在分配对象时，根据对象的大小，分成三类：小对象（小于等于16B）、一般对象（大于16B，小于等于32KB）、大对象（大于32KB）。

大体上的分配流程：

- 32KB 的对象，直接从mheap上分配；
- <=16B 的对象使用mcache的tiny分配器分配；
- (16B,32KB] 的对象，首先计算对象的规格大小，然后使用mcache中相应规格大小的mspan分配；
- 如果mcache没有相应规格大小的mspan，则向mcentral申请
- 如果mcentral没有相应规格大小的mspan，则向mheap申请
- 如果mheap中也没有合适大小的mspan，则向操作系统申请



# RabbitMQ（消息队列）八股

## 为什么需要消息队列

- 异步处理：对于一些不需要立即生效的操作，可以拆分出来，异步执行，使用消息队列实现，性能高
- 流量消峰：订单系统使用消息队列做缓冲，把一秒内下的订单分散成一段时间来处理，这时有些用户可能在下单十几秒后才能收到下单成功的操作
- 应用解耦：比如物流系统因为发生故障，需要几分钟来修复。在这几分钟的时间里，物流系统要处理的内存被缓存在消息队列中，用户的下单操作可以正常完成

## RabbitMQ的构造

![img](assets/7b36de2ce40435d0760af2d22a51ef52.png)

- 生产者Publisher：生产消息，就是投递消息的一方。消息一般包含两个部分：消息体（payload）和标签（Label）
- 消费者Consumer：消费消息，也就是接收消息的一方。消费者连接到RabbitMQ服务器，并订阅到队列上。消费消息时只消费消息体，丢弃标签。
- Broker服务节点：表示消息队列服务器实体。一般情况下一个Broker可以看做一个RabbitMQ服务器。
- Queue：消息队列，用来存放消息。一个消息可投入一个或多个队列，多个消费者可以订阅同一队列，这时队列中的消息会被平摊（轮询）给多个消费者进行处理。
- Exchange：交换器，接受生产者发送的消息，根据路由键将消息路由到绑定的队列上。
- Routing Key： 路由关键字，用于指定这个消息的路由规则，需要与交换器类型和绑定键(Binding Key)联合使用才能最终生效。
- Binding：绑定，通过绑定将交换器和队列关联起来，一般会指定一个BindingKey，通过BindingKey，交换器就知道将消息路由给哪个队列了。
- Connection ：网络连接，比如一个TCP连接，用于连接到具体broker
- Channel： 信道，AMQP 命令都是在信道中进行的，不管是发布消息、订阅队列还是接收消息，这些动作都是通过信道完成。因为建立和销毁 TCP 都是非常昂贵的开销，所以引入了信道的概念，以复用一条 TCP 连接，一个TCP连接可以用多个信道。客户端可以建立多个channel，每个channel表示一个会话任务。
- Message：消息，由消息头和消息体组成。消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等。
- Virtual host：虚拟主机，用于逻辑隔离，表示一批独立的交换器、消息队列和相关对象。一个Virtual host可以有若干个Exchange和Queue，同一个Virtual host不能有同名的Exchange或Queue。最重要的是，其拥有独立的权限系统，可以做到 vhost 范围的用户控制。当然，从 RabbitMQ 的全局角度，vhost 可以作为不同权限隔离的手段

## 六种工作模式

![img](assets/a4a6c8b848eb2e9aaf691a3355f8641e.png)

### 简单模式

- P：生产者，也就是要发送消息的程序
- C：消费者：消息的接收者，会一直等待消息到来
- queue：消息队列，图中红色部分。类似一个邮箱，可以缓存消息；生产者向其中投递消息，消费者从其中取出消息

### 工作队列模式

- Work Queues：与入门程序的简单模式相比，多了一个或一些消费端，多个消费端共同消费同一个队列中的消息。消费者之间对于同一个消息的关系是`竞争`的关系。
- 应用场景：对于任务过重或任务较多情况使用工作队列可以提高任务处理的速度。

### 发布/订阅模式

在订阅模型中，多了一个 Exchange 角色，而且过程略有变化：

- P：生产者，也就是要发送消息的程序，但是不再发送到队列中，而是发给X（交换机）

- C：消费者，消息的接收者，会一直等待消息到来

- Queue：消息队列，接收消息、缓存消息

- Exchange：交换机（X）。一方面，接收生产者发送的消息。另一方面，知道如何处理消息，例如递交给某个特别队列、递交给所有队列、或是将消息丢弃。到底如何操作，取决于Exchange的类型。

### 路由模式

- 队列与交换机的绑定，不能是任意绑定了，而是要指定一个 RoutingKey（路由key），消息的发送方在向 Exchange 发送消息时，也必须指定消息的 RoutingKey

- Exchange 不再把消息交给每一个绑定的队列，而是根据消息的 Routing Key 进行判断，只有队列的Routingkey 与消息的 Routing key 完全一致，才会接收到消息

### 通配符模式

- Topic 类型与 Direct 相比，都是可以根据 RoutingKey 把消息路由到不同的队列。只不过 Topic 类型Exchange 可以让队列在绑定 Routing key 的时候使用通配符！

- Routingkey 一般都是有一个或多个单词组成，多个单词之间以”.”分割，例如： item.insert

- 通配符规则：# 匹配一个或多个词，* 匹配不多不少恰好1个词，例如：item.# 能够匹配 item.insert.abc 或者 item.insert，item.* 只能匹配 item.insert

## 路由

### 消息是如何路由的

- 消息提供方 ——> 路由 ——> 一至多个队列
- 消息发布到交换器（Exchange）时，消息将拥有一个路由键（routing key），在消息创建时设定
- 通过队列路由键，可以把队列绑定到交换器上
- 消息到达交换器后，RabbitMQ会将消息的路由键与队列的路由键进行匹配（针对不同的交换器类型有不同的路由规则），常见的交换机类型有四种：direct、fanout、topic、headers

### Exchange的四种类型

- direct：

  Routing Key==Binding Key , 消息中的路由键 Routing Key 如果和 Binding 中的 Binding Key 完全匹配，交换器就将消息发到对应的队列中。是基于完全匹配、单播的模式

- fanout：

  把所有发送到fanout交换器的消息，路由到所有绑定该交换器的队列 Queue 中，fanout 类型转发消息是最快的

- topic：

  topic交换机使用routing key和binding key进行模糊匹配，匹配成功则将消息发送到相应的队列。routing key和binding key都是句点号“. ”分隔的字符串

  - routing_key 不能随意写，必须满足一定的要求，它必须是一个单词列表，以点号分隔开，这些单词可以是任意单词，比如说："stock.usd.nyse"单词列表最多不能超过 255 个字节
  - 替换符：binding key中可以存在两种特殊字符“*”与“##”,其中*用于匹配一个单词，##用于匹配多个单词
  - 当一个队列绑定键是#,那么这个队列将接收所有数据，就有点像 fanout，如果队列绑定键当中没有#和*出现，那么该队列绑定类型就是 direct

- headers：

  不依赖于routing key与binding key的匹配规则，而是根据发送消息内容中的headers属性进行匹配；除此之外 headers 交换器和 direct 交换器完全一致，但性能差很多（目前几乎用不到了）

## 消息分发

### 为什么RabbitMQ的消息分发基于信道

- TCP的创建和销毁开销大，创建需要三次握手，销毁需要四次分手
- 如果不使用信道，那么引用程序就会使用TCP的方式连接到rabbitmq，高峰时每秒成千上万条连接会造成资源的巨大浪费(一条tcp消耗资源，成千上万的tcp会非常消耗资源)，而且操作系统每秒处理TCP连接数量也是有限的，必定会造成性能瓶颈
- 信道的原理是一条线程一条信道，多条线程多条信道共同使用一条TCP连接。一条TCP连接可以容纳无限的信道，及时每秒造成成千上万的请求也不会造成性能瓶颈

### 消息属性和有效载荷(消息主体)

AMQP模型中的消息 (Message)对象是带有 属性(Attributes) 的。有些属性非常常见，例如：

- Content type: 内容类型
- Content encoding: 内容编码
- Routing Key: 路由键
- Delivery mode: 投递方式(持久化 or 非持久化)
- Message priority: 消息优先权
- Message publishing timestamp: 消息发布的时间戳
- Expiration period: 消息的有效期
- Publisher application id: 发布应用的id

有些属性是被 AMQP代理所使用的，比如 Routing Key，但是大多数是对给接收消息的消费者使用的，有些属性是可选为做消息头的。它们与HTTP协议的 X-headers很相似，比如 Content type、Content encoding 。

AMQP 的 消息除属性外，还含有一个消息体，即消息实际携带的数据，它对AMQP代理不透明。broker 不会检查或修改消息体，但是消息可以只包含属性而不携带消息体。

### 消息分发机制

主要有三种分发机制：轮训分发、不公平分发、预值分发

- 轮训分发

  RabbitMQ默认采用的轮训分发，当消费者有多个，且处理速度不相等（例如一个快一个慢）的时候不适用

- 不公平分发

  通过设置参数 channel.basicQos(1)实现不公平分发策略，能者多劳

- 预值分发

  当消息被消费者接收后，但是没有确认，此时这里就存在一个未确认的消息缓冲区，用于存储非被确认的消息，该缓存区的大小是没有限制的。

  通过使用basic.qos方法设置“预取计数”值定义通道上允许的未确认消息的最大数量

### 消息Pull模式

指消费者主动从消息队列中拉去消息而非消息队列主动推送

### **怎么设置消息的过期时间？**

过期时间就是TTL，TTL 全称 Time To Live（存活时间/过期时间）。

当消息到达存活时间后，还没有被消费，会被自动清除。

RabbitMQ可以对消息设置过期时间，也可以对整个队列（Queue）设置过期时间。

设置过期时间有两个办法：

- 在生产端发送消息时，给消息设置过期时间，单位毫秒(ms)
- 在消息队列创建队列时，指定队列的TTL，从消息入队列开始计算，超过该时间的消息将会被移除。



## 如何保证可靠性

RabbitMQ提供了四种保证措施：生产者确认机制、消费者手动确认消息、路由不可达消息和持久化。

### 生产者确认机制

生产者发送消息到队列，无法确保发送的消息成功的到达server。

解决方法：

- 事务机制。在一条消息发送之后会使发送端阻塞，等待RabbitMQ的回应，之后才能继续发送下一条消息。性能差。

- 开启生产者确认机制（confirm），只要消息成功发送到交换机之后，RabbitMQ就会发送一个ack给生产者（即使消息没有Queue接收，也会发送ack）。如果消息没有成功发送到交换机，就会发送一条Nack消息，提示发送失败。异步，性能好。

### 路由不可达消息

生产者确认机制只确保消息正确到达交换机，对于从交换机路由到Queue失败的消息，会被丢弃掉，导致消息丢失。

对于不可路由的消息，有两种处理方式：Return消息机制和备份交换机。

- Return消息机制，提供了回调函数 ReturnCallback，当消息从交换机路由到Queue失败才会回调这个方法

- 备份交换机，alternate-exchange 是一个普通的exchange，当你发送消息到对应的exchange时，没有匹配到queue，就会自动转移到备份交换机对应的queue，这样消息就不会丢失。

### 消费者手动确认消息

有可能消费者收到消息还没来得及处理MQ服务就宕机了，导致消息丢失。因为消息者默认采用自动ack，一旦消费者收到消息后会通知MQ Server这条消息已经处理好了，MQ 就会移除这条消息。

解决方法：

- 消费者设置为手动确认消息，即消费者处理完逻辑之后再给broker回复ack，表示消息已经成功消费，可以从broker中删除。

- 当消息者消费失败的时候，给broker回复nack，根据配置决定重新入队还是从broker移除，或者进入死信队列。只要没收到消费者的 ack，broker 就会一直保存着这条消息，但不会 requeue，也不会分配给其他 消费者。

### 持久化

如果RabbitMQ服务异常导致重启，将会导致消息丢失。

RabbitMQ 提供了持久化的机制，将内存中的消息持久化到硬盘上，即使重启RabbitMQ，消息也不会丢失。

消息持久化需要满足以下条件

- 消息设置持久化。发布消息前，设置投递模式delivery mode为2，表示消息需要持久化。
  - Queue队列设置持久化。
  - 交换机设置持久化。

- 持久化消息消费和恢复流程
  - 当发布一条消息到交换机上时，RabbitMQ 会先把消息写入持久化日志文件，然后才向生产者发送响应。
  - 一旦消费者从持久队列中消费了一条持久化消息并且做了确认，RabbitMQ会在持久化日志中把这条消息标记为等待垃圾收集，从而移除这条消息。
  - 如果持久化消息在被消费之前RabbitMQ重启，服务器会自动重建交换机和队列（以及绑定），并重新加载持久化日志中的消息到相应的队列或者交换机上，保证消息不会丢失。

## 死信队列

消费失败的消息存放的队列。

消息消费失败的原因：

- 消息被拒绝并且消息没有重新入队（requeue=false）

- 消息超时未消费

- 达到最大队列长度

当普通队列中有死信时，RabbitMQ 就会自动的将这个消息重新发布到设置的死信交换机去，然后被路由到死信队列。可以监听死信队列中的消息做相应的处理。

### RabbitMQ实现延迟队列

延迟队列，即消息进入队列后不会立即被消费，只有到达指定时间后，才会被消费。

需求：

- 下单后，30分钟未支付，取消订单，回滚库存。

- 新用户注册成功7天后，发送短信问候。

在RabbitMQ中并未提供延迟队列功能。

但是可以使用：TTL+死信队列 组合实现延迟队列的效果。



## 重复消费

### 产生原因
消息重复的原因有两个：1.生产时消息重复，2.消费时消息重复。

- 生产者发送消息给MQ，在MQ确认的时候出现了网络波动，生产者没有收到确认，这时候生产者就会重新发送这条消息，导致MQ会接收到重复消息。
- 消费者消费成功后，给MQ确认的时候出现了网络波动，MQ没有接收到确认，为了保证消息不丢失，MQ就会继续给消费者投递之前的消息。这时候消费者就接收到了两条一样的消息。

由于重复消息是由于网络原因造成的，无法避免。

### 解决方法

发送消息时让每个消息携带一个全局的唯一ID，在消费消息时先判断消息是否已经被消费过，保证消息消费逻辑的幂等性。

例如引入数据库或者redis：

- 数据库唯一主键去重：主键是不能冲突的，重复的数据无法插入

- 引入Redis解决重复消费问题
  - 利用Redis，首先系统生成全局唯一的 id，用set操作放入Redis中
  - 如订单信息id，消费后存储在Redis中，如果下次再来，先查看Redis中是否存在
  - 如果存在，即此消息已经被消费过（后续不做消费处理）
  - 如果不存在，即未消费，此时再将此id存入Redis中，进行后续的逻辑操作

综上，消费过程为：

- 消费者获取到消息后先根据id去查询 redis/db 是否存在该消息
- 如果不存在，则正常消费，消费完毕后写入redis/db
- 如果存在，则证明消息被消费过，直接丢弃



## 消息积压

### 导致消息积压的原因

- 要么是发送变快了

- 要么是消费变慢了。

### 如何解决

可以通过 扩容消费端的实例数来提升总体的消费能力。

如果短时间内没有足够的服务器资源进行扩容，那么就将系统降级，通过关闭一些不重要的业务，减少发送方发送的数据量，最低限度让系统还能正常运转，服务一些重要业务。

> 一个具体的例子：
>
> 几千万条数据在MQ里积压了七八个小时，最简单的方法可以让他恢复消费速度，然后等待几个小时消费完毕。
>
> 一般这个时候，只能操作临时紧急扩容了，具体操作步骤和思路如下：
>
> 先修复consumer的问题，确保其恢复消费速度，然后将现有cnosumer都停掉
>
> 新建一个topic，partition（queue）是原来的10倍，临时建立好原先10倍或者20倍的queue数量
>
> 然后写一个临时的分发数据的consumer程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的10倍数量的queue
>
> 接着临时征用10倍的机器来部署consumer，每一批consumer消费一个临时queue的数据
>
> 这种做法相当于是临时将queue资源和consumer资源扩大10倍，以正常的10倍速度来消费数据
>
> 等快速消费完积压数据之后，得恢复原先部署架构，重新用原先的consumer机器来消费消息
>



## 消费端如何限流

出于以下两个方面，所以需要对消费者进行一些限流策略

- 假设某个时候，在 rabbitmq 队列中已经堆积了非常非常多的消息，这个时候，如果有一个消费者启动，那么大量的消息将会一起推送到这个消费者上面，这种瞬间超大流量，很有可能导致服务器崩溃

- 生产者生产消息的效率比消费者处理消息的效率高很多，两端之间这种效率不平衡性。所以消费端需要做一些限流措施，否则可能导致消费端性能下降，服务器卡顿甚至崩溃等现象

**解决方法：**
在消费的时候使用Qos函数告诉消息队列要预取多少消息，在消费流程结束时候再发送ACK给消息队列获取后面的消息



# 其他常见八股

## 设计模式

设计模式分为三类：
1.创建型模式：用于控制对象的创建过程
2.结构型模式：用于处理类和对象之间的关系
3.行为型模式：用于描述对象之间的通信和协作

- 工厂模式（Factory Pattern）

  是一种创建型模式，它提供了一种统一的接口来创建对象，但是具体的对象创建过程则由子类来实现。工厂模式可以将对象的创建和使用解耦，使得代码更加灵活和可扩展。常见的工厂模式有简单工厂模式、工厂方法模式和抽象工厂模式。

- 代理模式（Proxy Pattern）

  是一种结构型模式，它为其他对象提供一种代理，以控制对这个对象的访问。代理模式可以在不改变原始对象的情况下，增加一些额外的功能或限制对原始对象的访问。常见的代理模式有静态代理和动态代理。

- 建造者模式（Builder Pattern）

  是一种创建型模式，它将一个复杂对象的创建过程分解成多个简单的步骤，从而可以灵活地组合这些步骤来构建不同的对象。建造者模式可以避免使用多个参数构造函数的情况，使得代码更加清晰和可读。同时，建造者模式还可以通过链式调用来简化对象的构建过程。



## 负载均衡

### 为什么需要负载均衡

对于早起的互联网应用，一台高性能的实例足以支撑业务需求。但是随着用户基数的扩大、性能要求的提高，单体架构不一定能够应付不断增加的请求量，并且如果发生了单点故障，会导致整个应用都不可用。因此引入了多台实例组成集群作为服务端

而负载均衡的目的就是为了让用户的请求合理的、平均的分发到集群的各个实例上



### 如何实现负载均衡

#### 基于硬件的负载均衡

在硬件上通过配置达到负载均衡的效果

- 优点：性能好，有售后支持

- 缺点：开销大，动态扩容比较困难（因为需要临时购买硬件）

#### 基于软件的负载均衡

常见的基于软件的负载均衡有nginx、HAProxy、LVS之类，优势在于价格亲民，并且灵活性强，可以进行自定义修改，是现在大部分企业的主流负载均衡方向

##### 基于DNS的负载均衡（DNS Load Balance）

> DNS是一个网络协议，主要功能是把域名转换成一个ip地址

在DNS负载均衡解析域名的时候，会随机将域名解析成不同的ip地址，这样就实现了负载均衡的效果

![image-20240824103800818](assets/image-20240824103800818.png)

- 优点：
  1. 使用简单：负载均衡工作，交给 DNS 服务器处理，省掉了负载均衡服务器维护的麻烦
  2. 提高性能：可以支持基于地址的域名解析，解析成距离用户最近的服务器地址（类似 CDN 的原理），可以加快访问速度，改善性能；

- 缺点：
  1. 可用性差：DNS 解析是多级解析，新增/修改 DNS 后，解析时间较长；解析过程中，用户访问网站将失败；
  2. 扩展性低：DNS 负载均衡的控制权在域名商那里，无法对其做更多的改善和扩展；
  3. 维护性差：也不能反映服务器的当前运行状态；支持的算法少；不能区分服务器的差异（不能根据系统与服务的状态来判断负载）。
- 适用场景：用于平衡不同地区请求数量的负载均衡，帮助用户将请求发送到物理距离最近的数据中心的ip地址

##### 基于应用层的负载均衡（Application Load Balance）

这是一种针对用户HTTP/HTTPS的负载均衡，例如用户访问某个域名下/image路径和/user路径，ALB会将这些请求发送到不同的应用上，常见ALB的有nginx和HAProxy

![image-20240824103728785](assets/image-20240824103728785.png)

- 优点：

  1. 可以将用户的请求发送到同一个ip的不同路径下，适合用于微服务之间互相转发的配置
  2. 由于HTTPS是通过了TLS安全加密的请求，ALB如果想要知道对于一条HTTPS请求，最终应该分发到哪一个实例，就需要用到TLS termination技术，将HTTPS请求进行解密，这样一来从ALB负载均衡器分发到每个后端实例的请求就都是未加密的请求了。对于后台服务实例来说就节省掉了加密和解密这样非常消耗CPU的操作
  3. 证书管理全部集中在ALB，降低证书出问题的概率

- 缺点：

  1. 由于需要TLS termination，所以tcp请求会被切分成两份，一份是用户和ALB之间的，一条是ALB和后端服务实例的，两条tcp在管理同步会较为复杂

  > 对于ALB一定要做好冗余设计，防止单台ALB服务故障导致整体服务不可用

##### 基于网络层的负载均衡（Network Load Balance）

用户端的请求会先发送到一个virtual ip，再由这个virtual ip经过负载均衡算法转发到对应集群中的某台实例上

NLB负载均衡在内核进程完成数据分发，较反向代理负载均衡有更好的从处理性能。但是，由于所有请求响应都要经过负载均衡服务器，集群的吞吐量受制于负载均衡服务器的带宽。



### 负载均衡算法

#### 随机

字面意思

#### 加权随机

在随机算法的基础上，按照概率调整权重，进行负载分配

#### 轮询

字面意思

该算法适合场景：各服务器处理能力相近，且每个事务工作量差异不大。如果存在较大差异，那么处理较慢的服务器就可能会积压请求，最终无法承担过大的负载

#### 加权轮询

在轮询算法的基础上，增加了权重属性来调节转发服务器的请求数目。性能高、处理速度快的节点应该设置更高的权重，使得分发时优先将请求分发到权重较高的节点上

> 大致思路为：
>
> 1. 遍历 Node 列表，检测当前 Node 是否有相应的 WeightedRoundRobin，没有则创建
> 2. 检测 Node 权重是否发生了变化，若变化了，则更新 WeightedRoundRobin 的 weight 字段
> 3. 让 current 字段加上自身权重，等价于 current += weight
> 4. 设置 lastUpdate 字段，即 lastUpdate = now
> 5. 寻找具有最大 current 的 Node，以及 Node 对应的 WeightedRoundRobin，暂存起来，留作后用
> 6. 计算权重总和
> 7. 对 weightMap 进行检查，过滤掉长时间未被更新的节点。该节点可能挂了，nodes 中不包含该节点，所以该节点的 lastUpdate 长时间无法被更新。若未更新时长超过阈值后，就会被移除掉，默认阈值为60秒。
>
> 其中WeightedRoundRobin

#### 最小活跃数

将请求分发到连接数/请求数最少的候选服务器（目前处理请求最少的服务器）

- 特点：根据候选服务器当前的请求连接数，动态分配。
- 场景：适用于对系统负载较为敏感或请求连接时长相差较大的场景。

由于每个请求的连接时长不一样，如果采用简单的轮循或随机算法，都可能出现某些服务器当前连接数过大，而另一些服务器的连接过小的情况，这就造成了负载并非真正均衡。虽然，轮询或算法都可以通过加权重属性的方式进行负载调整，但加权方式难以应对动态变化。

> 例如下图中，(1, 3, 5) 请求会被发送到服务器 1，但是 (1, 3) 很快就断开连接，此时只有 (5) 请求连接服务器 1；(2, 4, 6) 请求被发送到服务器 2，只有 (2) 的连接断开。该系统继续运行时，服务器 2 会承担过大的负载。
>
> ![img](assets/f28458976958cfc57a534444d7209e02.png)

最小活跃数算法会记录当前时刻，每个候选节点正在处理的连接数，然后选择连接数最小的节点。该策略能够动态、实时地反应服务器的当前状况，较为合理地将负责分配均匀，适用于对当前系统负载较为敏感的场景。

```go
// Node 表示一个负载均衡的节点
type Node struct {
	Active     int64  // 当前活跃数（正在处理的请求数）
	Identifier string // 节点的唯一标识符
}

// LoadBalancer 负载均衡器接口
type LoadBalancer interface {
	Select(nodes []*Node) *Node
}

// LeastActiveLoadBalancer 基于最小活跃数的负载均衡器
type LeastActiveLoadBalancer struct{}

// Select 选择一个节点
func (lb *LeastActiveLoadBalancer) Select(nodes []*Node) *Node {
	var selected *Node
	var minActive int64 = 1<<63 - 1 // 初始化为最大值

	for _, node := range nodes {
		active := atomic.LoadInt64(&node.Active)
		if active < minActive {
			minActive = active
			selected = node
		}
	}

	if selected != nil {
		atomic.AddInt64(&selected.Active, 1) // 增加活跃数
	}

	return selected
}
```



#### 加权最小活跃数

在最小活跃数的基础上，根据服务器的性能为每台服务器分配权重，再根据权重计算出每台服务器能处理的连接数。

最小活跃数算法实现要点：活跃调用数越小，表明该服务节点处理能力越高，单位时间内可处理更多的请求，应优先将请求分发给该服务。在具体实现中，每个服务节点对应一个活跃数 active。初始情况下，所有服务提供者活跃数均为 0。每收到一个请求，活跃数加 1，完成请求后则将活跃数减 1。在服务运行一段时间后，性能好的服务提供者处理请求的速度更快，因此活跃数下降的也越快，此时这样的服务提供者能够优先获取到新的服务请求、这就是最小活跃数负载均衡算法的基本思想。

```go
// Node 表示一个负载均衡的节点
type Node struct {
	Weight     int64 // 节点的权重
	Active     int64 // 当前活跃数（正在处理的请求数）
	Identifier string // 节点的唯一标识符
}

// LoadBalancer 负载均衡器接口
type LoadBalancer interface {
	Select(nodes []*Node) *Node
}

// WeightedLeastActiveLoadBalancer 基于加权最小活跃数的负载均衡器
type WeightedLeastActiveLoadBalancer struct{}

// Select 选择一个节点
func (lb *WeightedLeastActiveLoadBalancer) Select(nodes []*Node) *Node {
	var selected *Node
	var minActive int64 = 1<<63 - 1 // 初始化为最大值
	var totalWeight int64
	var sameWeight = true // 所有节点的权重是否相同

	for i, node := range nodes {
		active := atomic.LoadInt64(&node.Active)
		weight := atomic.LoadInt64(&node.Weight)

		if i == 0 {
			totalWeight = weight
		} else if weight != atomic.LoadInt64(&nodes[i-1].Weight) {
			sameWeight = false
		}

		if active < minActive {
			minActive = active
			selected = node
		} else if active == minActive && sameWeight {
			// 如果活跃数相同，且权重相同，则随机选择一个
			if selected == nil || weight > atomic.LoadInt64(&selected.Weight) {
				selected = node
			}
		}
	}

	if selected != nil {
		atomic.AddInt64(&selected.Active, 1) // 增加活跃数
	}

	return selected
}
```

#### 源地址哈希

根据请求源 IP，通过哈希计算得到一个数值，用该数值在候选服务器列表的进行取模运算，得到的结果便是选中的服务器。

可以保证同一 IP 的客户端的请求会转发到同一台服务器上，用来实现会话粘滞

> #### 会话粘滞的优点
>
> 1. **会话状态维护**：对于需要维护会话状态的应用，会话粘滞可以确保会话数据的一致性和完整性，因为所有相关的请求都会被发送到同一个服务器上。
> 2. **减少服务器负载**：由于会话数据不需要在服务器之间共享，可以减少服务器之间的通信开销。
>
> #### 会话粘滞的缺点
>
> 1. **负载不均衡**：如果某个服务器处理了大量会话，可能会导致该服务器负载过重，而其他服务器负载较轻，从而影响整体性能。
> 2. **单点故障风险**：如果某个服务器发生故障，所有与该服务器关联的会话都会丢失，影响用户体验。
> 3. **扩展性差**：增加或减少服务器时，需要重新配置会话映射，操作复杂。

#### 一致性哈希

一致性哈希（Consistent Hash）算法的目标是：相同的请求尽可能落到同一个服务器上。

一致性哈希 可以很好的解决 稳定性问题，可以将所有的 存储节点 排列在 首尾相接 的 Hash 环上，每个 key 在计算 Hash 后会 顺时针 找到 临接 的 存储节点 存放。而当有节点 加入 或 退出 时，仅影响该节点在 Hash环上顺时针相邻的后续节点。

**重点在于哈希环和虚拟节点**

![img](assets/ff9b3db1a3d8e5cd32e2e72840dba92c.png)

##### 一致性哈希环上某个节点挂了怎么办



##### 数据迁移怎么做

1. 新增节点

   假设现在已经有ABCD四个节点，新加入的E节点经过哈希运算之后在BC之间

   **下面是对新增节点流程的梳理：**

   - 找到节点 G 顺时针往下的第一个节点 C
   - 检索节点 C 中的数据，将从属于 (B,G] 范围的这部分数据摘出来，迁移到节点 G
   - 节点 G 添加入环

2. 删除节点

   假设现在已经有ABCDE五个节点，删除B节点

   **下面是删除节点的执行流程：**

   - • 找到 B 顺时针向下的第一个节点 C
   - • 将 B 中的全量数据迁移至节点 C
   - • 从哈希环中移除节点 B



```go
type HashRing struct {
	replicateCount int                // 每台服务所对应的节点数量（实际节点 + 虚拟节点）
	nodes          map[uint32]string  // 键：节点哈希值 ， 值：服务器地址
	sortedNodes    []uint32           // 从小到大排序后的所有节点哈希值切片，可以认为这个就是 哈希环
	rwLock				 sync.RWMutex			  // 读写锁，保护对共享数据的访问
}

/*
 * 作用：在哈希环上添加单个服务器节点（包含虚拟节点）的方法
 * 入参：服务器地址
 */
func (hr *HashRing) addNode(masterNode string) {
  hr.rwLock.Lock()
  defer hr.rwLock.Unlock()

	// 为每台服务器生成数量为 replicateCount-1 个虚拟节点
	// 并将其与服务器的实际节点一同添加到哈希环中
	for i := 0; i < hr.replicateCount; i++ {
		// 获取节点的哈希值，其中节点的字符串为 i+address
		key := hr.hashKey(strconv.Itoa(i) + masterNode)
		// 设置该节点所对应的服务器（建立节点与服务器地址的映射）
		hr.nodes[key] = masterNode
		// 将节点的哈希值添加到哈希环中
		hr.sortedNodes = append(hr.sortedNodes, key)
	}

	// 按照值从大到小的排序函数
	sort.Slice(hr.sortedNodes, func(i, j int) bool {
		return hr.sortedNodes[i] < hr.sortedNodes[j]
	})
}

/*
 * 作用：添加多个服务器节点（包含虚拟节点）的方法
 * 入参：服务器地址集合
 */
func (hr *HashRing) addNodes(masterNodes []string) {
  hr.rwLock.Lock()
  defer hr.rwLock.Unlock()
  
	if len(masterNodes) > 0 {
		for _, node := range masterNodes {
			// 调用 addNode 方法为每台服务器创建实际节点和虚拟节点并建立映射关系
			// 最后将创建好的节点添加到哈希环中
			hr.addNode(node)
		}
	}
}

/*
 * 作用：从哈希环上移除单个服务器节点（包含虚拟节点）的方法
 * 入参：服务器地址
 */
func (hr *HashRing) removeNode(masterNode string) {
  hr.rwLock.Lock()
  defer hr.rwLock.Unlock()
	
	// 移除时需要将服务器的实际节点和虚拟节点一同移除
	for i := 0; i < hr.replicateCount; i++ {
		// 计算节点的哈希值
		key := hr.hashKey(strconv.Itoa(i) + masterNode)
		// 移除映射关系
		delete(hr.nodes, key)
		// 从哈希环上移除实际节点和虚拟节点
		if success, index := hr.getIndexForKey(key); success {
			hr.sortedNodes = append(hr.sortedNodes[:index], hr.sortedNodes[index+1:]...)
		}
	}
}

/*
 * 作用：给定一个客户端地址获取应当处理其请求的服务器的地址
 * 入参：客户端地址
 * 返回：应当处理该客户端请求的服务器的地址
 */
func (hr *HashRing) getNode(key string) string {
  hr.rwLock.Lock()
  defer hr.rwLock.Unlock()

	if len(hr.nodes) == 0 {
		return ""
	}

	// 获取客户端地址的哈希值
	hashKey := hr.hashKey(key)
	nodes := hr.sortedNodes

	// 当客户端地址的哈希值大于服务器上所有节点的哈希值时默认交给首个节点处理
	masterNode := hr.nodes[nodes[0]]

	for _, node := range nodes {
		// 如果客户端地址的哈希值小于当前节点的哈希值
		// 说明客户端的请求应当由该节点所对应的服务器来进行处理（逆时针）
		if hashKey < node {
			masterNode = hr.nodes[node]
			break
		}
	}

	return masterNode
}

/*
 * 作用：哈希函数（这里使用 crc32 算法来实现，返回的是一个 uint32 整型）
 * 入参：节点或客户端地址
 * 返回：地址所对应的哈希值
 */
func (hr *HashRing) hashKey(key string) uint32 {
	scratch := []byte(key)
	return crc32.ChecksumIEEE(scratch)
}

/*
 * 作用：获取指定哈希值在哈希环上的索引
 * 入参：哈希值
 * 返回：是否成功找到，以及找到的索引
 */
func (hr *HashRing) getIndexForKey(key uint32) (bool, int) {
	for i, node := range hr.sortedNodes {
		if node == key {
			return true, i
		}
	}
	return false, -1
}
```



## JWT

### JWT和普通令牌的区别

- 无状态性：JWT是无状态的令牌，不需要在服务器端存储会话信息。相反，JWT令牌中包含了所有必要的信息，如用户身份、权限等。这使得JWT在分布式系统中更加适用，可以方便地进行扩展和跨域访问。
- 安全性：JWT使用密钥对令牌进行签名，确保令牌的完整性和真实性。只有持有正确密钥的服务器才能对令牌进行验证和解析。这种方式比传统的基于会话和Cookie的验证更加安全，有效防止了CSRF(跨站请求伪造)等攻击。
- 跨域支持：JWT令牌可以在不同域之间传递，适用于跨域访问的场景。通过在请求的头部或参数中携带JWT令牌，可以实现无需Cookie的跨域身份验证。

### JWT有哪些字段

包含三个部分：

1. header：简单声明了类型(JWT)以及产生签名所使用的算法
2. Claims：表示要发送的用户详细信息
3. Signature：签名的目的是为了保证上边两部分信息不被篡改。如果尝试使用Bas64对解码后的token进行修改，签名信息就会失效。一般使用一个私钥（private key）通过特定算法对Header和Claims进行混淆产生签名信息，所以只有原始的token才能于签名信息匹配

### 为什么JWT可以解决集群部署的问题

在传统的基于会话和Cookie的身份验证方式中，会话信息通常存储在服务器的内存或数据库中。但在集群部署中，不同服务器之间没有共享的会话信息，这会导致用户在不同服务器之间切换时需要重新登录，或者需要引入额外的共享机制(如Redis)，增加了复杂性和性能开销。

而JWT令牌通过在令牌中包含所有必要的身份验证和会话信息，使得服务器无需存储会话信息，从而解决了集群部署中的身份验证和会话管理问题。当用户进行登录认证后，服务器将生成一个JWT令牌并返回给客户端。客户端在后续的请求中携带该令牌，服务器可以通过对令牌进行验证和解析来获取用户身份和权限信息，而无需访问共享的会话存储。

由于JWT令牌是自包含的，服务器可以独立地对令牌进行验证，而不需要依赖其他服务器或共享存储。这使得集群中的每个服务器都可以独立处理请求，提高了系统的可伸缩性和容错性。

### JWT泄漏了怎么办

- 及时失效令牌

  当检测到JWT令牌泄露或存在风险时，可以立即将令牌标记为失效状态。服务器在接收到带有失效标记的令牌时，会拒绝对其进行任何操作，从而保护用户的身份和数据安全。

- 刷新令牌

  JWT令牌通常具有一定的有效期，过期后需要重新获取新的令牌。当检测到令牌泄露时,可以主动刷新令牌，即重新生成一个新的令牌，并将旧令牌标记为失效状态。这样，即使泄露的令牌被恶意使用，也会很快失效，减少了被攻击者滥用的风险。

- 使用黑名单

  服务器可以维护一个令牌的黑名单，将泄露的令牌添加到黑名单中。在接收到令牌时，先检查令牌是否在黑名单中，如果在则拒绝操作。这种方法需要服务器维护黑名单的状态，对性能有一定的影响，但可以有效地保护泄露的令牌不被滥用。



## 面向对象的理解

- 封装

  封装是面向对象编程的基本原则之一，它指的是将数据和操作数据的方法封装在一个单元内，即一个类中。封装通过访问控制修饰符（如private、protected、public）来限制对类的成员的访问。这种封装性能够隐藏对象的内部实现细节，只暴露必要的接口，提高了代码的安全性和可维护性。

- 继承

  继承是一种机制，允许一个类（子类）基于另一个类（父类）的定义来构建。子类继承了父类的属性和方法，同时可以通过扩展或修改来增加或改变其行为。继承促进了代码的重用，通过建立类的层次结构，提高了代码的可维护性和扩展性。

- 多态

  多态性是指同一个方法在不同的对象上产生不同的行为。在面向对象编程中，多态性有两种主要形式：编译时多态（静态多态）和运行时多态（动态多态）。

  - 编译时多态： 通过方法的重载实现，同一个类中的多个方法拥有相同的名称但不同的参数列表。

  - 运行时多态： 通过方法的覆盖实现，子类可以提供对父类方法的不同实现。

这三大特征共同构成了面向对象编程的基本框架，使得代码更易理解、扩展和维护。



## CAP理论

CAP是Consistency、Avaliability、Partitiontolerance三个词语的缩写，分别表示一致性、可用性、分区容忍性。

- 一致性

  写操作后的读操作可以读取到最新的数据状态，当数据分布在多个节点时，从任意节点读取到的数据都是最新的状态。

- 可用性

  可用性是指任何事务操作都可以得到相应结果，且不会出现响应超时或响应错误。

- 分区容忍性

  通常分布式系统的各个节点部署在不同的子网，这就是网络分区，不可避免的会出现由于网络问题而导致节点之间通信失败，此时仍可对外提供服务，这叫分区容忍性。

> **1.1 CA组合**
>
> CA组合就是保证一致性和可用性，放弃分区容忍性，即不进行分区，不考虑由于网络不通或节点挂掉的问题。那么系统将不是一个标准的分布式系统，我们最常用的关系型数据库就满足了CA。
>
> **1.2 CP组合**
>
> CP组合就是保证一致性和分区容忍性，放弃可用性。Zookerper就是追求强一致性，放弃了可用性，还有跨行转账，一次转账请求要等待双方银行系统都完成整个事务才能完成。
>
> **1.3 AP组合**
>
> AP组合就是保证可用性和分区容忍性，放弃一致性。这是分布式系统设计时的选择。

很多人都说CAP只能选择其中两个，但是事实上在设计分布式系统的时候，P是必须要具备的（不然分布体现在哪里）
